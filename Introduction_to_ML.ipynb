{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8a4c995-1983-45bc-ab94-46c7cbc73e0b",
      "metadata": {
        "id": "a8a4c995-1983-45bc-ab94-46c7cbc73e0b"
      },
      "source": [
        "# Лабораторная работа 1\n",
        "\n",
        "Результат лабораторной работы − отчет. Мы предпочитаем принимать отчеты в формате ноутбуков Jupyter (ipynb-файл). Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий, а также подкреплённый грамотной визуализацией. Помимо ответов на вопросы, в отчете так же должен быть код, однако чем меньше кода, тем лучше всем: нам − меньше проверять, вам —  проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета, кода и графиков. \n",
        "\n",
        "## Оценивание и штрафы\n",
        "\n",
        "* Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи)\n",
        "* Максимально допустимая оценка за работу — 15 баллов\n",
        "* Сдавать задание после указанного срока сдачи нельзя\n",
        "* «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают свою карму\n",
        "* Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому, чтобы исключить подозрение в плагиате, необходима ссылка на источник)\n",
        "* Не оцениваются задания с удалёнными формулировкам\n",
        "* Не оценивается лабораторная работа целиком, если она была выложена в открытый источник\n",
        "\n",
        "## Данные\n",
        "\n",
        "В данной лабораторной работе вы будете работать с данными из Dota2. \n",
        "\n",
        "Скачать датасет и подробно ознакомится с его описанием вы можете на странице первого конкурсного [соревнования](https://www.kaggle.com/t/8ddaf99c566b4d088b3e352ce1bbbc19).\n",
        "\n",
        "## Маштабирование признаков\n",
        "\n",
        "Масштабирование признаков можно выполнить, например, одним из следующих способов:\n",
        " - $x_{new} = \\dfrac{x - \\mu}{\\sigma}$, где $\\mu, \\sigma$ — среднее и стандартное отклонение значения признака по всей выборке (см. функцию [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html))\n",
        " - $x_{new} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$, где $[x_{min}, x_{max}]$ — минимальный интервал значений признака\n",
        "\n",
        "Похожие схемы масштабирования приведены в классах [sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) и [sklearn.preprocessing.MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler).\n",
        "\n",
        "## Подбор гиперпараметров модели\n",
        "\n",
        "В задачах машинного обучения следует различать параметры модели и гиперпараметры (структурные параметры). Обычно параметры модели настраиваются в ходе обучения (например, веса в линейной модели), в то время как гиперпараметры задаются заранее (например, значение силы регуляризации в линейной модели). Каждая модель, как правило, имеет множество гиперпараметров и нет универсальных наборов гиперпараметров, оптимально работающих во всех задачах, поэтому для каждой задачи нужно подбирать свой набор.\n",
        "\n",
        "Для оптимизации гиперпараметров модели часто используют _перебор по сетке (grid search)_: для каждого гиперпараметра выбирается несколько значений, далее перебираются все комбинации значений и выбирается комбинация, на которой модель показывает лучшее качество (с точки зрения оптимизируемой метрики). Однако, в этом случае нужно грамотно оценивать построенную модель, а именно делать разбиение на обучающую и тестовую выборку.\n",
        "\n",
        "В этом случае сравнение большого числа моделей при переборе гиперпараметров приводит к ситуации, когда лучшая на тестовой подвыборке модель не сохраняет свои качества на новых данных. Можно сказать, что происходит _переобучение_ на тестовую выборку.\n",
        "\n",
        "Для устранения описанной выше проблемы, **можно разбить данные на 3 непересекающихся подвыборки: обучение, валидация и тест**. Валидационную подвыборку используют для сравнения моделей, а тестовую — для окончательной оценки качества и сравнения семейств моделей с подобранными гиперпараметрами.\n",
        "\n",
        "**Другой способ сравнения моделей — [кросс-валидация](http://bit.ly/1CHXsNH)**. \n",
        "\n",
        "Существуют различные **схемы кросс-валидации**:\n",
        "  - Leave-One-Out\n",
        "  - K-Fold\n",
        "  - Многократное случайное разбиение выборки\n",
        "  \n",
        "Кросс-валидация вычислительно затратна, особенно если вы делаете перебор по сетке с очень большим числом комбинации. С учетом конечности времени на выполнение задания, возникает ряд компромиссов: \n",
        "  - сетку гиперпараметров можно делать более разреженной, перебирая меньше значений каждого гиперпараметра; однако, не стоит забывать, что в таком случае можно пропустить хорошую комбинацию гиперпараметров;\n",
        "  - кросс-валидацию можно делать с меньшим числом разбиений или фолдов, но в таком случае оценка качества становится более шумной и увеличивается риск выбрать неоптимальный набор гиперпараметров из-за случайности разбиения;\n",
        "  - гиперпараметры можно оптимизировать последовательно (жадно) — один за другим, а не перебирать все комбинации; такая стратегия не всегда приводит к оптимальному набору;\n",
        "  - перебирать не все комбинации гиперпараметров, а небольшое число случайно выбранных."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88a6bb1-28c4-4ced-802b-52e3493ffad3",
      "metadata": {
        "id": "c88a6bb1-28c4-4ced-802b-52e3493ffad3"
      },
      "source": [
        "## Линейная регрессия и SGD\n",
        "\n",
        "В данном блоке вам предстоит предсказать суммарное количество золота для команды `radiant` \n",
        "\n",
        "(игроки: `player_0`, `player_1`, `player_2`, `player_3`, `player_4`) на 600 секунде матча."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4393ebb-14fa-48dc-9e3a-2cfe9e9c265a",
      "metadata": {
        "id": "b4393ebb-14fa-48dc-9e3a-2cfe9e9c265a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "X = pd.read_csv('gold.csv', index_col=[0, 1])\n",
        "\n",
        "radiant_team = ['player_0', 'player_1', 'player_2', 'player_3', 'player_4']\n",
        "dire_team = ['player_5', 'player_6', 'player_7', 'player_8', 'player_9']\n",
        "\n",
        "Y = X.loc[pd.IndexSlice[:, 600], radiant_team].sum(axis=1).droplevel('times')\n",
        "X = X.unstack().drop([(player, 600) for player in radiant_team], axis=1)\n",
        "\n",
        "X.columns = ['{}_{}'.format(*column) for column in X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff41fc11",
      "metadata": {
        "id": "ff41fc11",
        "outputId": "2e90326c-a22a-4029-c1c9-882b18d0ffee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player_0_60</th>\n",
              "      <th>player_0_120</th>\n",
              "      <th>player_0_180</th>\n",
              "      <th>player_0_240</th>\n",
              "      <th>player_0_300</th>\n",
              "      <th>player_0_360</th>\n",
              "      <th>player_0_420</th>\n",
              "      <th>player_0_480</th>\n",
              "      <th>player_0_540</th>\n",
              "      <th>player_1_60</th>\n",
              "      <th>...</th>\n",
              "      <th>player_9_60</th>\n",
              "      <th>player_9_120</th>\n",
              "      <th>player_9_180</th>\n",
              "      <th>player_9_240</th>\n",
              "      <th>player_9_300</th>\n",
              "      <th>player_9_360</th>\n",
              "      <th>player_9_420</th>\n",
              "      <th>player_9_480</th>\n",
              "      <th>player_9_540</th>\n",
              "      <th>player_9_600</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>750</td>\n",
              "      <td>957</td>\n",
              "      <td>1161</td>\n",
              "      <td>1571</td>\n",
              "      <td>1721</td>\n",
              "      <td>1871</td>\n",
              "      <td>2022</td>\n",
              "      <td>2850</td>\n",
              "      <td>3303</td>\n",
              "      <td>350</td>\n",
              "      <td>...</td>\n",
              "      <td>1056</td>\n",
              "      <td>1360</td>\n",
              "      <td>2072</td>\n",
              "      <td>2283</td>\n",
              "      <td>3302</td>\n",
              "      <td>4071</td>\n",
              "      <td>4686</td>\n",
              "      <td>5207</td>\n",
              "      <td>5609</td>\n",
              "      <td>6384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>285</td>\n",
              "      <td>435</td>\n",
              "      <td>585</td>\n",
              "      <td>736</td>\n",
              "      <td>1334</td>\n",
              "      <td>1667</td>\n",
              "      <td>1818</td>\n",
              "      <td>2016</td>\n",
              "      <td>2328</td>\n",
              "      <td>344</td>\n",
              "      <td>...</td>\n",
              "      <td>513</td>\n",
              "      <td>851</td>\n",
              "      <td>1239</td>\n",
              "      <td>1840</td>\n",
              "      <td>2052</td>\n",
              "      <td>2321</td>\n",
              "      <td>3214</td>\n",
              "      <td>3603</td>\n",
              "      <td>4062</td>\n",
              "      <td>4623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>288</td>\n",
              "      <td>756</td>\n",
              "      <td>1224</td>\n",
              "      <td>1617</td>\n",
              "      <td>1920</td>\n",
              "      <td>2328</td>\n",
              "      <td>2611</td>\n",
              "      <td>2879</td>\n",
              "      <td>3069</td>\n",
              "      <td>288</td>\n",
              "      <td>...</td>\n",
              "      <td>438</td>\n",
              "      <td>646</td>\n",
              "      <td>796</td>\n",
              "      <td>946</td>\n",
              "      <td>1168</td>\n",
              "      <td>1660</td>\n",
              "      <td>1810</td>\n",
              "      <td>1959</td>\n",
              "      <td>2218</td>\n",
              "      <td>2491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>288</td>\n",
              "      <td>438</td>\n",
              "      <td>1230</td>\n",
              "      <td>1381</td>\n",
              "      <td>1916</td>\n",
              "      <td>2436</td>\n",
              "      <td>2585</td>\n",
              "      <td>2735</td>\n",
              "      <td>2886</td>\n",
              "      <td>556</td>\n",
              "      <td>...</td>\n",
              "      <td>288</td>\n",
              "      <td>438</td>\n",
              "      <td>795</td>\n",
              "      <td>946</td>\n",
              "      <td>1340</td>\n",
              "      <td>1591</td>\n",
              "      <td>1740</td>\n",
              "      <td>1890</td>\n",
              "      <td>2097</td>\n",
              "      <td>2247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>348</td>\n",
              "      <td>572</td>\n",
              "      <td>745</td>\n",
              "      <td>1170</td>\n",
              "      <td>1590</td>\n",
              "      <td>1787</td>\n",
              "      <td>2070</td>\n",
              "      <td>2520</td>\n",
              "      <td>2948</td>\n",
              "      <td>561</td>\n",
              "      <td>...</td>\n",
              "      <td>288</td>\n",
              "      <td>437</td>\n",
              "      <td>643</td>\n",
              "      <td>855</td>\n",
              "      <td>1065</td>\n",
              "      <td>1499</td>\n",
              "      <td>1649</td>\n",
              "      <td>1800</td>\n",
              "      <td>2070</td>\n",
              "      <td>2220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>286</td>\n",
              "      <td>435</td>\n",
              "      <td>650</td>\n",
              "      <td>981</td>\n",
              "      <td>1131</td>\n",
              "      <td>1926</td>\n",
              "      <td>2076</td>\n",
              "      <td>2226</td>\n",
              "      <td>2674</td>\n",
              "      <td>345</td>\n",
              "      <td>...</td>\n",
              "      <td>741</td>\n",
              "      <td>1131</td>\n",
              "      <td>1577</td>\n",
              "      <td>1901</td>\n",
              "      <td>2549</td>\n",
              "      <td>3051</td>\n",
              "      <td>3922</td>\n",
              "      <td>4183</td>\n",
              "      <td>5088</td>\n",
              "      <td>5598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>564</td>\n",
              "      <td>1146</td>\n",
              "      <td>1599</td>\n",
              "      <td>2008</td>\n",
              "      <td>2409</td>\n",
              "      <td>2685</td>\n",
              "      <td>3378</td>\n",
              "      <td>3722</td>\n",
              "      <td>4503</td>\n",
              "      <td>286</td>\n",
              "      <td>...</td>\n",
              "      <td>879</td>\n",
              "      <td>1206</td>\n",
              "      <td>1357</td>\n",
              "      <td>1507</td>\n",
              "      <td>1967</td>\n",
              "      <td>2244</td>\n",
              "      <td>2516</td>\n",
              "      <td>2666</td>\n",
              "      <td>2816</td>\n",
              "      <td>3027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>285</td>\n",
              "      <td>436</td>\n",
              "      <td>586</td>\n",
              "      <td>799</td>\n",
              "      <td>1013</td>\n",
              "      <td>1318</td>\n",
              "      <td>1468</td>\n",
              "      <td>1617</td>\n",
              "      <td>2116</td>\n",
              "      <td>285</td>\n",
              "      <td>...</td>\n",
              "      <td>285</td>\n",
              "      <td>436</td>\n",
              "      <td>586</td>\n",
              "      <td>736</td>\n",
              "      <td>885</td>\n",
              "      <td>1036</td>\n",
              "      <td>1186</td>\n",
              "      <td>1403</td>\n",
              "      <td>1894</td>\n",
              "      <td>2044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>288</td>\n",
              "      <td>437</td>\n",
              "      <td>585</td>\n",
              "      <td>736</td>\n",
              "      <td>959</td>\n",
              "      <td>1110</td>\n",
              "      <td>1258</td>\n",
              "      <td>2335</td>\n",
              "      <td>3842</td>\n",
              "      <td>540</td>\n",
              "      <td>...</td>\n",
              "      <td>585</td>\n",
              "      <td>1068</td>\n",
              "      <td>1602</td>\n",
              "      <td>1980</td>\n",
              "      <td>2510</td>\n",
              "      <td>2719</td>\n",
              "      <td>3189</td>\n",
              "      <td>4548</td>\n",
              "      <td>5300</td>\n",
              "      <td>5451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>484</td>\n",
              "      <td>870</td>\n",
              "      <td>1210</td>\n",
              "      <td>1759</td>\n",
              "      <td>2154</td>\n",
              "      <td>2640</td>\n",
              "      <td>3176</td>\n",
              "      <td>3760</td>\n",
              "      <td>4041</td>\n",
              "      <td>288</td>\n",
              "      <td>...</td>\n",
              "      <td>466</td>\n",
              "      <td>992</td>\n",
              "      <td>1330</td>\n",
              "      <td>1666</td>\n",
              "      <td>1816</td>\n",
              "      <td>2850</td>\n",
              "      <td>3345</td>\n",
              "      <td>3865</td>\n",
              "      <td>4513</td>\n",
              "      <td>4969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 95 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       player_0_60  player_0_120  player_0_180  player_0_240  player_0_300  \\\n",
              "mid                                                                          \n",
              "0              750           957          1161          1571          1721   \n",
              "1              285           435           585           736          1334   \n",
              "2              288           756          1224          1617          1920   \n",
              "3              288           438          1230          1381          1916   \n",
              "4              348           572           745          1170          1590   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "49943          286           435           650           981          1131   \n",
              "49944          564          1146          1599          2008          2409   \n",
              "49945          285           436           586           799          1013   \n",
              "49946          288           437           585           736           959   \n",
              "49947          484           870          1210          1759          2154   \n",
              "\n",
              "       player_0_360  player_0_420  player_0_480  player_0_540  player_1_60  \\\n",
              "mid                                                                          \n",
              "0              1871          2022          2850          3303          350   \n",
              "1              1667          1818          2016          2328          344   \n",
              "2              2328          2611          2879          3069          288   \n",
              "3              2436          2585          2735          2886          556   \n",
              "4              1787          2070          2520          2948          561   \n",
              "...             ...           ...           ...           ...          ...   \n",
              "49943          1926          2076          2226          2674          345   \n",
              "49944          2685          3378          3722          4503          286   \n",
              "49945          1318          1468          1617          2116          285   \n",
              "49946          1110          1258          2335          3842          540   \n",
              "49947          2640          3176          3760          4041          288   \n",
              "\n",
              "       ...  player_9_60  player_9_120  player_9_180  player_9_240  \\\n",
              "mid    ...                                                          \n",
              "0      ...         1056          1360          2072          2283   \n",
              "1      ...          513           851          1239          1840   \n",
              "2      ...          438           646           796           946   \n",
              "3      ...          288           438           795           946   \n",
              "4      ...          288           437           643           855   \n",
              "...    ...          ...           ...           ...           ...   \n",
              "49943  ...          741          1131          1577          1901   \n",
              "49944  ...          879          1206          1357          1507   \n",
              "49945  ...          285           436           586           736   \n",
              "49946  ...          585          1068          1602          1980   \n",
              "49947  ...          466           992          1330          1666   \n",
              "\n",
              "       player_9_300  player_9_360  player_9_420  player_9_480  player_9_540  \\\n",
              "mid                                                                           \n",
              "0              3302          4071          4686          5207          5609   \n",
              "1              2052          2321          3214          3603          4062   \n",
              "2              1168          1660          1810          1959          2218   \n",
              "3              1340          1591          1740          1890          2097   \n",
              "4              1065          1499          1649          1800          2070   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "49943          2549          3051          3922          4183          5088   \n",
              "49944          1967          2244          2516          2666          2816   \n",
              "49945           885          1036          1186          1403          1894   \n",
              "49946          2510          2719          3189          4548          5300   \n",
              "49947          1816          2850          3345          3865          4513   \n",
              "\n",
              "       player_9_600  \n",
              "mid                  \n",
              "0              6384  \n",
              "1              4623  \n",
              "2              2491  \n",
              "3              2247  \n",
              "4              2220  \n",
              "...             ...  \n",
              "49943          5598  \n",
              "49944          3027  \n",
              "49945          2044  \n",
              "49946          5451  \n",
              "49947          4969  \n",
              "\n",
              "[49948 rows x 95 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72e8334",
      "metadata": {
        "id": "c72e8334",
        "outputId": "bdbe42b9-706e-4759-9aa4-045cc7e6b6cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mid</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>21723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>19926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>17017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>19283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>25107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0\n",
              "mid         \n",
              "0      21454\n",
              "1      22165\n",
              "2      21392\n",
              "3      20628\n",
              "4      18038\n",
              "...      ...\n",
              "49943  21723\n",
              "49944  19926\n",
              "49945  17017\n",
              "49946  19283\n",
              "49947  25107\n",
              "\n",
              "[49948 rows x 1 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y.to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275ce661-1c2c-453e-9650-5c6c7c58b4a7",
      "metadata": {
        "id": "275ce661-1c2c-453e-9650-5c6c7c58b4a7"
      },
      "source": [
        "Для оценки качества предсказания будем использовать [sklearn.metrics.mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31e133e-c48c-4a2a-9089-510fc21fdcea",
      "metadata": {
        "id": "f31e133e-c48c-4a2a-9089-510fc21fdcea"
      },
      "source": [
        "**Задание 1** (3 балла)\n",
        "\n",
        "Реализуйте класс `CustomSGDRegressor`, который бы обучал многомерную линейную регрессию минимизируя сумму квадратов ошибки, используя `SGD`.\n",
        "\n",
        "Класс, должен принимать следующие параметры при инициализации:\n",
        "    \n",
        "    learning_rate : float : параметр задающий скорось обучения\n",
        "    fit_intercept : bool : если True, то добавляем свободный член\n",
        "    max_iter : int : максимальное число эпох\n",
        "    shuffle : bool : если True, то перемещиваем данные обучения перед каждой эпохой\n",
        "    \n",
        "и иметь методы `fit` и `predict`.\n",
        "\n",
        "После обучения (запуска метода `fit`) мы должны мочь обратиться к атрибутам класса: \n",
        "    \n",
        "    coef_  : ndarray of shape (n_features,)\n",
        "    intercept_ : ndarray of shape (1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b4def0-fa8d-4fff-abe9-310e27544164",
      "metadata": {
        "id": "e1b4def0-fa8d-4fff-abe9-310e27544164"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CustomSGDRegressor(BaseEstimator):\n",
        "    def __init__(self, learning_rate: float, fit_intercept: bool,\n",
        "                 max_iter: int, shuffle: bool, batch_size: int = 256):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.max_iter = max_iter\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_history = []\n",
        "\n",
        "    def fit(self, x_train: np.ndarray, y_train: np.ndarray):\n",
        "\n",
        "        self.batch_size = min(self.batch_size, len(x_train))\n",
        "        \n",
        "        _x_train = np.array(x_train)\n",
        "        _y_train = np.array(y_train).reshape(-1, 1)\n",
        "\n",
        "        if self.fit_intercept:\n",
        "            _x_train = np.hstack((_x_train, np.ones((len(x_train), 1))))\n",
        "\n",
        "        self.weights = np.random.randn(_x_train.shape[1], 1)\n",
        "\n",
        "        last_ind = 0\n",
        "        for epoch_num in range(1, self.max_iter + 1):\n",
        "            if self.shuffle:\n",
        "                indexes = np.random.randint(low=0, high=len(_x_train), size=self.batch_size)\n",
        "            else:\n",
        "                indexes = np.array([ind % len(_x_train) for ind in range(last_ind, last_ind + self.batch_size)])\n",
        "                last_ind = (last_ind + self.batch_size) % len(_x_train)\n",
        "            x = _x_train[indexes]\n",
        "            y_pred = x @ self.weights\n",
        "            grad = x.T @ (y_pred - _y_train[indexes])\n",
        "            self.weights -= self.learning_rate * grad\n",
        "            self.loss_history.append(mean_squared_error(y_pred, _y_train[indexes]) ** 0.5)\n",
        "        \n",
        "        if self.fit_intercept:\n",
        "            self.coef_ = self.weights[:-1].flatten()\n",
        "            self.intercept_ = float(self.weights[-1])\n",
        "        else:\n",
        "            self.coef_ = self.weight.flatten()\n",
        "            self.intercept_ = None\n",
        "            \n",
        "        return self\n",
        "\n",
        "    def predict(self, x_test: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        _x_test = np.array(x_test)\n",
        "\n",
        "        if self.fit_intercept:\n",
        "            _x_test = np.hstack((_x_test, np.ones((len(x_test), 1))))\n",
        "\n",
        "        predictions = _x_test @ self.weights\n",
        "        return predictions.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96dff5e1-1f7f-415f-b9d6-12d5865cd331",
      "metadata": {
        "id": "96dff5e1-1f7f-415f-b9d6-12d5865cd331"
      },
      "source": [
        "Разбейте `X` и `Y` на `train` и `test`, используя [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).\n",
        "\n",
        "Далее вам нужно подобрать такой `learning_rate`, на котором достигается минимум среднеквадратичная ошибка, \n",
        "\n",
        "а также принять решение о необходимости масштабирования признаков.\n",
        "\n",
        "Постройте соответствующие кривые обучения и сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc75ba43-7fb6-40bf-8703-2d39ba0ce1a8",
      "metadata": {
        "id": "dc75ba43-7fb6-40bf-8703-2d39ba0ce1a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ffebeea",
      "metadata": {
        "id": "0ffebeea"
      },
      "outputs": [],
      "source": [
        "# масштабируем признаки\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a77f3e5",
      "metadata": {
        "id": "9a77f3e5",
        "outputId": "b6e72f1d-d0be-4ec3-856e-bf78e4358f1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': [0.00030000000000000003,\n",
              "  0.0006000000000000001,\n",
              "  0.0009000000000000001,\n",
              "  3.0000000000000004e-05,\n",
              "  6.000000000000001e-05,\n",
              "  9e-05,\n",
              "  3e-06,\n",
              "  6e-06,\n",
              "  9e-06]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# подбираем lr\n",
        "\n",
        "params = dict(learning_rate = [10 ** (-k) * (3 * i) for k in range(4, 7) for i in [1, 2, 3]])\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c059cb",
      "metadata": {
        "id": "82c059cb"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "rmse_history = {}\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "for lr in params['learning_rate']:\n",
        "    regressor = CustomSGDRegressor(learning_rate=lr, fit_intercept=True, max_iter=200, shuffle=True, batch_size=2048)\n",
        "    regressor.fit(x_train, y_train)\n",
        "    y_pred = regressor.predict(x_val)\n",
        "    rmse_history[lr] = mean_squared_error(y_pred, y_val) ** 0.5\n",
        "    \n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "971d174d",
      "metadata": {
        "id": "971d174d",
        "outputId": "a013613b-4aa5-4ae8-c4f3-31d61e302c30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0.00030000000000000003: 2.902406128044245e+139,\n",
              " 0.0006000000000000001: inf,\n",
              " 0.0009000000000000001: inf,\n",
              " 3.0000000000000004e-05: 838.1910722839527,\n",
              " 6.000000000000001e-05: 817.068842249035,\n",
              " 9e-05: 817.1153881684769,\n",
              " 3e-06: 5967.875947177838,\n",
              " 6e-06: 1965.6672481891096,\n",
              " 9e-06: 1052.4560294478633}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rmse_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8f947e",
      "metadata": {
        "id": "7b8f947e",
        "outputId": "c7a0689d-4321-4ee7-cc60-760941aad2b6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7kAAAE9CAYAAADOGaUnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABOWUlEQVR4nO3dd3xV9f3H8dcney9CAoQRRgSZAgFBURH3BFdr60BLa4e17a8tVltbW7W1amurHVqrVrS1VnFRtS40KioKyJ5hQ9gJBLLH/f7+uAcMU0Zuzs3N+/l43Afnfs+557wTv+bez/2e8z3mnENEREREREQkEkT5HUBERERERESkuajIFRERERERkYihIldEREREREQihopcERERERERiRgqckVERERERCRiqMgVERERERGRiBHjd4BQyM7Odvn5+X7HOKTKykqSk5P9jiFtnPqhhAv1RQkH6ocSDtQPJVyEe1+cNWvWNudc+wOti8giNz8/n5kzZ/od45CKiooYPXq03zGkjVM/lHChvijhQP1QwoH6oYSLcO+LZrbmYOt0urKIiIiIiIhEDBW5IiIiIiIiEjFU5IqIiIiIiEjEUJErIiIiIiIiEUNFroiIiIiIiESMkBa5ZpZhZpPNbImZLTazkWaWZWZvmVmx92+mt62Z2YNmttzM5pnZkCb7Ge9tX2xm40OZWURERERERFqvUI/kPgC87pzrAwwCFgO3AFOdcwXAVO85wHlAgfe4AXgIwMyygNuBE4HhwO27C2MRERERERGRpkJW5JpZOnAq8BiAc67OObcDGAtM8jabBIzzlscCT7qg6UCGmXUEzgHecs6VOee2A28B54Yqt4iIiLQtL80u4eTfvsN1r1dy8m/f4aXZJX5HEhGRYxDKkdzuwFbgH2Y228weNbNkINc5t9HbZhOQ6y3nAeuavH6913awdhEREZFj8tLsEm59YT4lO6oBKNlRza0vzFehKyLSisWEeN9DgJucc5+Y2QN8fmoyAM45Z2auOQ5mZjcQPM2Z3NxcioqKmmO3IVNRURH2GSXyqR9KuFBfFL/cWVRFdf3eH0Wq6xu58+W5ZJQX+5RK2jL9PZRw0Zr7YiiL3PXAeufcJ97zyQSL3M1m1tE5t9E7HXmLt74E6NLk9Z29thJg9D7tRfsezDn3CPAIQGFhoRs9evS+m4SVoqIiwj2jRD71QwkX6ovil7LXXz1we41TnxRf6O+hhIvW3BdDdrqyc24TsM7MentNZwCLgCnA7hmSxwMve8tTgGu9WZZHAOXeac1vAGebWaY34dTZXpuIiIjIMemUkXhE7SIiEv5COZILcBPwLzOLA1YC1xMsrJ81swnAGuBL3ravAecDy4Eqb1ucc2Vmdicww9vuDudcWYhzi4iISBtwUq92vPhZCQ2Bz09ZjjL48VnH+ZhKRESORUiLXOfcHKDwAKvOOMC2DrjxIPt5HHi8WcOJiIhIm7aurIpX522kZ04KFTX1lOyoIT0xhvLqBpZuqfA7noiIHKVQj+SKiIiIhB3nHLe8MA8DHr9uGHkZiRQVFXHaaafx61cXc0KXdL8jiojIUVKRKyIiIm3Of2as48Plpdw1rj95Ta6/NTNuu7DvnueVtQ0kx+vjkohIaxLK++SKiIiIhJ2K2gZ+89piRvZox1eHdz3odv+du4HT7iti1bbKFkwnIiLHSkWuiIiItCkp8TE8ft0w7rlsIFFRdtDtBnZOpzEQYMITMyivqm/BhCIicixU5IqIiEibsbtYLczPomu7pENu261dMo9cW8j67dV8+1+zqG8MtEREERE5RipyRUREpE3YsquG0b97lyc/Xn3YrxmWn8Xdlw7goxWl/PylBQRvBiEiIuFMMymIiIhIxHPO8fOXFlBZ18jJvbKP6LWXDe3M6tJKYqI0NiAi0hqoyBUREZGI99r8TbyxcDO3nNeHnu1Tjvj1Pzq7957luoYAcTEqeEVEwpX+QouIiEhEK62o5RcvL2Bg53S+Pqr7Me1rzrodnP67IhZuKG+mdCIi0txU5IqIiEhEm7e+nLrGAPddPoiY6GP76NMpPYGAc0x4Yiabd9Y0U0IREWlOKnJFREQkop3eJ4ePbhlD7w6px7yvnLQEHhs/jJ019XzjyZlU1zU2Q0IREWlOKnJFREQkIpVX1fPa/I0450hNiG22/fbtlMaDVw5mfkk5P3x2DoGAZlwWEQknKnJFREQkIt316iJu+vds1pRWNfu+z+yby8/OPx6AOt0/V0QkrGh2ZREREYk47y3bynOz1nPj6T3Jz04OyTEmjOrO107uTlSUEQg4oqIsJMcREZEjo5FcERERiSi7auq59fl59MpJ4aYxBSE7jpkRFWVsLK9m7F8+5NNVZSE7loiIHD4VuSIiIhJR7nl9CRt31nDv5QNJiI0O+fGSYmOorG3gm0/NZPW2ypAfT0REDk1FroiIiESUkT2y+eGZxzGka2aLHC89KZbHrxuGA742aQblVfUtclwRETkwFbkiIiISUS4Y2JGbzgjdacoHkp+dzMNXD2VdWRU3Pv0Z9ZqMSkTENypyRUREJCLc8/oS/v7+St+OP6JHO35zyQBKK+sor9ZoroiIX1TkioiISKs3c3UZD7+3gnXbm/92QUfiisIuvHzjyWSnxPuaQ0SkLVORKyIiIq1aTX0jN0+eR6f0RH5ybh+/4xAXE0VVXQPf/ucs3l602e84IiJtjopcERERadX++HYxK7dVcs9lA0mOj/E7zh7rtlfxvWdms2jDTr+jiIi0KSpyRUREpNXaVF7DY9NWcuWwLowqyPY7zh5JcTE8Nn4YaQmxfH3SDLbsrPE7kohIm6EiV0RERFqtDukJPHPDCH56wfF+R9lPbloCj44vZHtVPd94ciY19Y1+RxIRaRNU5IqIiEirtKk8ODo6tFsWaQmxPqc5sP556Txw5QmU7KhhTam/k2KJiLQVKnJFRESk1Vm0YSen3vsuL85e73eUL3R2vw68N3E0vTuk+h1FRKRNUJErIiIirUp9Y4CJk+eSlhjD6ONy/I5zWJLjYwgEHPe/uZQXPgv/wlxEpDVTkSsiIiKtyiPvr2Thhp3cObY/mclxfsc5bI3OMWP1dm55fj4zVpf5HUdEJGKpyBUREZFWY/mWXTzwdjHnD+jAeQM6+h3niMRGR/HQ1UPIy0zkm0/NYq2u0RURCQkVuSIiItJqLNm0i+yUOH51cX+/oxyVjKQ4Hr9uGI0Bx9cmzaC8ut7vSCIiEUdFroiIiLQaFw7sxLsTR9M+Nd7vKEete3YyD189lJLt1Xy2drvfcUREIk6M3wFEREREvsjqbZXMKynnooEdiY+J9jvOMRvZsx3TfnI67VJab7EuIhKuVOSKiIhIWAsEHDc/P4/FG3dySq/sVjXZ1KHsLnBfnbeR0sparh2Z728gEZEIoSJXREREwtq/PlnDp6vKuPfygRFT4O7mnOO/czfw5qJNdM5MZEyfXL8jiYi0eromV0RERMLWurIq7v7fEk4pyOaKoZ39jtPszIz7vzyI4zumcdPTs1myaaffkUREWj0VuSIiIhKWnHPc+sJ8DLj70gGYmd+RQiIpLobHxg8jJSGGCU/MZMuuGr8jiYi0aipyRUREJCyZGV89sSt3jO1P58wkv+OEVIf0BB4bP4yyyjpenbfR7zgiIq2arskVERGRsOOcw8w4f0BHv6O0mP556bz5f6fSJSuyC3oRkVDTSK6IiIiEFecc3316Nv/4cJXfUVrc7gJ3QUk5j36w0uc0IiKtU0iLXDNbbWbzzWyOmc302rLM7C0zK/b+zfTazcweNLPlZjbPzIY02c94b/tiMxsfyswiIiLir5fnbODV+RtpDDi/o/jmmRlruevVxbw0u8TvKCIirU5LjOSe7pw7wTlX6D2/BZjqnCsApnrPAc4DCrzHDcBDECyKgduBE4HhwO27C2MRERGJLFt31fLL/y5kSNcMrj+5u99xfPOLC/txYvcsbp48j5mry/yOIyLSqvhxuvJYYJK3PAkY16T9SRc0Hcgws47AOcBbzrky59x24C3g3BbOLCIiIi3gFy8voKqukXsvH0R0VGTOpnw44mKiePjqoeRlJvLNp2axrqzK70giIq1GqItcB7xpZrPM7AavLdc5t3vawE3A7rue5wHrmrx2vdd2sHYRERGJIIs37uT1hZv4wZkF9MpJ8TuO7zKT43hsfCENAcff3l/hdxwRkVbDnAvd9S5mluecKzGzHIIjsDcBU5xzGU222e6cyzSzV4DfOuemee1TgZ8Ao4EE59xdXvvPgWrn3O/2OdYNBE9zJjc3d+gzzzwTsp+rOVRUVJCSojdw8Zf6oYQL9UXZbXV5I11So3wZxQ3XflhSESA3yYhpwyPbbUm49kNpe8K9L55++umzmlwSu5eQ3kLIOVfi/bvFzF4keE3tZjPr6Jzb6J2OvMXbvATo0uTlnb22EoKFbtP2ogMc6xHgEYDCwkI3evTofTcJK0VFRYR7Rol86ocSLtQXZfmWCt9Hb8O9H26rqGXKnA18bVTbvVa5LQj3fihtR2vuiyE7XdnMks0sdfcycDawAJgC7J4heTzwsrc8BbjWm2V5BFDundb8BnC2mWV6E06d7bWJiIhIBHh70WbOvP893lmy2e8oYe2ZT9dyxyuLeKIN3lpJRORIhHIkNxd40cx2H+dp59zrZjYDeNbMJgBrgC95278GnA8sB6qA6wGcc2Vmdicww9vuDuecphkUERGJAOXV9fzspfn06ZDKqF7t/Y4T1r49uhdz1pVzxyuL6JadzOm9c/yOJCISlkJW5DrnVgKDDtBeCpxxgHYH3HiQfT0OPN7cGUVERMRfv351Edsq6nj02mHExfhx04fWIzrKeODKE7ji4Y+56enZPP/tk+jdIdXvWCIiYUfvJiIiIuKL95dt5dmZ67nh1B4M6Jzud5xWITk+hseuKyQpLpo7X1nkdxwRkbAU0omnRERERA5m884aBuSl8/0zCvyO0qp0TE/kyQnDyU1N8DuKiEhY0kiuiIiI+OKKwi68dOPJJMRG+x2l1enTIY3M5DjqGgL8+9O1hPKWkCIirY2KXBEREWlRn64q4+U5JTjnfLkfbiR5Zd4Gbn1hPg9MLfY7iohI2NDpyiIiItJiqusamTh5Ls7BOf06aBT3GF0yOI8Pl5fyx7eL6Z6dzNgT8vyOJCLiO43kioiISIv53ZtLWVNaxT2XDVSB2wzMjN9c2p/h+VlMnDyPWWu2+x1JRMR3KnJFRESkRcxas53HP1zF1SO6MrJnO7/jRIz4mGgevmYoHdMT+NGzc2hoDPgdSUTEVzpdWUREREKuvjHAzZPn0ik9kVvOO97vOBEnKzmOx68bRkOjIyZaYxgi0rapyBUREZGQi42O4sdn9yY9MZaUeH38CIWe7VMAcM7xxsJNnHl8rgpeEWmT9JdPREREQqoxELy9zXkDOnJSr2yf00S+Gau3861/fsZdry72O4qIiC9U5IqIiEjI1DUEuPShj/jXJ2v8jtJmDO+exYRR3Xnio9U89fFqv+OIiLQ4FbkiIiISMg8VrWDuuh3kpCb4HaVN+en5x3NGnxx++d9FvLdsq99xRERalIpcERERCYklm3by53eLGXtCJ87qm+t3nDYlOsp44CuDKchJ4aanP6O8ut7vSCIiLUYzP4iIiEiza2gMMPG5eaQnxnL7Rf38jtMmpcTH8Ph1w1i0YSfpibF+xxERaTEayRUREZFm9+mqMhZsKOeOsf3JSo7zO06b1SkjkTO9UfRZa8qoqW/0OZGISOipyBUREZFmd1KvbN76v9M4f0BHv6MIsK6sii//bTq3PD8P55zfcUREQkpFroiIiDSbxoBj7rodAPTKSfE3jOzRJSuJH5xZwEtzNvDnd5b7HUdEJKRU5IqIiEizeeKj1Yz9y4fM8QpdCR83nt6LSwfn8fu3lvHKvA1+xxERCRkVuSIiItIs1pRWct8bSxjTJ4dBndP9jiP7MDPuvmwAhd0y+dGzc1m1rdLvSCIiIaHZlUVEROSYBQKOnzw/j9ioKH5zyQDMzO9IcgDxMdH87Zqh/HfuBvLbJfkdR0QkJDSSKyIiIsfsX5+uZfrKMm678Hg6pCf4HUcOoV1KPNed3B0zY/W2SipqG/yOJCLSrFTkioiIyDGLMjirby5fKuzidxQ5TBW1DVz+8Ed879+zaQxoxmURiRwqckVEROSYXXViNx65ZqhOU25FUuJj+P6Zx/HOki38+tXFfscREWk2uiZXREREjtqUuRtwznHxoE4qcFuha0Z0Y+XWCh7/cBU92idz9YhufkcSETlmGskVERGRo7J5Zw0/e3E+/5q+FqezXVut2y7oy5g+Odw+ZSEzVpf5HUdE5JipyBUREZEj5pzjZy/Op64hwD2XDyQqSqO4rVV0lPHgVwZzw6k9GJCnWz+JSOunIldERESO2JS5G3h78RZ+fHZvumcn+x1HjlFKfAw/ObcPCbHR7Kypp6yyzu9IIiJHTUWuiIiIHJGdNfX8cspCTuiSwddGdfc7jjSjQMBx9aOf8M2nZlLb0Oh3HBGRo6IiV0RERI5IWkIsd186gPsuH0i0TlOOKFFRxg2n9mDG6u3c+vx8nC62FpFWSLMri4iIyGGrbWgkPiaac/t39DuKhMiFAzuxcmsl97+1jJ45Kdx4ei+/I4mIHBGN5IqIiMhh2V5Zx5jfvccLn633O4qE2E1jejHuhE7c98ZSXl+wye84IiJHRCO5IiIicljueGURm3fW0KdDmt9RJMTMjN9eNpCk+BhO6JLhdxwRkSOiIldERES+0NTFm3lxdgnfO6OAvp1U5LYFCbHR/OaSAQA0Bhzl1fVkJcf5nEpE5IvpdGURERE5pPLqen764nx656byXV2f2Sb9+Lm5fPXv06mobfA7iojIF1KRKyIiIof04fJtlFXWce/lA4mL0UeHtmjc4DyKt1Tw/X/PpjGgGZdFJLzpnUpEREQO6fwBHfng5jEM0rWZbdZpx7Xnlxf1ZeqSLdz92mK/44iIHJKKXBERETmgytoGPl1VBkCH9ASf04jfrhmZz3Un5fPotFU8/clav+OIiByUilwRERE5oHtfX8KVj3zMurIqv6NImLjtguO5bEhn+nRM9TuKiMhBaXZlERER2c+nq8qY9PEarjspny5ZSX7HkTAREx3F7780aM/zitoGUuL1cVJEwkvIR3LNLNrMZpvZK97z7mb2iZktN7P/mFmc1x7vPV/urc9vso9bvfalZnZOqDOLiIi0ZdV1jdw8eS5dshK5+dzefseRMPVQ0QouePADyirr/I4iIrKXljhd+ftA0xkK7gH+4JzrBWwHJnjtE4DtXvsfvO0ws77AlUA/4Fzgr2YW3QK5RURE2qT731rK6tIq7rl0IElxGqWTAzuxRxYby2v41lOzqG1o9DuOiMgeIS1yzawzcAHwqPfcgDHAZG+TScA4b3ms9xxv/Rne9mOBZ5xztc65VcByYHgoc4uIiLRlnTOTmDCqOyf1yvY7ioSxIV0z+d0Vg/h0dRk/fWEBzunWQiISHkL99ewfgZuB3bMTtAN2OOd230l8PZDnLecB6wCccw1mVu5tnwdMb7LPpq8RERGRZjb+pHy/I0grcfGgTqzcWsEf3y6mZ04y3xndy+9IIiKhK3LN7EJgi3NulpmNDtVxmhzvBuAGgNzcXIqKikJ9yGNSUVER9hkl8qkfSrhQXwwPr62sIyMhipM6tc1TlNUPj86gaMeovBjKSlZRVLTe7zitnvqhhIvW3BdD+S52MnCxmZ0PJABpwANAhpnFeKO5nYESb/sSoAuw3sxigHSgtEn7bk1fs4dz7hHgEYDCwkI3evToUPxMzaaoqIhwzyiRT/1QwoX6ov8WlJTz/JsfcsngPEaPHvTFL4hA6odH7/TTP1+uawgQF6O7VB4t9UMJF625L4bsL5Bz7lbnXGfnXD7BiaPecc5dBbwLXO5tNh542Vue4j3HW/+OC17cMQW40pt9uTtQAHwaqtwiIiJtTV1DgB8/N5d2yXH8/IK+fseRVuzNhZs48/732Fhe7XcUEWnD/Pia7SfAD81sOcFrbh/z2h8D2nntPwRuAXDOLQSeBRYBrwM3Ouc0hZ+IiEgzefi9FSzZtItfXzKA9KRYv+NIK9atXTJllXVMeGImlbUNX/wCEZEQaJEi1zlX5Jy70Fte6Zwb7pzr5Zy7wjlX67XXeM97eetXNnn9r51zPZ1zvZ1z/2uJzCIiIm1ByY5q/vROMRcP6sRZfXP9jiOtXO8Oqfz5q4NZsmknP/jPHBoDmnFZRFreFxa5ZnavmaWZWayZTTWzrWZ2dUuEExERkdDKy0jkz18dwu0X6TRlaR6je+dw+0X9eGvRZu59fYnfcUSkDTqckdyznXM7gQuB1UAvYGIoQ4mIiEjo7aqpB+Ccfh1olxLvcxqJJONPyufakd1oDDjdP1dEWtzhFLm7Z2C+AHjOOVcewjwiIiLSAlZsreCk377D6ws2+R1FItSvLu7HbRf2xcwI6LRlEWlBh1PkvmJmS4ChwFQzaw/UhDaWiIiIhEpjwHHz5HlEmTGka4bfcSRCmRkAizbs5LwHPmDl1gqfE4lIW/GFRa5z7hbgJKDQOVcPVAJjQx1MREREQuPJj1cza812fnFhX3LSEvyOIxEuNSGGbRW1fO2JGWyvrPM7joi0AYcz8dQVQL1zrtHMbgP+CXQKeTIRERFpdmtLq7j39aWM7t2eS4fk+R1H2oAuWUk8cu1QNuyo4Vv/nEVdQ8DvSCIS4Q7ndOWfO+d2mdko4EyC97N9KLSxREREJBQ+XrmNuJgo7r50wJ7TSUVCbWi3LO69fCCfrCrjtpfmazIqEQmpmC/ehEbv3wuAR5xzr5rZXSHMJCIiIiHy5WFdObdfR9KTYv2OIm3MuMF5rNxawex1O6htCJAQG+13JBGJUIdT5JaY2d+As4B7zCyewxsBFhERkTCxYUc1a0qrGNmznQpc8c3/nXUcjQFHTHQUzjmdTSAiIXE4xeqXgDeAc5xzO4AsdJ9cERGRVsM5x60vzOfrk2ZQXlXvdxxpw8yMmOgotu6q5apHP2FBie5MKSLN73BmV64CVgDnmNl3gRzn3JshTyYiIiLNYvKs9by3bCs3n9tHo7gSFhyONaVVTJg0g03lujOliDSvw5ld+fvAv4Ac7/FPM7sp1MFERETk2G3eWcOdryxiWH4m14zo5nccEQByUhN4dHwhFTUNfP3JGVTVNfgdSUQiyOGcrjwBONE59wvn3C+AEcA3QhtLREREjpVzjp+9uIDahgD3XDaQqChd/yjh4/iOafzpq4NZtGEnP3hmDoGAZlwWkeZxOEWu8fkMy3jLepcUERFpBUb1asdPzz+eHu1T/I4isp8xfXK57YK+FG+poLSyzu84IhIhDmd25X8An5jZi97zcQTvlSsiIiJhzMy47uTufscQOaTrT87nyuFdSIo7nI+lIiJf7HAmnrofuB4o8x7XO+f+GOJcIiIicgxue2k+L88p8TuGyBcyM5LiYqhtaOTmyXP5eEWp35FEpJU76FdmZpbV5Olq77FnnXOuLHSxRERE5Gi9vmAT/5y+lg5pCX5HETlstQ0BPlu7gzcXbebF75xM9+xkvyOJSCt1qJHcWcBM79/dyzObLIuIiEiY2VFVx89fXkDfjml887SefscROWxpCbE8Pn4YUWZMeGIGO6p0ja6IHJ2DFrnOue7OuR7ev7uXdz/v0ZIhRURE5PDc8coitlfWcd8VA4mNPpz5JUXCR9d2SfztmqGs317Nt//5GXUNAb8jiUgrpHc/ERGRCLGgpJwXPivh26N70q9Tut9xRI7KsPwsfnvZABZsKGfF1gq/44hIK6Rp7ERERCJE/7x0nrh+GCN7tvM7isgxuXRIZ0b3ziErOc7vKCLSCmkkV0REJAJs3VULwOjeOcTHRPucRuTYZSXH4Zzj7++v5M2Fm/yOIyKtyEGLXDMb02S5+z7rLg1lKBERETl8Hy7fxsn3vMNHy7f5HUWkWdU3Ol6Zv5HvPzOHBSXlfscRkVbiUCO5v2uy/Pw+624LQRYRERE5QpW1Dfzk+Xl0zkhkSLdMv+OINKu4mCj+fu1QMpNi+fqkmWzeWeN3JBFpBQ5V5NpBlg/0XERERHxw3xtLKdlRzb2XDyQhVqcpS+TJSU3g0fHD2FVTz9cnzaS6rtHvSCIS5g5V5LqDLB/ouYiIiLSwT1eV8cRHqxk/Mp/C/Cy/44iETN9OaTz4lcEs3riTj1botHwRObRDza7cw8ymEBy13b2M97z7wV8mIiIiLWHe+h3kt0vi5nN7+x1FJOTOOD6Xoomj6ZyZ5HcUEQlzhypyxzZZ/t0+6/Z9LiIiIi3s66f04OoR3XSasrQZuwvcoqVb2F5VxyWDO/ucSETC0UGLXOfce02fm1ks0B8occ5tCXUwERERObD568vZVVvPST2zVeBKm+Oc4x8fruajFdvolJ7IiT10X2gR2duhbiH0sJn185bTgbnAk8BsM/tKC+UTERGRJmobGvnRc3P48bNzqW3QBDzS9pgZD35lMF2zkvjmP2exelul35FEJMwcauKpU5xzC73l64FlzrkBwFDg5pAnExERkf38+Z3lLNtcwa8vGUB8jEZxpW1KT4zl8euGYcDXJs2gvKre70giEkYOVeTWNVk+C3gJwDm3KZSBRERE5MAWlJTz16IVXDokj9P75PgdR8RX3dol87drCllXVsVzs9b5HUdEwsihJp7aYWYXAiXAycAEADOLARJbIJuIiIh46hsD3Dx5HlnJcfziwr5+xxEJC8O7Z/Hfm0bROzfV7ygiEkYOVeR+E3gQ6AD8oMkI7hnAq6EOJiIiIp+LNuPLw7rQKSORjKQ4v+OIhI0+HdIAWLG1gllrtvOlwi4+JxIRvx1qduVlwLkHaH8DeCOUoURERGRvUVHG+JPy/Y4hErb+9t4Knpu1nqykOM7sm+t3HBHx0UGLXDN78FAvdM59r/njiIiISFONAceESTO4YmgXLhjY0e84ImHrVxf3Z8mmXXzvmdk8962R9OuU7nckEfHJoSae+hYwCtgAzARm7fMQERGREHts2kqKlm4l4JzfUUTCWmJcNI9eW0h6YixfnzSTLTtr/I4kIj45VJHbEXgEOAe4BogFXnbOTXLOTWqJcCIiIm3Zyq0V/P7NZZzdN5cLNYor8oVy0hJ4dHwh5dX1PDC12O84IuKTQ12TWwo8DDxsZp2BK4FFZvYT59xTLRVQRESkLQoEHD95fh7xMVHcNa4/ZuZ3JJFWoV+ndJ7+xgj6dNCMyyJt1aFGcgEwsyHA94Grgf9xmKcqm1mCmX1qZnPNbKGZ/cpr725mn5jZcjP7j5nFee3x3vPl3vr8Jvu61WtfambnHMXPKSIi0qq8V7yVGau384uL+pGTluB3HJFW5YQuGSTERlNeXc/zs9b7HUdEWthBi1wzu8PMZgE/BN4DCp1zE5xziw5z37XAGOfcIOAE4FwzGwHcA/zBOdcL2I53/13v3+1e+x+87TCzvgRHkfsRnO35r2YWfWQ/poiISOtyeu8cnvvWSC4bkud3FJFW6/Fpq/jRc3N54TMVuiJtyaFGcm8DMoBBwN3AZ2Y2z8zmm9m8L9qxC6rwnsZ6DweMASZ77ZOAcd7yWO853vozLHhu1ljgGedcrXNuFbAcGH54P56IiEjr4pxjbWkVAMPys3Sassgx+O6YXpzUsx23PD+fGavL/I4jIi3kUEVud4IF6YXe4yLvsXv5C5lZtJnNAbYAbwErgB3OuQZvk/XA7q+o84B1AN76cqBd0/YDvEZERCSiPDNjHWfe/x4LSsr9jiLS6sVGR/HQVUPpnJnIDU/OZE1ppd+RRKQFHGriqTUHajezKOArwAHX77OPRuAEM8sAXgT6HF3ML2ZmNwA3AOTm5lJUVBSqQzWLioqKsM8okU/9UMKF+mJQaXWAX02rpmd6FFuXfUZRsUZxW5L6YeS64fgAd06v5zuPv8/EYYl+xzkk9UMJF625Lx60yDWzNOBGgqOmUwiOxH4X+BEwF/jX4R7EObfDzN4FRgIZZhbjjdZ2Bkq8zUqALsB6M4sB0oHSJu27NX1N02M8QvCWRxQWFrrRo0cfbjxfFBUVEe4ZJfKpH0q4UF8Mnqb8tSdmYFF1/G3CqXRtl+R3pDZH/TCyHTdgO3kZieSG+URu6ocSLlpzXzzU6cpPAb2B+cDXgXeBy4FxzrmxX7RjM2vvjeBiZonAWcDiJvsBGA+87C1P8Z7jrX/HOee89iu92Ze7AwXAp4f7A4qIiLQGL3xWwrtLt3Lzub1V4IqEwJCumeSmJdAYcLw6byPBj5kiEokOOpIL9HDODQAws0eBjUBX51zNYe67IzDJmwk5CnjWOfeKmS0CnjGzu4DZwGPe9o8BT5nZcqCM4IzKOOcWmtmzwCKgAbjROw1aREQkYmwsr2Z49yzGj8z3O4pIRHv+s/XcPHkev7iwL18b1d3vOCISAocqcut3LzjnGs1s/REUuDjn5gGDD9C+kgPMjuzt+4qD7OvXwK8P99giIiKtzXfHFPCt03oSFaXrcEVC6fIhnZm6eDN3vbqI/OwkxvTJ9TuSiDSzQ52uPMjMdnqPXcDA3ctmtrOlAoqIiESyqYs38+HybQDERB/qbVlEmkNUlPGHL59A305p3PT0bBZv1MdakUhz0HdT51y0cy7Ne6Q652KaLKe1ZEgREZFIVFpRy82T53HvG0t1faBIC0qKi+HRa4eRkhDDjU9/RmNA//+JRJJDna4sIiIiIfTL/y5iZ0099142EDOdpizSkjqkJ/DY+GE0BBzRukxAJKLovCgREREfvLFwE/+du4GbxhTQu0Oq33FE2qT+eemc0CUDgI+WbyOgEV2RiKAiV0REpIWVV9dz20sLOL5jGt8e3dPvOCJt3vSVpXz10U/449vL/I4iIs1ApyuLiIi0sNT4GG4a04shXTOJ1WRTIr47sXsWXyrszIPvLKdH+xTGDc7zO5KIHAMVuSIiIi0oEHBERRnX6n64ImHDzLhr3ADWllVx8+R5dM5MpDA/y+9YInKU9PWxiIhIC9lVU8/5D37Amws3+R1FRPYRFxPFw1cPJS8zkW8+NYudNfV+RxKRo6SRXBERkRZy9/+WsGzzLtqnxvsdRUQOICMpjsfGFzK/pJy0hFi/44jIUVKRKyIi0gI+Wr6Npz9ZyzdO6c7grpl+xxGRg+jRPoUe7VMAWLxxJwU5KcTo2nmRVkX/x4qIiIRYVV0DP3lhHvntkvjhWb39jiMih2FtaRVj//Ihv/rvIpzTrYVEWhMVuSIiIiH2v/mbWFdWzT2XDSQxLtrvOCJyGLq2S+L6k/J5avoaJn202u84InIEdLqyiIhIiF02tDP98tLo0yHN7ygicgRuPrcPK7dVcscri+iWnczpvXP8jiQih0EjuSIiIiFSU99I8eZdACpwRVqh6CjjgStP4PiOadz09GzWllb5HUlEDoOKXBERkRD549vFXPDgNNZv1wdjkdYqKS6GR8cX8s1Te9A5M9HvOCJyGFTkioiIhMDcdTt45P0VXDokj86ZSX7HEZFj0DE9kZvOKCAqythYXk1NfaPfkUTkEFTkioiINLPahkYmTp5LTmoCP73geL/jiEgzqahtYNxfPmTi5HmacVkkjKnIFRERaWZ/eXcFyzZX8JtL+5OWEOt3HBFpJinxMVx3Unf+O3cDf3y72O84InIQml1ZRESkmTnnuHxoZ8b0yfU7iog0s2+d1oMVWyt4YGoxPdonM/aEPL8jicg+VOSKiIg0sx+d3VunMopEKDPjN5cMYG1ZFRMnzyO/XTKDumT4HUtEmtDpyiIiIs3kX5+s4cPl24DgB2ERiUxxMVH87eqhXDG0Mz3aJ/sdR0T2oSJXRESkGRRv3sWvpizi6U/X+h1FRFpAZnIcv75kAKkJsVTXNbKrpt7vSCLiUZErIiJyjBoDjomT55EcH82vLu7ndxwRaUGNAcdVj07nu0/PpqEx4HccEUFFroiIyDH7x4ermLNuB7+8uB/ZKfF+xxGRFhQdZVxR2IX3lm3lrlcX+x1HRNDEUyIiIsdk/fYq7ntjKWcen8vFgzr5HUdEfPCV4V1ZubWCv3+wih7tk7l2ZL7fkUTaNBW5IiIix6BTeiK3XdiXs/vmarIpkTbslvOOZ9W2Kn45ZSHd2iVz2nHt/Y4k0mbpdGUREZGjVN8YICrKuGZEN3LTEvyOIyI+io4yHrjyBM4f0JH8dkl+xxFp01TkioiIHIV1ZVWceu+7vL9sq99RRCRMJMfH8OevDqFbu2Scc1TUNvgdSaRNUpErIiJyhJxz3PrCfHZW1+semSJyQLe+MJ/xj39KTX2j31FE2hwVuSIiIkfo2ZnrmLZ8G7ecfzydM3Vaoojs79Tj2jNrzXZ+8vw8nHN+xxFpU1TkioiIHIFN5TXc9cpiRvTI4qrhXf2OIyJh6vwBHZl4Tm9enrOBP72z3O84Im2KZlcWERE5Aq/O30h9IMA9lw0kKkqzKYvIwX1ndE9WbK3g/reW0T07mYt0mzGRFqEiV0RE5AhMGNWdc/rl6jRlEflCZsbdlw5gZ3UDWclxfscRaTNU5IqIiByGLbtq2F5ZT+8OqSpwReSwxcdE8+j4wj3PaxsaiY+J9jGRSOTTNbkiIiJfwDnHz19awGUPfcTOmnq/44hIK/WPD1cx7i8fsUt/R0RCSkWuiIjIF3ht/ibeWLiZG0/vRVpCrN9xRKSVKshJZdnmXXzv37NpDGjGZZFQUZErIiJyCGWVdfzi5QUMyEvnG6d09zuOiLRiowqy+dXF/Xh36VbuenWR33FEIpauyRURETmEX/13ITtr6vnXFScSE63vhkXk2Fw9ohsrt1by+Ier6NE+hWtGdPM7kkjEUZErIiJyEIGAo1NGIt8bU0CfDml+xxGRCPGzC45nbVkVAZ2yLBISIStyzawL8CSQCzjgEefcA2aWBfwHyAdWA19yzm03MwMeAM4HqoDrnHOfefsaD9zm7fou59ykUOUWERHZLSrK+Mm5ffyOISIRJjrK+Pu1Qwl+/A1+oab7bos0n1Ced9UA/Mg51xcYAdxoZn2BW4CpzrkCYKr3HOA8oMB73AA8BOAVxbcDJwLDgdvNLDOEuUVERPj9m0uZVrzN7xgiEqF2F7jvLdvKBX+aRmlFrc+JRCJHyEZynXMbgY3e8i4zWwzkAWOB0d5mk4Ai4Cde+5POOQdMN7MMM+vobfuWc64MwMzeAs4F/h2q7CIi0ra9v2wrf3pnOY2jHaMKsv2OIyIRLD0xlpVbK7j8oY+obQiwobyGvOnvMPGc3owbnOd3PJFWqUVm0DCzfGAw8AmQ6xXAAJsIns4MwQJ4XZOXrffaDtYuIiLS7CpqG7j1hfn0bJ/M984o8DuOiES4E7pkcOWwLqwqrWJDeQ0AJTuqufWF+bw0u8TndCKtU8gnnjKzFOB54AfOuZ27T80AcM45M2uWK+7N7AaCpzmTm5tLUVFRc+w2ZCoqKsI+o0Q+9UMJF+HUF59cWMuGHQ387MQEpn/4gd9xpAWFUz+UtuWV2VX7tVXXN3Lny3PJKC/2IZFI6/6bGNIi18xiCRa4/3LOveA1bzazjs65jd7pyFu89hKgS5OXd/baSvj89Obd7UX7Hss59wjwCEBhYaEbPXr0vpuElaKiIsI9o0Q+9UMJF+HSF+et38E7r3/IhFHd+fqFff2OIy0sXPqhtD1lr796wPbSGod16sfw/CwS46JbOJW0da35b2IoZ1c24DFgsXPu/iarpgDjgd96/77cpP27ZvYMwUmmyr1C+A3gN00mmzobuDVUuUVEpO3q3ymdey8fyEUDO/kdRUTakE4ZiZTsqD7guvGPf0pcdBSF+ZmMKshm7Al55GUktnBCkdYllNfkngxcA4wxszne43yCxe1ZZlYMnOk9B3gNWAksB/4OfAfAm3DqTmCG97hj9yRUIiIizaWytoGoKONLhV00YiIiLWriOb1JjN37705ibDT3XjaQJ64fxrUju1FWWce9ry9l1dZKAJZu2sUzn65l/fb9T3UWaetCObvyNOBgN/w64wDbO+DGg+zrceDx5ksnIiLyuVlrypgwaSaPjR/G0G66S52ItKzdsyjf98ZSSnZUk5eRuNfsyqN75wCwZVcNGYlxALy5cBO/f2sZAN2zkxnVK5tRBdmM6ZNDbHSLzC0rErZCPvGUiIhIOKupb2Ti5Hkkx8XQu0Oq33FEpI0aNziPcYPzDnkdZE5qwp7l747pxTn9O/BB8TamFW9l8qz1vDi7hNm/OAsI3n83OS6aQV0yVPRKm6MiV0RE2rQHphazcmslT00YTkq83hZFpHUwM47LTeW43FQmjOpOXUOAVdsq9xS0d7+2mCWbdpESH8OIHu049bhsTi1oT352ss/JRUJPX+uIiEibNW/9Dh55fyVfLuzCKQXt/Y4jInLU4mKi9job5ZkbRvDXq4Zw0aBOLN28k1+8vJDfvbl0z/o3F26irLLOj6giIaevrEVEpM16e/EWslPi+OkFx/sdRUSkWWUkxXH+gI6cP6AjAGtKK6lvdACsLa3ihqdmYQb9OqUxqld7TinIZmi3TBJiNfGetH4ayRURkTbrh2cdx/++fyrpibF+RxERCalu7ZLplZMCQF5mIi9+5yR+eOZxJMXF8OgHK7nq0U94Y+EmALbuqmXRhp0E54UVaX00kisiIm1O8eZdNDpHnw5pZCXH+R1HRKRFRUcZg7tmMrhrJjedUUBFbQOfrCzdM7v8lLkbuPOVRWSnxHmzNrdnVK9sOqQnfMGeRcKDilwREWlTGhoD/N+zcyitqOO9iacTF6OTmkSkbUuJj+GM43P3PL9oUEfSEmKYtnwb05Zv46U5G4iOMubefjYp8TGsLa2iXUocyZqsT8KUeqaIiLQpj3ywkgUlO/nrVUNU4IqIHEBOagJXFHbhisIuBAKOJZt2sWTTzj0z0P/k+XnMXFPG4K6ZnOLdn3dg5wyio8zn5CJBencXEZE2Y/mWXfzx7WLO699hz2QsIiJycFFRRt9OaVw6pPOetu+dUcDXRnWnsraB37+1jEv++hE3PDlzz/rNO2v8iCqyh0ZyRUSkTWgMOG6ePI+kuGjuGNvf7zgiIq3WyJ7tGNmzHZwHpRW1fLiilFRvlLe8up6Rd0+lc2YSowqyOaVXNif1zCY9SRP8SctRkSsiIm1CY8Axsmc7xp+UT/vUeL/jiIhEhHYp8Vw8qNOe51EGt1/Ujw+KtzFlzgae/mQtUQb3f+kExg3Oo7ahEcN0uYiElIpcERFpE+Jioph4Th+/Y4iIRLTUhFjGn5TP+JPyqW8MMHfdDj4o3sYJXTIAeH3BJm59YT4jerRjVK9sTinIpldOCma6nleaj4pcERGJaIGA48fPzeWyoZ05uVe233FERNqM2OgoCvOzKMzP2tPWIzuFy4Z0ZtrybbyzZAsAuWnxvPGDU8lIiqOhMUBMtEZ55dioyBURkYj2r0/W8MLsEkb0aOd3FBGRNm9A53QGdE4HYF1ZFdOWb2PRhp1kJAXvWf5/z86lePMuTikI3p93eH4WiXHRfkaWVkhFroiIRKx1ZVXc/b8lnFKQzRWFnb/4BSIi0mK6ZCXxleFd92obnp/Jtl21TPpoDX//YBVx0VFcNrQzd186AADnnE5tli+kIldERCKSc46fvjgfA+6+dIA+FImItALXjMznmpH5VNU18OmqMqYVb6NDegIA9Y0Bzvj9ewzsnL5npDcvI9HnxBKOVOSKiEhEenvxFj4o3sadY/vROTPJ7zgiInIEkuJiGN07h9G9c/a0VdQ0UJifybTibbwybyMAPbKT+en5x3Nm31y/okoYUpErIiIR6Yw+OTx01RDO6dfB7ygiItIMMpPjuP9LJ+Cco3hLBR8Ub2Na8VYyvHvwfrRiG/e/uSx4f96CbAZ1ztAkVm2UilwREYkozjm2V9WTlRzHeQM6+h1HRESamZlxXG4qx+WmMmFU9z3t9Y2O+sYAD0wt5o9vF5MaH8OInu2457KBZCXH+ZhYWpqKXBERiShT5m7gtpcWMPlbJ9G7Q6rfcUREpIWcdlx7TjuuPTuq6vhoRSkfFG9j7rodpCcGR3r/NLWY9durGVWQzcm9slX4RjAVuSIiEjG27qrl9ikL6ZWTQq+cFL/jiIiIDzKS4jh/QEfO3+dsnu1V9by2YCP/mbkOM+jXKY2LBnbim6f19CmphIqKXBERiRi/nLKQqtpG7rt8INFRmk1ZREQ+94uL+vLT8/swr6ScacXbmFa8jRVbK4DgpS4/fHYufTumMaogmz4dUjUrfyumIldERCLC/+Zv5NX5G5l4Tm965eg0ZRER2V9MdBRDumYypGsm3zujAOccAOXV9cwvKefF2SUAZKfEM6pXO64Z2Y2h3bL8jCxHQUWuiIhEhJlrttM/L40bTu3hdxQREWkldo/WZiTF8fYPT2NjebU3a/M2Pijexll9gzP0L9+yi399spZTC9pzYo8skuJURoUz/dcREZGI8PML+1JZ20CsbhchIiJHqWN6Il8q7MKXCrsQCDgC3kjvkk27ePqTtfzjw9XERhtDumZySkE214zIJ927hZGEDxW5IiLSqk1fWUpGUix9OqSRHK+3NRERaR5RUUYUwZHeCwd24szjc5m5ejsfLN/KtOJtPDh1OdedHLyF0f/mb6Ssqo5TerWna7skP2MLKnJFRKQVK6+u5/vPzCYnNYEp3z1Zk4SIiEjIJMRGM6ogm1EF2XAe7KypJ8X7cvXlORt4feEmALpmJTGqIJsxvXM4s2+un5HbLBW5IiLSav3m1cVsq6jj0WuHqcAVEZEWlZbw+WnKD109hJXbKvlg2VamLd/Gy7NLWFdWtafIffLj1fTOTWVw10ziYnRZTaipyBURkVbpg+Kt/GfmOr49uicDOqf7HUdERNowM6Nn+xR6tk/hupO7U98YYHtlHQC7auq585VF1Dc6kuKiGdGjHaN6ZXNW31y6ZOnU5lBQkSsiIq1ORW0Dtzw/nx7tk/n+GQV+xxEREdlLbHQUOWkJAKQmxDLztrP4eEUp07zred9ZsoWYaOPakflsq6hlWvE2Tu6VTfvUeJ+TRwYVuSIi0urERhvjBndiTJ8cEmKj/Y4jIiJySOmJsZzbvwPn9g/ekmhdWRWpCcFS7P1lW/nhs3MBOL5jGqcUZDOqVzYn9sgiPkbvcUdDRa6IiLQ68THRTDynj98xREREjkrT05THnpBHr5yUPffnfeLD1Tzy/ko+umUMnTISWbihHOegb8c0oqI0/8ThUJErIiKtRnVdI19/cgY3jSlgRI92fscRERE5ZtFRxsDOGQzsnMGNp/eiqq6BOet20CkjEYA/TV3O6ws3kZUcx8m9sjmlV3CG593rZX+a2ktERFqN37+5lA+XlxJwzu8oIiIiIZEUF8NJPbP3PL9jXD/+8OVBjD6uPdNXlnLz8/P4+qSZe9Z/tnY7u2rq/YgatjSSKyIircJna7fz2IeruOrErnu9+YuIiESynNQELhncmUsGd8Y5x7LNFZRXB4va2oZGrvr7J9Q1BhjcJYNRBdmcUpDNoM4ZxES33fHMtvuTi4hIq1FT38jNk+fRKT2RW88/3u84IiIivjAzendIZXj3LACizXjsukK+eWoP6hoDPDC1mMse+pi/vLsCCL5/rtxagWtjZ0BpJFdERMLeC5+VsHxLBZO+NpyUeL11iYiIAMRER3FSz2xO6pnNzcD2yjo+WlFKn46pAHy8spTr/zGDvIxERnnX8p7cK5us5Dh/g4eYPimIiEjYu3JYF/Kzk3SasoiIyCFkJsdxwcCOe57365jGneP6M614K68t2Mh/Zq7DDN78wakU5KZSVllHcnz0Xrcqeml2Cfe9sZSSHdXkTX+Hief0ZtzgPD9+nKOmIldERMJWXUOA7VV15KYlqMAVERE5QjlpCVwzohvXjOhGQ2OAeSXlTF9ZSs/2KUBwQsfnP1vP8O7tOKVXNvWBAH+aWkx1fQCAkh3V3PrCfIBWVeiG7JpcM3vczLaY2YImbVlm9paZFXv/ZnrtZmYPmtlyM5tnZkOavGa8t32xmY0PVV4REQk/DxWt4Mz732NTeY3fUURERFq1mOgohnTN5Duje+253+6FAztx5bCulGyv4tevLebe15fuKXB3q65v5L43lvoR+aiFcuKpJ4Bz92m7BZjqnCsApnrPAc4DCrzHDcBDECyKgduBE4HhwO27C2MREYlsSzbt5M/vFjOmTw4d0hP8jiMiIhJxRvZsxy8v7sfUH43m41vHHHS7DTuqWzDVsQtZkeucex8o26d5LDDJW54EjGvS/qQLmg5kmFlH4BzgLedcmXNuO/AW+xfOIiISYRoaA0x8bh5pCbHcflE/v+OIiIhEvI7pieRlJB5wXaeDtIerlr6FUK5zbqO3vAnI9ZbzgHVNtlvvtR2sXUREItjfP1jF/JJy7hjbP+JngBQREQkXE8/pTWJs9F5tibHRTDynt0+Jjo5vE08555yZNdsNm8zsBoKnOpObm0tRUVFz7TokKioqwj6jRD71QwkX+/bFTxbUUpgbTXLZUoqKWtd1QNJ66W+ihAP1Q/FTBnDN8dE8vyxAaU2AdglRXHZcNBnlxRQVFfsd77C1dJG72cw6Ouc2eqcjb/HaS4AuTbbr7LWVAKP3aS860I6dc48AjwAUFha60aNHH2izsFFUVES4Z5TIp34o4WLfvjh6NNQ3BoiNbukTjqQt099ECQfqh+K30cBPad19saU/PUwBds+QPB54uUn7td4syyOAcu+05jeAs80s05tw6myvTUREItB/525g8cadACpwRURE5KiE8hZC/wY+Bnqb2XozmwD8FjjLzIqBM73nAK8BK4HlwN+B7wA458qAO4EZ3uMOr01ERCLMmtJKJk6ey/1vLfM7ioiIiLRiITtd2Tn3lYOsOuMA2zrgxoPs53Hg8WaM5quXZpdw3xtLKdlRTd70d5h4Tu9WdWNlEZFQCDjHLc/PJzYqijvGajZlEREROXq+TTzVFr00u4RbX5hPdX0jACU7qrn1hfkAKnSlRenLFgkXTfsiVPHlws50TG9dtykQERGR8KIitwXd98bSPQXubtX1jdz5yiLSE2Pp2T6Fru2SqK5rZOaaMqLMMAPDiDLonp1MTloCVXUNLNm0iygLthvB7bpkJpGeFEtVXQMl26uDrzXDgCgzctLiSYqLobqukbKquj3twe0gIzGOuJgoahsaqakLgEGUfb5NQkw0UVFGIOAIOIftPr6ZL79POTr6skXCxb59EWDK3A2M7JmtvigiIiJHTUVuC9qwo/qA7aWVdVz/xAxuPa8P3zytJ5t21nDNY5/ut91d4/pz9YhurNxayaV//Wi/9X/88gmMG5zHvPXlXPnI9P3W//3aQs7qm8tHK7YxYdLM/dY//Y0TOalnNq8v2MT3n5mz3/qXbzyZQV0yeGbGOn764vy91pnBW/93Gr1yUnh82irueX3JXgW6mTH1R6eRm5bA395bwd/eX0mUAXvWw9QfjSYlPoa/vLucZ2asDRbXBItsDN7+v9OIijIenFrMa/M37im+o8xIjI3m2W+NBOAPby1j2vJte30BkJkUx8PXDN2zfva6Hd764Otz0xP4zSUD9qwv3rLLK+KDGbpkJTLxnD7B3/Pby1i/vXqvLwB6ZKfwjVN7APCnqcWUVtbtyWZAQW4KXx7WFYC/Fi2nqrZxry8h+nRI5bwBHQF49IOV1DUG9vr5+3RM5ZSC9gBM+mi197tlT8Y+HVMZ0jWT+sYAL80u2et3Ywa9O6TSp0MaNfWN3PHfhQf8suW+N5Z6/aN0v//2fTqk0iUrifLqej5dtf9l8f3z0uiYnkhZZR2z1mzfb/2gLunkpCawZVcNc9eV77d+aLdMspLj2FhezYKSnfutH949i/TEWNZvr2Lxxl37rT+pZzuS42NYU1rJss0V+60/pSCbhNhoVmytYOXWyv3Wj+7dntjoKJZt3sWa0qr91p95fA5mxqINO70Rx89FR8GYPsFbfs9fX86mnTV7rY+LieK044L/7Wav3c62irq91ifFRXNyr2wAZq4uY3tV/V7rUxNiGNGjHQDTV5ayq6Zhr/WZSbEU5mcB8OHybVTV7f3fNjsljsFdMwF4f9lWahsCe63vkJbAgM7pALy7ZAsNgb3v7NYpI4F+nYLr31q0eb/fTdesJHp3SKWhMcA7S7bst75H+xR65aRQU9/I+8u27rXujlcWHaAvBrjvjaUqckVEROSoqchtQZ0yEvf7gAzQPiWev48vpGN6AgAd0xN47lsjcS54nVrAOXDQvX0yAF3bJfGP64fBnvXgnKN/XvCDaK+cFP781cF72oMvd/TrlAbA8R3TuPeygQScwxHch3PBQg2gf146v7iwb/C4fH6M3fkGdk7nR2cdF9w/nx8/MykWgH6d0rju5Pzgcb19BxwkxgVvLH1cbioXDOi4Z7/gCAQgJlj10jkzkWHdsj7/2bwMuweMs5Lj6JqV9PnPB8RGfz6aHBcTRUJs1J7f3+5/d6uqa6C8uh727N/tNRpdsqOaZZsrmmR3exUO89aXs2Tjzj25Ag4Gd6nbU+S+vXgzq7ZVer/34DanHdd+T5H7r+lr2bSzZk82gIsGddpT5P7hrWVU7lOofGV4lz1F7u1TFu7XhyaM6s6QrpnUNgSYOHnefuu/d0YBfTqksbO6nrJ9iqjdNuyoZsuuWr7x5P5fgPz6kv5cdWI31pVVHXD97i9Ylm3edcD1wS9YEpi/vvyA63d/wfLpqrIDfsEy5bsnM7BzBh8Ub9sz6tzU1B+dRs/2Kby1aDN3vbp4v/XTbz2DDunR/HfuBv749v73eJv/y7OJjY7iuZnr+PsHq/Zbv+ru8wF4avoa/v3p2r3WJcVFs+iOcwF4dNpKXp6zYa/17VPjmfGzMwH4y7vLeXvx3oVgfrskiiaeDsDv3lzK9JV7f4nQr1Mar37vFAB+/epi5pfs/SXB8O5ZPPvN4Bc8P39pASu37V3Ej+mTw+PXDQNg4uS5bN5Zu9f6iwZ14k9fGQzATf+eTUXt3kX0lcO68NvLBgIc8L/dhFHd+fmFfaltCHDDU7P2W/+9Mwr44VnHsbO6/oDrD+RgXwiKiIiIHA5zTT78R4rCwkI3c+b+H8b8dqBT8xJjo7n70gEatWjDdhfTUV6RX+P1jz1FOsEvABJig18SlFXW4ZoU6M5BQmw06YmxBAKOkh3Ve7024BwZibG0S4mnvjHAyb99hy27avfLkZeRyNQfncbyLfuPhHZMT6BdSjzVdY2s2Lr/+s6ZiWQkxVFR28DqbfuPlHbJSiI9MZadNfWsPcBIaX52MinxMeyoqmP99v0LnB7tk0mKi6Gssu6ABVCvnBQSYqPZuquWzfuMpELwi5W4mCi27Kw54M9+fMc0oqOMjeXVlO4z0grBQtPMKNlRzfbKvdebsWekc11ZVfALlCaio4zjOwa/YFpTWrnfSGxcTBTH5aYCsHJrxX4jsQmxUfTKCa5fvqViT//YLSkumh7tg19QLdu8i7p9RmpT4mPIzw5+QbZk004aGvf+m5+eGEuXrCQAFm4oZ9+3hIykWDpnBtcvKNl/FL5dShwd0xNpDLg9t/5pKic1npy0BOobAyzdtPco/PVPzGDrQfrih7eM2a9dJNRa8z0hJXKoH0q4CPe+aGaznHOFB1ynIrdl7TXhT0aiJvyRFqcvWyRcqC9KuAn3D3TSNqgfSrgI9754qCJXpyu3sHGD8xg3OC/sO41Ert3Fg75sEb+pL4qIiEgoqMgVaYP0ZYuEC/VFERERaW5RfgcQERERERERaS4qckVERERERCRiqMgVERERERGRiKEiV0RERERERCKGilwRERERERGJGCpyRUREREREJGKoyBUREREREZGIoSJXREREREREIoY55/zO0OzMbCuwxu8cXyAb2OZ3CGnz1A8lXKgvSjhQP5RwoH4o4SLc+2I351z7A62IyCK3NTCzmc65Qr9zSNumfijhQn1RwoH6oYQD9UMJF625L+p0ZREREREREYkYKnJFREREREQkYqjI9c8jfgcQQf1Qwof6ooQD9UMJB+qHEi5abV/UNbkiIiIiIiISMTSSKyIiIiIiIhFDRe4XMLNzzWypmS03s1sOsD7ezP7jrf/EzPKbrLvVa19qZud80T7NrLu3j+XePuO89m+Z2Xwzm2Nm08ys7xcdQyJLOPRDb92XzGyRmS00s6ebtI83s2LvMT4kvwQJC+HQF82sm5lNNbN5ZlZkZp2bvEZ9sQ0Ik354wGOY2XDv/XqOmc01s0tC+bsQ/4RzP/TWDTSzj7337PlmlhCq34X4K5z7opnFmdk/vD4418xGh+430YRzTo+DPIBoYAXQA4gD5gJ999nmO8DD3vKVwH+85b7e9vFAd28/0YfaJ/AscKW3/DDwbW85rcnxLgZeP9Qx/P696RGx/bAAmA1kes9zvH+zgJXev5necqbfvzc9IrovPgeM95bHAE+pL7adRxj1w4MdIwmI8ZY7Alt2P9cjch6toB/GAPOAQd7zdugzYkQ+WkFfvBH4h7ecA8wCokL9e9FI7qENB5Y751Y65+qAZ4Cx+2wzFpjkLU8GzjAz89qfcc7VOudWAcu9/R1wn95rxnj7wNvnOADn3M4mx0sGdl9IfbBjSGQJi34IfAP4i3NuO4BzbovXfg7wlnOuzFv3FnBu8/34EkbCpS/2Bd7xlt9tkkF9sW0Il354wGM456qccw1eewKfv2dLZAnrfgicDcxzzs0FcM6VOucam+/HlzAS7n1xz3u299lxBxDye++qyD20PGBdk+frvbYDbuO9qZUT/LbsYK89WHs7YEeTN8a9jmVmN5rZCuBe4HtHkE9av3Dph8cBx5nZh2Y23cx2Fw/qh21HuPTFucCl3vIlQKqZHeoYElnCpR8e7BiY2YlmthCYD3yryeslcoR7PzwOcGb2hpl9ZmY3H/VPKuEu3PviXOBiM4sxs+7AUKDLUf6sh01FbivhnPuLc64n8BPgNr/zSJsUQ/CU5dHAV4C/m1mGn4GkzfoxcJqZzQZOA0oAjVBI2HDOfeKc6wcMA27VtZDigxhgFHCV9+8lZnaGv5GkjXqcYDE8E/gj8BEt8J6tIvfQStj7m4bOXtsBtzGzGCAdKD3Eaw/WXgpkePs42LEgeLrAuCPIJ61fuPTD9cAU51y9d0rLMoJFr/ph2xEWfdE5t8E5d6lzbjDwM69tx2Hmk9YvLPrhIY6xh3NuMVAB9D/Cn1HCX7j3w/XA+865bc65KuA1YMhR/qwS3sK6LzrnGpxz/+ecO8E5NxbIIPgZMqRU5B7aDKDAm0UsjuBF1FP22WYKsHsGz8uBd1zwyuopwJXeTGPdCRYDnx5sn95r3vX2gbfPlwHMrKDJ8S4Aipsc+0DHkMgSFv0QeIngKC5mlk3wVKiVwBvA2WaWaWaZBK8DeqMZf34JH2HRF80s28x2v3/dSvBbYlBfbCvCoh8e7BjePmIgOBM40AdY3Xw/voSJsO6HBP/2DTCzJK8/ngYsasafX8JHWPdFrw8mA5jZWUCDcy70ffFoZqtqSw/gfILfNqwAfua13QFc7C0nEJzpcznBTtGjyWt/5r1uKXDeofbptffw9rHc22e81/4AsBCYQ7Bj9fuiY+gRWY8w6YcG3E/wTXI+3sx63rqvedsvB673+/elR8T3xcsJftm3DHh0d7u3Tn2xDTzCpB8e8BjANXz+nv0ZMM7v35ceba8feuuu9vriAuBev39ferTNvgjke/teDLwNdGuJ34l5BxcRERERERFp9XS6soiIiIiIiEQMFbkiIiIiIiISMVTkioiIiIiISMRQkSsiIiIiIiIRQ0WuiIiIiIiIRAwVuSIiIs3AzCpa4BjfMrNrQ32cfY45zsz6tuQxRUREjoVuISQiItIMzKzCOZfSDPuJds41Nkem5jimmT0BvOKcm9ySmURERI6WRnJFRESamZlNNLMZZjbPzH7VpP0lM5tlZgvN7IYm7RVm9nszmwuM9J7/2szmmtl0M8v1tvulmf3YWy4ys3vM7FMzW2Zmp3jtSWb2rJktMrMXzewTMys8QMbV3us/A64ws294meea2fPefk4CLgbuM7M5ZtbTe7zu/RwfmFmf0P42RUREjoyKXBERkWZkZmcDBcBw4ARgqJmd6q3+mnNuKFAIfM/M2nntycAnzrlBzrlp3vPpzrlBwPvANw5yuBjn3HDgB8DtXtt3gO3Oub7Az4Ghh4hb6pwb4px7BnjBOTfMO+ZiYIJz7iNgCjDROXeCc24F8Ahwk/dz/Bj46+H/dkREREIvxu8AIiIiEeZs7zHbe55CsOh9n2Bhe4nX3sVrLwUageeb7KMOeMVbngWcdZBjvdBkm3xveRTwAIBzboGZzTtE1v80We5vZncBGV7mN/bd2MxSgJOA58xsd3P8IfYvIiLS4lTkioiINC8D7nbO/W2vRrPRwJnASOdclZkVAQne6pp9romtd59PmtHIwd+vaw9jm0OpbLL8BDDOOTfXzK4DRh9g+yhgh3PuhKM4loiISIvQ6coiIiLN6w3ga96oJ2aWZ2Y5QDrB04irvOtYR4To+B8CX/KO3RcYcJivSwU2mlkscFWT9l3eOpxzO4FVZnaFt38zs0HNFVxERKQ5qMgVERFpRs65N4GngY/NbD4wmWCR+DoQY2aLgd8C00MU4a9AezNbBNwFLATKD+N1Pwc+IVgkL2nS/gww0cxmm1lPggXwBG+SrIXA2OYMLyIicqx0CyEREZEIYmbRQKxzrsYrSt8Gejvn6nyOJiIi0iJ0Ta6IiEhkSQLe9U47NuA7KnBFRKQt0UiuiIiIiIiIRAxdkysiIiIiIiIRQ0WuiIiIiIiIRAwVuSIiIiIiIhIxVOSKiIiIiIhIxFCRKyIiIiIiIhFDRa6IiIiIiIhEjP8HbTrs02v6EMIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "VERY_BIG_NUMBER = 1e10\n",
        "\n",
        "to_plot = dict()\n",
        "for key, value in rmse_history.items():\n",
        "    if value < VERY_BIG_NUMBER:\n",
        "        to_plot[f'{key:.6f}'] = value\n",
        "\n",
        "plt.plot(to_plot.keys(), to_plot.values(), '--o')\n",
        "plt.ylabel('RMSE loss')\n",
        "plt.xlabel('learning rate')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a27dd9a7",
      "metadata": {
        "id": "a27dd9a7"
      },
      "source": [
        "###### Видим, что адекватные значения получаются только при learning_rate порядка $10^{-5}$. Найдём самый оптимальный среди них:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387f6f91",
      "metadata": {
        "id": "387f6f91",
        "outputId": "a60c2855-6626-4d4b-b124-79efa4d0a6f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': [1e-05,\n",
              "  2e-05,\n",
              "  3.0000000000000004e-05,\n",
              "  4e-05,\n",
              "  5e-05,\n",
              "  6.000000000000001e-05,\n",
              "  7.000000000000001e-05,\n",
              "  8e-05,\n",
              "  9e-05]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = dict(learning_rate = [10 ** (-5) * i for i in range(1, 10)])\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493ecf36",
      "metadata": {
        "id": "493ecf36"
      },
      "outputs": [],
      "source": [
        "rmse_history = {}\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "for lr in params['learning_rate']:\n",
        "    regressor = CustomSGDRegressor(learning_rate=lr, fit_intercept=True, max_iter=200, shuffle=True, batch_size=2048)\n",
        "    regressor.fit(x_train, y_train)\n",
        "    y_pred = regressor.predict(x_val)\n",
        "    rmse_history[lr] = mean_squared_error(y_pred, y_val) ** 0.5\n",
        "    \n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1f762f",
      "metadata": {
        "id": "6b1f762f",
        "outputId": "5b80f615-a688-43b8-9314-aedf63e07f90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1e-05: 975.1940119138898,\n",
              " 2e-05: 860.471510434291,\n",
              " 3.0000000000000004e-05: 838.994042082948,\n",
              " 4e-05: 827.537881641697,\n",
              " 5e-05: 820.9447743524858,\n",
              " 6.000000000000001e-05: 819.1929643346389,\n",
              " 7.000000000000001e-05: 814.9346559365646,\n",
              " 8e-05: 814.6068211870884,\n",
              " 9e-05: 814.7484757601975}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rmse_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9945765",
      "metadata": {
        "id": "d9945765"
      },
      "source": [
        "###### При learning_rate $= 8 \\cdot 10^{-5}$ достигается минимум RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee1ebf3",
      "metadata": {
        "id": "0ee1ebf3"
      },
      "source": [
        "Посмотрим, куда сойдётся наша модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c38313a",
      "metadata": {
        "id": "4c38313a",
        "outputId": "0277cad8-7720-4193-aa61-f54712661ffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "794.6514511176796"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regressor = CustomSGDRegressor(learning_rate=8e-5, fit_intercept=True, max_iter=200, shuffle=True, batch_size=2048)\n",
        "regressor.fit(x_train, y_train)\n",
        "y_pred = regressor.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f382122d",
      "metadata": {
        "id": "f382122d",
        "outputId": "c3632f25-cad3-44f8-d4d5-34a8f49d0785"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.title('Learning curve, learning rate is $8 * 10^{-5}$')\n",
        "plt.plot(regressor.loss_history, color='darkblue', label='Train')\n",
        "plt.ylabel('RMSE loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2cd087",
      "metadata": {
        "id": "de2cd087"
      },
      "source": [
        "###### Как мы видим, моделька сходится достаточно быстро -- на 50-й эпохе уже можно утверждать, что мы вышли на плато"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe47e39",
      "metadata": {
        "id": "3fe47e39"
      },
      "source": [
        "###### Здесь мы работали с отмасштабированными признаками (имхо, признаки надо масштабировать примерно всегда хотя бы для того, чтобы моделька сходилась быстрее). Посмотрим, что будет, если бы мы не масштабировали их:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c25f223a",
      "metadata": {
        "id": "c25f223a",
        "outputId": "501ad889-6679-4851-ded9-3b4f697292c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/tmp/ipykernel_383/3060014732.py:37: RuntimeWarning: overflow encountered in matmul\n",
            "  grad = x.T @ (y_pred - _y_train[indexes])\n",
            "/home/vlad/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:427: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
            "/tmp/ipykernel_383/3060014732.py:38: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.weights -= self.learning_rate * grad\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_383/1987629390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomSGDRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_383/3060014732.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     )\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \"\"\"\n\u001b[1;32m     89\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5)\n",
        "\n",
        "regressor = CustomSGDRegressor(learning_rate=8e-5, fit_intercept=True, max_iter=1000, shuffle=True, batch_size=2048)\n",
        "regressor.fit(x_train, y_train)\n",
        "y_pred = regressor.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e55f20",
      "metadata": {
        "id": "c9e55f20"
      },
      "source": [
        "###### Я достаточно долго старался, но номально запустить обучение на немасштабированных признаках у меня так и не удалось из-за того, что на первых итерациях значения улетают в бесконечность, а оттуда мы выбраться уже не в состоянии))0)\n",
        "\n",
        "Короче говоря, вывод: признаки нужно масштабировать примерно всегда. Это сильно повлиляет на скорость сходимости, а иногда (как здесь) даже запустить обучение на изначальных признаках в принципе может быть проблематично"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6611d915",
      "metadata": {
        "id": "6611d915"
      },
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261d03f1-5800-49af-8616-4c01ab238388",
      "metadata": {
        "id": "261d03f1-5800-49af-8616-4c01ab238388"
      },
      "source": [
        "Обучите [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) выставив его параметры при инициализации так, что бы он максимально был близок к реализованной вами модели.\n",
        "\n",
        "Сравните вашу реализацию и реализацию из sklearn: \n",
        "    \n",
        "    1. Достигают ли они сравнимого качества на данном наборе данных?\n",
        "    2. Одинаковая ли у них скорость сходимости? \n",
        "    3. Одинаковая ли скорость обучения при заданном количестве эпох?\n",
        "    4. Похожие ли веса модели были найдены?\n",
        "    \n",
        "Если ваша реализация сильно отличается по данным параметрам от реализации из sklearn, то проанализируйте почему."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a13115",
      "metadata": {
        "id": "e5a13115"
      },
      "source": [
        "###### Попробуем сначала обучиться на немасштабированных данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d43091",
      "metadata": {
        "id": "35d43091",
        "outputId": "00c36611-86f3-4663-bde8-ac2453d6ab25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 11402084348105.19, NNZs: 95, Bias: -435394529.913359, T: 34963, Avg. loss: 360640199470544055805229127434240.000000\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13697173091275.69, NNZs: 95, Bias: -329546721.846981, T: 69926, Avg. loss: 362505055509766111292102027509760.000000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 12103675373535.12, NNZs: 95, Bias: -240052295.608530, T: 104889, Avg. loss: 362680649964527757433340434579456.000000\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 11767062004620.76, NNZs: 95, Bias: 435105360.705197, T: 139852, Avg. loss: 363158893916444127225308860383232.000000\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 12372427413079.05, NNZs: 95, Bias: 1699238668.751636, T: 174815, Avg. loss: 362935944116135841172189561749504.000000\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 12561153382586.24, NNZs: 95, Bias: 1537383758.948627, T: 209778, Avg. loss: 361673527918679651272975643049984.000000\n",
            "Total training time: 0.10 seconds.\n",
            "Convergence after 6 epochs took 0.10 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.621129260579501e+16"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sklearn_sgd = SGDRegressor(learning_rate='constant', eta0=8e-5, penalty='None', fit_intercept=True,\n",
        "                           shuffle=True, max_iter=1000, verbose=True)\n",
        "\n",
        "sklearn_sgd.fit(x_train, y_train)\n",
        "y_pred = sklearn_sgd.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50564f80",
      "metadata": {
        "id": "50564f80"
      },
      "source": [
        "###### Модель расходится -- так что нет ничего удивительного в том, что наша самописная также не вывезла)\n",
        "\n",
        "Нормализуем же теперь признаки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73060b51",
      "metadata": {
        "id": "73060b51"
      },
      "outputs": [],
      "source": [
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4b79dc-1151-4aa3-8208-3f30abbdd510",
      "metadata": {
        "id": "8a4b79dc-1151-4aa3-8208-3f30abbdd510",
        "outputId": "c8f395bc-dfce-4e50-f4c5-a713f549cc8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 1527.61, NNZs: 95, Bias: 18856.492023, T: 34963, Avg. loss: 36729680.898419\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1782.30, NNZs: 95, Bias: 20000.856856, T: 69926, Avg. loss: 539442.895208\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1941.71, NNZs: 95, Bias: 20071.464395, T: 104889, Avg. loss: 366060.091194\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2057.30, NNZs: 95, Bias: 20075.374843, T: 139852, Avg. loss: 347145.290466\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2159.18, NNZs: 95, Bias: 20075.567600, T: 174815, Avg. loss: 336578.168312\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2228.89, NNZs: 95, Bias: 20075.831511, T: 209778, Avg. loss: 329890.497034\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2285.60, NNZs: 95, Bias: 20076.794660, T: 244741, Avg. loss: 325885.697061\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2332.51, NNZs: 95, Bias: 20078.257988, T: 279704, Avg. loss: 322837.786765\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2376.69, NNZs: 95, Bias: 20076.267295, T: 314667, Avg. loss: 320700.082998\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2413.73, NNZs: 95, Bias: 20078.609169, T: 349630, Avg. loss: 318698.630758\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2444.69, NNZs: 95, Bias: 20081.132672, T: 384593, Avg. loss: 317671.561404\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 2473.22, NNZs: 95, Bias: 20078.865788, T: 419556, Avg. loss: 316901.536242\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2492.99, NNZs: 95, Bias: 20079.315867, T: 454519, Avg. loss: 316052.344470\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2514.15, NNZs: 95, Bias: 20082.037447, T: 489482, Avg. loss: 315592.055134\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 2532.81, NNZs: 95, Bias: 20077.068482, T: 524445, Avg. loss: 315133.232770\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 2547.28, NNZs: 95, Bias: 20074.186561, T: 559408, Avg. loss: 314649.277032\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 2562.95, NNZs: 95, Bias: 20073.390765, T: 594371, Avg. loss: 314319.918846\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 2572.85, NNZs: 95, Bias: 20075.241057, T: 629334, Avg. loss: 314252.639601\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 2581.30, NNZs: 95, Bias: 20077.635928, T: 664297, Avg. loss: 313854.850200\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 2597.29, NNZs: 95, Bias: 20076.722683, T: 699260, Avg. loss: 313697.317537\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 2604.54, NNZs: 95, Bias: 20075.522373, T: 734223, Avg. loss: 313692.498624\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 2611.95, NNZs: 95, Bias: 20078.946158, T: 769186, Avg. loss: 313508.540683\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 2621.49, NNZs: 95, Bias: 20080.472754, T: 804149, Avg. loss: 313627.571815\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 2627.55, NNZs: 95, Bias: 20077.814544, T: 839112, Avg. loss: 313366.736234\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 2635.64, NNZs: 95, Bias: 20070.878537, T: 874075, Avg. loss: 313442.175077\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 2639.87, NNZs: 95, Bias: 20076.948599, T: 909038, Avg. loss: 313239.551421\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 2644.20, NNZs: 95, Bias: 20081.696134, T: 944001, Avg. loss: 313337.111653\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 2648.28, NNZs: 95, Bias: 20075.174339, T: 978964, Avg. loss: 313263.849753\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 2651.74, NNZs: 95, Bias: 20078.800320, T: 1013927, Avg. loss: 313126.948968\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 2654.25, NNZs: 95, Bias: 20077.587860, T: 1048890, Avg. loss: 313193.627680\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 2659.54, NNZs: 95, Bias: 20076.053374, T: 1083853, Avg. loss: 313145.473625\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 2659.17, NNZs: 95, Bias: 20077.548719, T: 1118816, Avg. loss: 313185.118876\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 2663.64, NNZs: 95, Bias: 20074.633898, T: 1153779, Avg. loss: 313219.272653\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 2664.65, NNZs: 95, Bias: 20078.839623, T: 1188742, Avg. loss: 312905.212330\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 2671.44, NNZs: 95, Bias: 20075.540706, T: 1223705, Avg. loss: 313178.897370\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 2670.77, NNZs: 95, Bias: 20075.584000, T: 1258668, Avg. loss: 312949.213993\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 2671.09, NNZs: 95, Bias: 20079.495078, T: 1293631, Avg. loss: 313062.410544\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 2677.58, NNZs: 95, Bias: 20081.030839, T: 1328594, Avg. loss: 313088.839104\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 2674.39, NNZs: 95, Bias: 20081.304061, T: 1363557, Avg. loss: 313249.091364\n",
            "Total training time: 0.64 seconds.\n",
            "Convergence after 39 epochs took 0.64 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "804.5558349995373"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sklearn_sgd = SGDRegressor(learning_rate='constant', eta0=8e-5, penalty='None', fit_intercept=True,\n",
        "                           shuffle=True, max_iter=1000, verbose=True)\n",
        "\n",
        "sklearn_sgd.fit(x_train, y_train)\n",
        "y_pred = sklearn_sgd.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fba1c41",
      "metadata": {
        "id": "2fba1c41"
      },
      "source": [
        "###### Ответы на вопроски:\n",
        "1. Да, реализации имеют одинаковое качество на тестовых данных\n",
        "2. Да, одинаковая: модель из sklearn делает early_stopping на 39 эпохе. Наша модель к 39 эпохе тоже сходится, что можно видеть на графике"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc51813",
      "metadata": {
        "id": "7bc51813"
      },
      "source": [
        "Посчитаем теперь время обучения моделей:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc5850c",
      "metadata": {
        "id": "efc5850c",
        "outputId": "6a2d2619-932d-493a-83a4-1653eb9a4655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 1508.21, NNZs: 95, Bias: 18855.076401, T: 34963, Avg. loss: 36741277.337556\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1788.03, NNZs: 95, Bias: 20004.924503, T: 69926, Avg. loss: 537870.466851\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1952.99, NNZs: 95, Bias: 20080.197567, T: 104889, Avg. loss: 363791.724539\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2073.82, NNZs: 95, Bias: 20078.864059, T: 139852, Avg. loss: 345004.293789\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2163.43, NNZs: 95, Bias: 20078.661336, T: 174815, Avg. loss: 335162.337917\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2238.86, NNZs: 95, Bias: 20073.358289, T: 209778, Avg. loss: 329105.212297\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 2297.59, NNZs: 95, Bias: 20078.614472, T: 244741, Avg. loss: 324909.748209\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 2345.65, NNZs: 95, Bias: 20075.844996, T: 279704, Avg. loss: 321965.108217\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 2385.95, NNZs: 95, Bias: 20075.650309, T: 314667, Avg. loss: 320054.355054\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 2418.25, NNZs: 95, Bias: 20072.129224, T: 349630, Avg. loss: 318427.215915\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 2452.70, NNZs: 95, Bias: 20073.613790, T: 384593, Avg. loss: 317541.604816\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 2476.37, NNZs: 95, Bias: 20082.904676, T: 419556, Avg. loss: 316674.415927\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2500.42, NNZs: 95, Bias: 20077.418314, T: 454519, Avg. loss: 315972.019089\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2524.14, NNZs: 95, Bias: 20081.113018, T: 489482, Avg. loss: 315352.080853\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 2540.30, NNZs: 95, Bias: 20077.534421, T: 524445, Avg. loss: 314971.020774\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 2549.02, NNZs: 95, Bias: 20079.663492, T: 559408, Avg. loss: 314741.407477\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 2561.22, NNZs: 95, Bias: 20074.088480, T: 594371, Avg. loss: 314296.838466\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 2583.09, NNZs: 95, Bias: 20081.287343, T: 629334, Avg. loss: 314083.690853\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 2591.75, NNZs: 95, Bias: 20071.146371, T: 664297, Avg. loss: 313859.543376\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 2599.04, NNZs: 95, Bias: 20077.104131, T: 699260, Avg. loss: 313761.909118\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 2610.53, NNZs: 95, Bias: 20079.288304, T: 734223, Avg. loss: 313637.365137\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 2615.26, NNZs: 95, Bias: 20074.954328, T: 769186, Avg. loss: 313670.612851\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 2621.45, NNZs: 95, Bias: 20078.228861, T: 804149, Avg. loss: 313388.617656\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 2627.24, NNZs: 95, Bias: 20074.720642, T: 839112, Avg. loss: 313547.162516\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 2633.73, NNZs: 95, Bias: 20075.676616, T: 874075, Avg. loss: 313397.145467\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 2635.69, NNZs: 95, Bias: 20083.401176, T: 909038, Avg. loss: 313267.211795\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 2644.29, NNZs: 95, Bias: 20080.495839, T: 944001, Avg. loss: 313371.058339\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 2647.80, NNZs: 95, Bias: 20079.711248, T: 978964, Avg. loss: 313297.995723\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 2648.09, NNZs: 95, Bias: 20075.794289, T: 1013927, Avg. loss: 313180.534973\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 2653.08, NNZs: 95, Bias: 20071.450319, T: 1048890, Avg. loss: 313336.792928\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 2660.01, NNZs: 95, Bias: 20081.143583, T: 1083853, Avg. loss: 313223.199472\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 2661.53, NNZs: 95, Bias: 20080.937338, T: 1118816, Avg. loss: 313233.252128\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 2664.97, NNZs: 95, Bias: 20080.394129, T: 1153779, Avg. loss: 313107.435119\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 2666.93, NNZs: 95, Bias: 20072.422743, T: 1188742, Avg. loss: 313010.663201\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 2667.80, NNZs: 95, Bias: 20078.060843, T: 1223705, Avg. loss: 313125.486126\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 2674.24, NNZs: 95, Bias: 20082.788356, T: 1258668, Avg. loss: 312954.261242\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 2670.68, NNZs: 95, Bias: 20083.815048, T: 1293631, Avg. loss: 313243.787357\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 2675.98, NNZs: 95, Bias: 20072.887288, T: 1328594, Avg. loss: 313116.561596\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 2671.10, NNZs: 95, Bias: 20075.148079, T: 1363557, Avg. loss: 313057.005890\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 2677.82, NNZs: 95, Bias: 20076.025107, T: 1398520, Avg. loss: 313021.358299\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 2675.99, NNZs: 95, Bias: 20075.558222, T: 1433483, Avg. loss: 313158.907247\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 2680.27, NNZs: 95, Bias: 20079.635053, T: 1468446, Avg. loss: 313201.130709\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 2680.54, NNZs: 95, Bias: 20073.784318, T: 1503409, Avg. loss: 313097.899482\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 2684.28, NNZs: 95, Bias: 20078.733940, T: 1538372, Avg. loss: 313196.958808\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 2684.47, NNZs: 95, Bias: 20072.248559, T: 1573335, Avg. loss: 313088.709519\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 2685.37, NNZs: 95, Bias: 20075.507489, T: 1608298, Avg. loss: 312954.870261\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 2685.90, NNZs: 95, Bias: 20076.332945, T: 1643261, Avg. loss: 312906.599963\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 2684.68, NNZs: 95, Bias: 20076.050140, T: 1678224, Avg. loss: 313224.288648\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 2687.43, NNZs: 95, Bias: 20080.980188, T: 1713187, Avg. loss: 312971.721352\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 2685.95, NNZs: 95, Bias: 20075.767628, T: 1748150, Avg. loss: 313257.103601\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 2687.69, NNZs: 95, Bias: 20077.884568, T: 1783113, Avg. loss: 313025.870385\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 2688.29, NNZs: 95, Bias: 20079.336713, T: 1818076, Avg. loss: 312931.748244\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 2686.36, NNZs: 95, Bias: 20071.832481, T: 1853039, Avg. loss: 313031.784853\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 2690.42, NNZs: 95, Bias: 20085.255004, T: 1888002, Avg. loss: 312937.782270\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 2688.85, NNZs: 95, Bias: 20082.087652, T: 1922965, Avg. loss: 312991.920643\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 2690.81, NNZs: 95, Bias: 20076.529172, T: 1957928, Avg. loss: 312803.035335\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 2687.18, NNZs: 95, Bias: 20072.904408, T: 1992891, Avg. loss: 313041.161391\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 2692.45, NNZs: 95, Bias: 20078.891272, T: 2027854, Avg. loss: 313035.108443\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 2694.08, NNZs: 95, Bias: 20078.664103, T: 2062817, Avg. loss: 313112.536092\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 2693.26, NNZs: 95, Bias: 20078.857332, T: 2097780, Avg. loss: 312982.287986\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 2689.56, NNZs: 95, Bias: 20075.705768, T: 2132743, Avg. loss: 312987.472318\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 2691.64, NNZs: 95, Bias: 20078.272943, T: 2167706, Avg. loss: 312921.762938\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 2689.32, NNZs: 95, Bias: 20079.018567, T: 2202669, Avg. loss: 313048.176493\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 2690.92, NNZs: 95, Bias: 20072.284427, T: 2237632, Avg. loss: 313024.172608\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 2688.73, NNZs: 95, Bias: 20076.315831, T: 2272595, Avg. loss: 313126.192706\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 2692.71, NNZs: 95, Bias: 20081.800415, T: 2307558, Avg. loss: 313085.606057\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 2690.69, NNZs: 95, Bias: 20078.425289, T: 2342521, Avg. loss: 312911.471069\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 2694.87, NNZs: 95, Bias: 20074.536346, T: 2377484, Avg. loss: 313084.232656\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 2694.61, NNZs: 95, Bias: 20084.386043, T: 2412447, Avg. loss: 312945.421211\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 2693.07, NNZs: 95, Bias: 20080.717805, T: 2447410, Avg. loss: 313076.959254\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 71\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norm: 2690.23, NNZs: 95, Bias: 20077.751942, T: 2482373, Avg. loss: 312899.556626\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 2693.71, NNZs: 95, Bias: 20071.791871, T: 2517336, Avg. loss: 313079.954360\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 2696.44, NNZs: 95, Bias: 20079.032522, T: 2552299, Avg. loss: 313028.449306\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 2696.33, NNZs: 95, Bias: 20078.957458, T: 2587262, Avg. loss: 313152.010015\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 2695.19, NNZs: 95, Bias: 20078.104443, T: 2622225, Avg. loss: 313005.605469\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 2695.10, NNZs: 95, Bias: 20075.809866, T: 2657188, Avg. loss: 312929.754758\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 2695.66, NNZs: 95, Bias: 20072.991224, T: 2692151, Avg. loss: 312931.748926\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 2694.92, NNZs: 95, Bias: 20076.189151, T: 2727114, Avg. loss: 313092.327138\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 2695.36, NNZs: 95, Bias: 20068.484606, T: 2762077, Avg. loss: 313073.325413\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 2698.08, NNZs: 95, Bias: 20076.671289, T: 2797040, Avg. loss: 313018.117309\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 2694.21, NNZs: 95, Bias: 20076.552819, T: 2832003, Avg. loss: 312917.296511\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 2691.85, NNZs: 95, Bias: 20071.276596, T: 2866966, Avg. loss: 312679.726606\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 2693.15, NNZs: 95, Bias: 20079.252225, T: 2901929, Avg. loss: 313195.262434\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 2694.84, NNZs: 95, Bias: 20075.124797, T: 2936892, Avg. loss: 313057.788402\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 2692.61, NNZs: 95, Bias: 20073.510339, T: 2971855, Avg. loss: 313155.684540\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 2693.55, NNZs: 95, Bias: 20081.982825, T: 3006818, Avg. loss: 313027.202408\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 2693.49, NNZs: 95, Bias: 20078.957584, T: 3041781, Avg. loss: 313037.981837\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 2692.74, NNZs: 95, Bias: 20074.949384, T: 3076744, Avg. loss: 313116.800750\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 2694.89, NNZs: 95, Bias: 20075.936667, T: 3111707, Avg. loss: 313032.526476\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 2693.78, NNZs: 95, Bias: 20079.038385, T: 3146670, Avg. loss: 313100.768192\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 2693.35, NNZs: 95, Bias: 20077.612042, T: 3181633, Avg. loss: 313111.137300\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 2693.23, NNZs: 95, Bias: 20081.446602, T: 3216596, Avg. loss: 313026.063683\n",
            "Total training time: 1.54 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 2696.44, NNZs: 95, Bias: 20079.555712, T: 3251559, Avg. loss: 312929.760461\n",
            "Total training time: 1.55 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 2692.91, NNZs: 95, Bias: 20078.017169, T: 3286522, Avg. loss: 313028.449248\n",
            "Total training time: 1.57 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 2695.58, NNZs: 95, Bias: 20076.876187, T: 3321485, Avg. loss: 313199.197535\n",
            "Total training time: 1.59 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 2694.96, NNZs: 95, Bias: 20075.988472, T: 3356448, Avg. loss: 313111.185004\n",
            "Total training time: 1.60 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 2693.33, NNZs: 95, Bias: 20079.765316, T: 3391411, Avg. loss: 312992.842833\n",
            "Total training time: 1.61 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 2697.08, NNZs: 95, Bias: 20078.073188, T: 3426374, Avg. loss: 313076.939457\n",
            "Total training time: 1.63 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 2699.38, NNZs: 95, Bias: 20079.020879, T: 3461337, Avg. loss: 312928.005836\n",
            "Total training time: 1.65 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 2696.16, NNZs: 95, Bias: 20076.556675, T: 3496300, Avg. loss: 313131.786872\n",
            "Total training time: 1.66 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 2694.10, NNZs: 95, Bias: 20075.888392, T: 3531263, Avg. loss: 313087.199156\n",
            "Total training time: 1.67 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 2695.70, NNZs: 95, Bias: 20076.502643, T: 3566226, Avg. loss: 312976.157363\n",
            "Total training time: 1.69 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 2693.35, NNZs: 95, Bias: 20076.766981, T: 3601189, Avg. loss: 313118.889163\n",
            "Total training time: 1.70 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 2699.29, NNZs: 95, Bias: 20078.020912, T: 3636152, Avg. loss: 313086.543929\n",
            "Total training time: 1.72 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 2695.36, NNZs: 95, Bias: 20076.674432, T: 3671115, Avg. loss: 312902.251586\n",
            "Total training time: 1.74 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 2696.82, NNZs: 95, Bias: 20080.284215, T: 3706078, Avg. loss: 313059.766800\n",
            "Total training time: 1.76 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 2696.07, NNZs: 95, Bias: 20078.872269, T: 3741041, Avg. loss: 312722.845726\n",
            "Total training time: 1.79 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 2692.97, NNZs: 95, Bias: 20075.613338, T: 3776004, Avg. loss: 313134.242451\n",
            "Total training time: 1.81 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 2691.11, NNZs: 95, Bias: 20080.084049, T: 3810967, Avg. loss: 313099.778865\n",
            "Total training time: 1.85 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 2695.43, NNZs: 95, Bias: 20078.538954, T: 3845930, Avg. loss: 312844.090238\n",
            "Total training time: 1.88 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 2697.53, NNZs: 95, Bias: 20082.464981, T: 3880893, Avg. loss: 313127.165407\n",
            "Total training time: 1.91 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 2697.94, NNZs: 95, Bias: 20078.905001, T: 3915856, Avg. loss: 313081.683381\n",
            "Total training time: 1.93 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 2700.25, NNZs: 95, Bias: 20078.531012, T: 3950819, Avg. loss: 313048.048885\n",
            "Total training time: 1.95 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 2694.36, NNZs: 95, Bias: 20076.854594, T: 3985782, Avg. loss: 313122.437838\n",
            "Total training time: 1.97 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 2694.08, NNZs: 95, Bias: 20082.475872, T: 4020745, Avg. loss: 313060.808904\n",
            "Total training time: 1.99 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 2696.73, NNZs: 95, Bias: 20079.650764, T: 4055708, Avg. loss: 313059.954473\n",
            "Total training time: 2.01 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 2699.61, NNZs: 95, Bias: 20080.588895, T: 4090671, Avg. loss: 313009.609747\n",
            "Total training time: 2.03 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 2693.42, NNZs: 95, Bias: 20072.577055, T: 4125634, Avg. loss: 313070.090920\n",
            "Total training time: 2.05 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 2692.11, NNZs: 95, Bias: 20076.487361, T: 4160597, Avg. loss: 312949.241504\n",
            "Total training time: 2.07 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 2696.51, NNZs: 95, Bias: 20077.048848, T: 4195560, Avg. loss: 313082.077568\n",
            "Total training time: 2.08 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 2693.89, NNZs: 95, Bias: 20079.691670, T: 4230523, Avg. loss: 313115.798424\n",
            "Total training time: 2.10 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 2697.73, NNZs: 95, Bias: 20078.035356, T: 4265486, Avg. loss: 312994.564225\n",
            "Total training time: 2.12 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 2697.78, NNZs: 95, Bias: 20075.614509, T: 4300449, Avg. loss: 313151.324300\n",
            "Total training time: 2.13 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 2694.27, NNZs: 95, Bias: 20074.863822, T: 4335412, Avg. loss: 312903.613604\n",
            "Total training time: 2.16 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 2697.59, NNZs: 95, Bias: 20081.126938, T: 4370375, Avg. loss: 313035.502148\n",
            "Total training time: 2.18 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 2698.92, NNZs: 95, Bias: 20079.811001, T: 4405338, Avg. loss: 313052.274558\n",
            "Total training time: 2.19 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 2693.98, NNZs: 95, Bias: 20081.432151, T: 4440301, Avg. loss: 313128.659331\n",
            "Total training time: 2.21 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 2697.01, NNZs: 95, Bias: 20078.205636, T: 4475264, Avg. loss: 312932.146285\n",
            "Total training time: 2.22 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 2693.07, NNZs: 95, Bias: 20079.506257, T: 4510227, Avg. loss: 313089.800705\n",
            "Total training time: 2.24 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 2692.66, NNZs: 95, Bias: 20075.787367, T: 4545190, Avg. loss: 313061.543612\n",
            "Total training time: 2.25 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 2696.86, NNZs: 95, Bias: 20075.327306, T: 4580153, Avg. loss: 313063.602641\n",
            "Total training time: 2.27 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 2694.94, NNZs: 95, Bias: 20085.812943, T: 4615116, Avg. loss: 312955.874264\n",
            "Total training time: 2.28 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 2690.34, NNZs: 95, Bias: 20077.124709, T: 4650079, Avg. loss: 312900.790321\n",
            "Total training time: 2.30 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 2693.97, NNZs: 95, Bias: 20073.949282, T: 4685042, Avg. loss: 312874.582441\n",
            "Total training time: 2.31 seconds.\n",
            "-- Epoch 135\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norm: 2692.87, NNZs: 95, Bias: 20072.210407, T: 4720005, Avg. loss: 313049.599813\n",
            "Total training time: 2.33 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 2693.79, NNZs: 95, Bias: 20075.330318, T: 4754968, Avg. loss: 312896.076102\n",
            "Total training time: 2.34 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 2694.11, NNZs: 95, Bias: 20078.608275, T: 4789931, Avg. loss: 313026.171598\n",
            "Total training time: 2.36 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 2697.22, NNZs: 95, Bias: 20082.994715, T: 4824894, Avg. loss: 313109.549854\n",
            "Total training time: 2.38 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 2695.05, NNZs: 95, Bias: 20085.152107, T: 4859857, Avg. loss: 312992.092780\n",
            "Total training time: 2.40 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 2695.80, NNZs: 95, Bias: 20078.674313, T: 4894820, Avg. loss: 313103.103140\n",
            "Total training time: 2.44 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 2692.24, NNZs: 95, Bias: 20074.969141, T: 4929783, Avg. loss: 313256.497110\n",
            "Total training time: 2.45 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 2693.51, NNZs: 95, Bias: 20077.444170, T: 4964746, Avg. loss: 313077.117839\n",
            "Total training time: 2.47 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 2694.53, NNZs: 95, Bias: 20079.078170, T: 4999709, Avg. loss: 313045.696671\n",
            "Total training time: 2.48 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 2693.70, NNZs: 95, Bias: 20079.762666, T: 5034672, Avg. loss: 313081.815149\n",
            "Total training time: 2.50 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 2697.71, NNZs: 95, Bias: 20080.884771, T: 5069635, Avg. loss: 312975.680615\n",
            "Total training time: 2.52 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 2695.67, NNZs: 95, Bias: 20079.069069, T: 5104598, Avg. loss: 313060.703276\n",
            "Total training time: 2.53 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 2696.12, NNZs: 95, Bias: 20080.466192, T: 5139561, Avg. loss: 313149.823863\n",
            "Total training time: 2.55 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 2698.72, NNZs: 95, Bias: 20078.198959, T: 5174524, Avg. loss: 313037.711751\n",
            "Total training time: 2.57 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 2693.30, NNZs: 95, Bias: 20077.152080, T: 5209487, Avg. loss: 312980.198807\n",
            "Total training time: 2.58 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 2697.10, NNZs: 95, Bias: 20085.727548, T: 5244450, Avg. loss: 313005.824495\n",
            "Total training time: 2.59 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 2698.34, NNZs: 95, Bias: 20085.395401, T: 5279413, Avg. loss: 313024.054749\n",
            "Total training time: 2.61 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 2697.44, NNZs: 95, Bias: 20077.051819, T: 5314376, Avg. loss: 313016.950543\n",
            "Total training time: 2.62 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 2692.11, NNZs: 95, Bias: 20077.580455, T: 5349339, Avg. loss: 313046.659311\n",
            "Total training time: 2.64 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 2693.60, NNZs: 95, Bias: 20081.371232, T: 5384302, Avg. loss: 313051.752776\n",
            "Total training time: 2.66 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 2692.31, NNZs: 95, Bias: 20078.884939, T: 5419265, Avg. loss: 313000.365493\n",
            "Total training time: 2.68 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 2695.68, NNZs: 95, Bias: 20080.165474, T: 5454228, Avg. loss: 313076.572697\n",
            "Total training time: 2.70 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 2694.71, NNZs: 95, Bias: 20082.984171, T: 5489191, Avg. loss: 312996.599296\n",
            "Total training time: 2.72 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 2693.03, NNZs: 95, Bias: 20080.766623, T: 5524154, Avg. loss: 312909.186039\n",
            "Total training time: 2.75 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 2691.38, NNZs: 95, Bias: 20084.305979, T: 5559117, Avg. loss: 313033.142401\n",
            "Total training time: 2.77 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 2697.55, NNZs: 95, Bias: 20084.277112, T: 5594080, Avg. loss: 313166.645064\n",
            "Total training time: 2.79 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 2696.77, NNZs: 95, Bias: 20075.279589, T: 5629043, Avg. loss: 312994.347190\n",
            "Total training time: 2.84 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 2695.55, NNZs: 95, Bias: 20076.223355, T: 5664006, Avg. loss: 312988.015844\n",
            "Total training time: 2.88 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 2692.12, NNZs: 95, Bias: 20076.363335, T: 5698969, Avg. loss: 313031.471611\n",
            "Total training time: 2.91 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 2694.69, NNZs: 95, Bias: 20080.826903, T: 5733932, Avg. loss: 313013.730218\n",
            "Total training time: 2.93 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 2694.04, NNZs: 95, Bias: 20080.644939, T: 5768895, Avg. loss: 312902.481507\n",
            "Total training time: 2.95 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 2695.98, NNZs: 95, Bias: 20073.581407, T: 5803858, Avg. loss: 312836.310701\n",
            "Total training time: 2.97 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 2694.62, NNZs: 95, Bias: 20078.088072, T: 5838821, Avg. loss: 312927.910649\n",
            "Total training time: 3.00 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 2696.21, NNZs: 95, Bias: 20080.675481, T: 5873784, Avg. loss: 313084.003184\n",
            "Total training time: 3.02 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 2699.04, NNZs: 95, Bias: 20077.680996, T: 5908747, Avg. loss: 313071.906353\n",
            "Total training time: 3.04 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 2698.30, NNZs: 95, Bias: 20080.499513, T: 5943710, Avg. loss: 313126.932705\n",
            "Total training time: 3.07 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 2698.92, NNZs: 95, Bias: 20078.156922, T: 5978673, Avg. loss: 313131.339787\n",
            "Total training time: 3.10 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 2692.57, NNZs: 95, Bias: 20076.999002, T: 6013636, Avg. loss: 313074.197491\n",
            "Total training time: 3.13 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 2696.32, NNZs: 95, Bias: 20078.393006, T: 6048599, Avg. loss: 313013.275758\n",
            "Total training time: 3.16 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 2693.54, NNZs: 95, Bias: 20078.513011, T: 6083562, Avg. loss: 313110.907296\n",
            "Total training time: 3.18 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 2693.33, NNZs: 95, Bias: 20076.142981, T: 6118525, Avg. loss: 312951.255882\n",
            "Total training time: 3.21 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 2691.43, NNZs: 95, Bias: 20078.947090, T: 6153488, Avg. loss: 313057.207215\n",
            "Total training time: 3.23 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 2694.98, NNZs: 95, Bias: 20076.263191, T: 6188451, Avg. loss: 312876.521989\n",
            "Total training time: 3.26 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 2693.05, NNZs: 95, Bias: 20077.929478, T: 6223414, Avg. loss: 313079.739327\n",
            "Total training time: 3.27 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 2694.97, NNZs: 95, Bias: 20074.633980, T: 6258377, Avg. loss: 312974.435667\n",
            "Total training time: 3.29 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 2698.44, NNZs: 95, Bias: 20077.813914, T: 6293340, Avg. loss: 313116.017750\n",
            "Total training time: 3.31 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 2696.35, NNZs: 95, Bias: 20078.466540, T: 6328303, Avg. loss: 313017.267960\n",
            "Total training time: 3.32 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 2697.13, NNZs: 95, Bias: 20077.399059, T: 6363266, Avg. loss: 313007.596339\n",
            "Total training time: 3.33 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 2696.86, NNZs: 95, Bias: 20076.983354, T: 6398229, Avg. loss: 313072.043732\n",
            "Total training time: 3.35 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 2696.32, NNZs: 95, Bias: 20078.512432, T: 6433192, Avg. loss: 313123.648125\n",
            "Total training time: 3.37 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 2694.00, NNZs: 95, Bias: 20077.601942, T: 6468155, Avg. loss: 312894.040864\n",
            "Total training time: 3.38 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 2698.89, NNZs: 95, Bias: 20080.511636, T: 6503118, Avg. loss: 313052.149120\n",
            "Total training time: 3.40 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 2693.86, NNZs: 95, Bias: 20078.577460, T: 6538081, Avg. loss: 313045.311692\n",
            "Total training time: 3.42 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 2696.12, NNZs: 95, Bias: 20073.004517, T: 6573044, Avg. loss: 312979.405177\n",
            "Total training time: 3.44 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 2696.69, NNZs: 95, Bias: 20071.443183, T: 6608007, Avg. loss: 313090.332731\n",
            "Total training time: 3.45 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 2695.91, NNZs: 95, Bias: 20080.113907, T: 6642970, Avg. loss: 312936.685774\n",
            "Total training time: 3.47 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 2695.62, NNZs: 95, Bias: 20080.897264, T: 6677933, Avg. loss: 313077.285259\n",
            "Total training time: 3.48 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 2692.52, NNZs: 95, Bias: 20075.337081, T: 6712896, Avg. loss: 313048.969417\n",
            "Total training time: 3.50 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 2697.97, NNZs: 95, Bias: 20076.494212, T: 6747859, Avg. loss: 313039.395444\n",
            "Total training time: 3.53 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 2695.59, NNZs: 95, Bias: 20079.132922, T: 6782822, Avg. loss: 312925.736296\n",
            "Total training time: 3.55 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 2697.83, NNZs: 95, Bias: 20076.654714, T: 6817785, Avg. loss: 313146.055568\n",
            "Total training time: 3.58 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 2695.65, NNZs: 95, Bias: 20073.795468, T: 6852748, Avg. loss: 313001.907800\n",
            "Total training time: 3.60 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 2690.37, NNZs: 95, Bias: 20074.913694, T: 6887711, Avg. loss: 313046.627079\n",
            "Total training time: 3.63 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 2694.32, NNZs: 95, Bias: 20076.072586, T: 6922674, Avg. loss: 313202.997930\n",
            "Total training time: 3.65 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 2697.08, NNZs: 95, Bias: 20078.744613, T: 6957637, Avg. loss: 313078.182524\n",
            "Total training time: 3.69 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 2700.28, NNZs: 95, Bias: 20079.170932, T: 6992600, Avg. loss: 313056.696406\n",
            "Total training time: 3.72 seconds.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "sklearn_sgd = SGDRegressor(learning_rate='constant', eta0=8e-5, penalty='None', fit_intercept=True,\n",
        "                           shuffle=True, max_iter=200, verbose=True, early_stopping=False, tol=None)\n",
        "\n",
        "t_start = time.time()\n",
        "sklearn_sgd.fit(x_train, y_train)\n",
        "t_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8239d354",
      "metadata": {
        "id": "8239d354",
        "outputId": "720d61a7-bf0b-421f-84e7-86bbef3184ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For sklearn model training time is 3.7201855182647705 for 200 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For sklearn model training time is {t_end - t_start} for {sklearn_sgd.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128e135b",
      "metadata": {
        "id": "128e135b",
        "outputId": "0b349cb8-111f-46ae-c976-f49fb31e1844"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "806.9027841792893"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = sklearn_sgd.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520a9902",
      "metadata": {
        "id": "520a9902"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec295b36",
      "metadata": {
        "id": "ec295b36",
        "outputId": "eaf3d390-61fa-4ec8-c4ec-1829b75cec75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For our model training time is 0.6365442276000977 for 200 epochs\n"
          ]
        }
      ],
      "source": [
        "regressor = CustomSGDRegressor(learning_rate=8e-5, fit_intercept=True, max_iter=200, shuffle=True, batch_size=2048)\n",
        "\n",
        "t_start = time.time()\n",
        "regressor.fit(x_train, y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "print(f'For our model training time is {t_end - t_start} for {regressor.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093fc448",
      "metadata": {
        "id": "093fc448",
        "outputId": "990abe29-ef29-41de-80f2-f7f95c8a8b1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "810.9311647537671"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = regressor.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28858b23",
      "metadata": {
        "id": "28858b23"
      },
      "source": [
        "###### 3. Модель из sklearn прогоняет 200 эпох за почти 3с, наша же справляется менее, чем за 0.7с. Такая \"эффективность\" объясняется тем, что в моей имплементации мы на каждой эпохе прогоняемся не по всем объектам обучающей выборки, а только по количеству, равному batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451aec23",
      "metadata": {
        "id": "451aec23"
      },
      "source": [
        "Сравним теперь веса наших моделей:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a73ed7",
      "metadata": {
        "id": "37a73ed7",
        "outputId": "73d7de2e-0955-4765-c5fb-be2b1b781fcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20079.170932211775, 20069.839618697544)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "float(sklearn_sgd.intercept_), regressor.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7226252f",
      "metadata": {
        "scrolled": true,
        "id": "7226252f",
        "outputId": "21609c7c-1f2b-4eef-9013-c358b919396f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-4.49148245e+00, -7.50774548e+00,  1.56500344e+01, -2.32632786e+01,\n",
              "         1.24919800e+01, -1.84003824e+01,  5.73176791e+01,  1.32427818e+02,\n",
              "         1.19664661e+03, -4.45825529e+00, -2.24230053e+01,  3.64814792e+00,\n",
              "         1.57168427e+00,  2.70414446e+01, -2.92347863e+01,  3.62726238e+01,\n",
              "         1.55342325e+02,  1.16805321e+03, -5.59131506e+00,  2.44997680e+01,\n",
              "        -2.72928018e+01,  2.55291768e+01, -2.79234474e+01, -2.68149054e+01,\n",
              "         3.88418473e+01,  1.24759932e+02,  1.22411241e+03, -9.07196799e+00,\n",
              "        -2.45053399e+01, -2.75704991e+01,  3.83584147e+01, -1.77272569e+01,\n",
              "         1.73632235e+01,  1.57522924e+01,  1.32169429e+02,  1.20567175e+03,\n",
              "        -3.21525820e+00, -1.31469747e+00, -3.46710106e+00,  6.34520910e+00,\n",
              "         1.92607184e+01, -1.82482867e+01,  2.41066840e+01,  1.64588413e+02,\n",
              "         1.16155519e+03,  1.77867696e+00, -1.60470064e+01,  1.85366687e+01,\n",
              "         2.59524030e+00, -1.41895950e+01, -5.35310774e+00, -7.02944084e+00,\n",
              "         3.92700418e+01,  5.92455392e+01, -1.06628478e+02,  7.17009285e+00,\n",
              "         1.05680068e+00, -1.36642855e+01,  1.73020083e+01, -1.11084877e+01,\n",
              "        -3.57634498e+01,  1.68138543e+01,  2.86503099e+01,  6.48875213e+01,\n",
              "        -6.01413749e+01, -2.12372911e+00,  9.87373016e+00, -9.87693741e+00,\n",
              "         7.56235994e+00,  2.11750549e+00, -1.67288144e+01,  2.78358440e+01,\n",
              "         7.10159106e+00,  9.91026283e+01, -1.13447036e+02, -2.27826159e+00,\n",
              "        -5.13106593e+00,  4.08411103e+01, -2.47324444e+01,  1.06696263e+01,\n",
              "        -5.64008050e+01,  3.83919550e+01,  3.36962094e+01,  8.86425837e-02,\n",
              "        -5.82185259e+01, -7.30352011e+00,  6.29916431e+00,  1.09946942e+01,\n",
              "        -1.80679006e+01, -1.01616036e+01,  3.16120159e+01, -2.26909553e+01,\n",
              "         4.29937905e+01,  3.83582641e+01, -6.72466238e+01]),\n",
              " array([-9.29655609e+00, -1.44816117e+01,  1.10512527e+01, -1.13482385e+01,\n",
              "        -4.08634980e+01, -2.07688636e+01,  6.13044263e+01,  2.71248909e+02,\n",
              "         1.07215245e+03, -3.84082014e-01, -7.92433888e+00,  5.01338252e+00,\n",
              "         6.39743621e+00, -1.39521604e+00, -2.85710217e+01,  5.46903101e+01,\n",
              "         2.99521355e+02,  1.04999038e+03, -1.98082599e+01,  1.51982538e+01,\n",
              "        -1.00947665e+01,  1.18024023e+01, -4.05733374e+01, -2.95423065e+01,\n",
              "         2.07158052e+01,  2.82417815e+02,  1.09070358e+03, -2.64581774e+00,\n",
              "        -1.93393667e+01, -1.81834450e+01,  1.29202585e+01, -1.54154169e+01,\n",
              "        -9.74566325e+00,  3.86194593e+01,  2.75315796e+02,  1.06917587e+03,\n",
              "        -1.75951724e+01, -1.81230945e+00, -1.58434382e+01,  7.21591010e+00,\n",
              "        -1.49619801e+01, -2.03445331e+01,  2.81598754e+01,  2.72117331e+02,\n",
              "         1.03895671e+03,  1.56778632e+01, -1.16150699e+01,  1.60337731e+01,\n",
              "        -9.73662429e+00, -1.69894002e+00, -2.23217980e+01,  2.67994753e+01,\n",
              "         4.67845203e+01,  3.33626501e+01, -9.75950928e+01,  4.18983791e+00,\n",
              "         7.22803268e-01, -7.82190455e+00,  1.18704125e+01, -5.60187257e+00,\n",
              "        -2.79484447e+01,  1.49017641e+01,  5.92004630e+01,  2.60394435e+01,\n",
              "        -4.86799304e+01, -1.29188693e+01,  1.13957803e+01,  5.04365938e+00,\n",
              "        -5.47948513e+00, -4.32086074e-01, -1.17090744e+00,  2.92311723e+01,\n",
              "         2.43717747e+01,  1.46862210e+01, -5.88252063e+01, -1.31707014e+01,\n",
              "        -9.41830582e+00,  4.41171685e+01, -1.65273260e+01, -5.65343417e+00,\n",
              "        -3.37562427e+01,  4.05306056e+01,  4.60917960e+01, -5.59304381e+00,\n",
              "        -5.07357413e+01, -1.29662189e+01,  6.78552616e+00,  1.09291257e+01,\n",
              "        -2.25313451e+01,  8.47783760e-01,  9.39476308e+00,  3.53064895e+00,\n",
              "         3.21306488e+01,  2.30006705e+01, -5.62879007e+01]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sklearn_sgd.coef_, regressor.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21f363b",
      "metadata": {
        "id": "a21f363b",
        "outputId": "bd9aec90-36e0-4099-c643-8d63895f3dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-4.46, -0.384\n",
            "27.0, -1.4\n",
            "-16.7, -1.17\n",
            "0.0886, -5.59\n",
            "-10.2, 0.848\n"
          ]
        }
      ],
      "source": [
        "# посмотрим, есть ли значения весов, отличающиеся на порядок\n",
        "\n",
        "for sk, our in zip(sklearn_sgd.coef_, regressor.coef_):\n",
        "    if not 0.1 <= abs(sk / our) <= 10:\n",
        "        print(f'{sk:.{3}}, {our:.{3}}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a2ab62",
      "metadata": {
        "id": "e4a2ab62"
      },
      "source": [
        "###### 4. Видим, что есть веса, отличающиеся на порядок, но их всего 5 из 96, так что в целом мы можем сказать, что веса моделей довольно близки, обе модели сходятся в один и тот же минимум"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3237edef",
      "metadata": {
        "id": "3237edef"
      },
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79fa910-f204-4ef9-b8b8-b100c8a3d25c",
      "metadata": {
        "id": "f79fa910-f204-4ef9-b8b8-b100c8a3d25c"
      },
      "source": [
        "**Задание 2** (2 балла)\n",
        "\n",
        "Повысьте качество модели `SGDRegressor` проведя эксперименты со стратегией обучения, а также с регуляризацией. \n",
        "\n",
        "Для подбора гиперпараметров воспользуйтесь перебором по сетке, который реализован в классе [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). В качестве схемы кросс-валидации используйте 5-Fold CV, которую можно задать с помощью класса [KFoldCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
        "\n",
        "Постройте график среднего значения качества по кросс-валидации алгоритма при заданном значении гиперпараметра, на котором также отобразите доверительный интервал. Для получения значения качества на каждом фолде, среднего значение качества и другой полезной информации можно воспользоваться полем [*cv results_*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
        "\n",
        "Какая комбинация стратегии обучения и регуляризации оказалась выигрышной?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cecfbc6",
      "metadata": {
        "id": "9cecfbc6",
        "outputId": "6a69b702-6491-4707-ee75-6a1fad68fcfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': [0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09],\n",
            " 'eta0': [0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07],\n",
            " 'learning_rate': ['invscaling', 'optimal', 'constant'],\n",
            " 'penalty': ['l1', 'l2', None]}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "sgd = SGDRegressor(eta0=8e-5, fit_intercept=True, max_iter=300, shuffle=True)\n",
        "\n",
        "params = dict(eta0 = [10 ** (-i) for i in range(2, 8)],\n",
        "          penalty = ['l1', 'l2', None],\n",
        "          alpha = [10 ** (-i) for i in range(1, 10)],\n",
        "          learning_rate = ['invscaling', 'optimal', 'constant'])\n",
        "\n",
        "pprint(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5cb66b3",
      "metadata": {
        "scrolled": true,
        "id": "a5cb66b3",
        "outputId": "b99f2404-91e5-4bbe-cce2-1ee7fc56337b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-612316.378 total time=   0.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-645243.510 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-644907.007 total time=   0.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-642200.693 total time=   0.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-627298.552 total time=   0.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-743701.469 total time=   0.4s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-778387.591 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-789043.028 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-759049.051 total time=   0.4s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-756640.348 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-615496.168 total time=   0.3s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-647145.923 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-647472.059 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-639692.679 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-621561.953 total time=   0.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-2516569683614185291776.000 total time=   7.3s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-1750421136046014595072.000 total time=   6.4s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-1588442482870889480192.000 total time=   6.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-2080163298496222593024.000 total time=   6.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-1630722242859449712640.000 total time=   6.9s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-135490274129.714 total time=   4.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-90647284011.434 total time=   5.3s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-94207199290.270 total time=   4.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-90483137617.398 total time=   4.0s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-126297914836.982 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None;, score=-2373625321469551575040.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None;, score=-1018301101448893038592.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None;, score=-2093320341480213053440.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None;, score=-2435341701531220049920.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=optimal, penalty=None;, score=-2093725920216770412544.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1;, score=-702128.850 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1;, score=-885301.382 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1;, score=-762767.000 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1;, score=-747672.733 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l1;, score=-750348.243 total time=   0.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1131438.475 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1586511.549 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1556407.680 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1444062.530 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1247165.077 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None;, score=-818505.583 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None;, score=-1212138.573 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None;, score=-1136168.139 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None;, score=-1416360.999 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.01, learning_rate=constant, penalty=None;, score=-1323381.602 total time=   0.3s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-607718.257 total time=   1.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-642785.641 total time=   2.0s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640408.537 total time=   2.0s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-631882.599 total time=   1.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615486.368 total time=   2.1s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-730374.936 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-770467.239 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-761303.530 total time=   0.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-755777.922 total time=   0.7s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-743929.044 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-608204.227 total time=   0.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-643226.573 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-640571.790 total time=   1.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-632951.681 total time=   1.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-616567.148 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1021226250675216449536.000 total time=   6.0s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1980465946840837914624.000 total time=   6.0s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1288270588168547598336.000 total time=   5.9s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1909497095561741860864.000 total time=   5.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-2438207385064109506560.000 total time=   6.0s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-82573611450.447 total time=   3.4s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-65922156416.833 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-128882748909.931 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-176885542904.432 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-70814764516.934 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None;, score=-1971353959547026014208.000 total time=   3.4s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None;, score=-1902450399598895169536.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None;, score=-1686721337273851838464.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None;, score=-1822539767013904482304.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=optimal, penalty=None;, score=-1531062849940878524416.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1;, score=-642321.254 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1;, score=-674086.377 total time=   0.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1;, score=-677230.753 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1;, score=-654543.818 total time=   0.4s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l1;, score=-638261.937 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2;, score=-745574.552 total time=   0.3s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2;, score=-806859.054 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2;, score=-776357.834 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2;, score=-770165.591 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=l2;, score=-774997.614 total time=   0.3s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None;, score=-648502.219 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None;, score=-717963.086 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None;, score=-685308.238 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None;, score=-646066.001 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.001, learning_rate=constant, penalty=None;, score=-647395.677 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628101.160 total time=   6.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662426.889 total time=   8.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-656904.860 total time=   7.9s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-648062.352 total time=   7.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632129.424 total time=   7.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-732487.776 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-772180.286 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-765500.898 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-758729.660 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-744724.138 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628257.530 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662460.159 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657199.461 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-647951.278 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632303.638 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-1022635085939161563136.000 total time=   7.3s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-1963623996591061073920.000 total time=   6.5s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-2339266551977959227392.000 total time=   7.1s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-2918317733274165182464.000 total time=   6.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-2142181891285079293952.000 total time=   6.9s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-57297692006.771 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-182702117080.698 total time=   4.1s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-80491371972.178 total time=   4.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-126145721347.135 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-173318686172.070 total time=   3.8s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-2597505186204444786688.000 total time=   4.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-1554915692304510746624.000 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-2781939886076106637312.000 total time=   4.1s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-2613256394140401795072.000 total time=   4.0s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-3790426045357744455680.000 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-609585.465 total time=   0.9s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-645442.883 total time=   1.1s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-641899.086 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-636502.275 total time=   0.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-614776.176 total time=   1.1s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-732738.672 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-771765.424 total time=   0.3s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-763813.194 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-754279.258 total time=   0.4s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-743837.984 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=None;, score=-608226.809 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=None;, score=-644642.275 total time=   0.5s\n",
            "[CV 3/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=None;, score=-643200.912 total time=   0.7s\n",
            "[CV 4/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=None;, score=-635180.972 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.1, eta0=0.0001, learning_rate=constant, penalty=None;, score=-617027.646 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7189974.973 total time=   7.1s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294344.327 total time=   6.4s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7314147.974 total time=   6.4s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7420970.363 total time=   6.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269404.926 total time=   6.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7227174.209 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7327451.482 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7361003.410 total time=   3.7s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7468203.597 total time=   4.2s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7306525.471 total time=   4.4s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190725.324 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294364.953 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7313876.469 total time=   3.9s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421325.151 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269384.588 total time=   3.8s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-1982850494929433001984.000 total time=   6.3s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-1806743904259107192832.000 total time=   6.1s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-741444689710379171840.000 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-3055985284205701496832.000 total time=   6.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-2032087283035203698688.000 total time=   6.3s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-96934859137.864 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-155317232809.665 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-72336832189.277 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-133115706158.211 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-78921473001.852 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2522104755402891591680.000 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-3195523575452528017408.000 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2037106046538309435392.000 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-1275094892940066816000.000 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-1536537317258342694912.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-607333.752 total time=   5.9s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-642880.424 total time=   5.9s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640399.333 total time=   5.4s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631536.413 total time=   6.7s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615592.029 total time=   5.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-729873.311 total time=   1.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-769260.927 total time=   1.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-763371.218 total time=   1.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-755421.768 total time=   1.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-741301.077 total time=   1.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607433.994 total time=   3.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=None;, score=-642756.486 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640399.505 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631692.531 total time=   3.0s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615850.422 total time=   2.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268084824.424 total time=   6.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268269179.391 total time=   6.7s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269127910.929 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269505448.832 total time=   6.3s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268034309.540 total time=   6.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268106602.799 total time=   3.9s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268286033.064 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269149930.316 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269523278.898 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268048323.695 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268083070.492 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268271199.274 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269127414.163 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269503813.766 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268034293.138 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1791306379454623514624.000 total time=   6.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-2242539127509770305536.000 total time=   6.3s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-2119152919638066593792.000 total time=   6.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1929041930488775966720.000 total time=   6.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1740878312974384889856.000 total time=   6.4s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-90769881310.598 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-118188117027.079 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-127735070837.014 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-111027788221.866 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-144572491422.338 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-2333693097338205896704.000 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-2380656213859339075584.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-1358523922958238351360.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-1429517433175320297472.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-1588766969846183755776.000 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687670.695 total time=   6.5s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727476.940 total time=   6.2s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714668.231 total time=   6.2s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710651.509 total time=   6.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693360.245 total time=   6.2s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-765722.063 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-806265.842 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-797059.371 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-794423.118 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-777852.070 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687661.922 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727551.824 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714744.042 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710650.966 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693410.202 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403945.238 total time=   6.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948921.910 total time=   6.2s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528746.743 total time=   6.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393665897.284 total time=   6.5s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391534054.197 total time=   6.4s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392405362.113 total time=   3.9s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391950041.653 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393529688.668 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393666428.797 total time=   4.2s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391535386.103 total time=   4.1s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392403163.816 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949227.554 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393529441.933 total time=   4.0s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393665864.044 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391534046.347 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1858015695305596469248.000 total time=   6.2s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1584662405201320738816.000 total time=   6.1s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-966859617304998445056.000 total time=   7.1s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1726125191433796976640.000 total time=   6.8s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1348087718993192026112.000 total time=   6.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-66517116110.163 total time=   4.3s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-180724823138.587 total time=   4.0s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-76008641876.054 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-90039228571.873 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-110404498141.983 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-2015172703595935301632.000 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-2259663784688793681920.000 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-2412088663364840980480.000 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-1711210099360742506496.000 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-2104212283726541094912.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301016.317 total time=   6.6s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592029.267 total time=   6.5s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795521.989 total time=   6.4s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149367.513 total time=   6.1s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548619.928 total time=   6.3s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76327808.107 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76607294.483 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76827804.583 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77175452.124 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76561766.465 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301224.404 total time=   4.0s\n",
            "[CV 2/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592358.361 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795666.966 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149575.796 total time=   4.2s\n",
            "[CV 5/5] END alpha=0.1, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548848.012 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-616155.015 total time=   0.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-657454.337 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-647328.906 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-643455.043 total time=   1.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-627169.121 total time=   0.8s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-625672.375 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-656014.192 total time=   0.3s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-649174.018 total time=   0.5s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-645349.890 total time=   0.7s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-628978.766 total time=   0.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-612856.443 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-648064.018 total time=   0.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-646214.044 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-639763.313 total time=   0.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-626160.915 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-73500403583075.953 total time=   6.4s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-311956579023422.188 total time=   6.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-52595530231632.008 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-53361382544955.078 total time=   6.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-52238558670380.703 total time=   6.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-779726.461 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-1112589.101 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-709578.218 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-794137.694 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-748220.352 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None;, score=-77690748220559.828 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None;, score=-102798610676271.844 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None;, score=-296588119953548.438 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None;, score=-125895313247244.078 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=optimal, penalty=None;, score=-67998160441708.117 total time=   3.8s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1;, score=-678869.170 total time=   1.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1;, score=-962080.996 total time=   1.1s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1;, score=-965943.899 total time=   0.9s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1;, score=-866334.480 total time=   1.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l1;, score=-923724.538 total time=   1.2s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1130016.859 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1348880.684 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2;, score=-2046056.614 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1003602.123 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1051785.728 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None;, score=-3106645.160 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None;, score=-1385926.252 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None;, score=-1027965.143 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None;, score=-1112431.201 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.01, learning_rate=constant, penalty=None;, score=-1053576.887 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-607547.237 total time=   1.9s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-642903.715 total time=   1.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640318.613 total time=   1.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-632082.854 total time=   2.2s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615976.329 total time=   1.8s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-613743.169 total time=   1.0s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-650663.937 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-645911.200 total time=   0.9s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-636796.103 total time=   1.0s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-621974.109 total time=   1.1s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-608173.258 total time=   0.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-643180.887 total time=   1.2s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-640781.170 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-631540.845 total time=   0.8s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-615693.906 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-33795010462706.840 total time=   6.4s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-29857853446433.676 total time=   6.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-29319817231526.746 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-70428370850156.547 total time=   6.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-350945072900478.000 total time=   6.5s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-728583.145 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-822211.883 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-968344.451 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-730011.701 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-639959.153 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None;, score=-101508325038248.359 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None;, score=-89901901516897.578 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None;, score=-29073375301152.914 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None;, score=-75197275486963.391 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=optimal, penalty=None;, score=-239193962103473.656 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1;, score=-615613.864 total time=   0.4s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1;, score=-665699.709 total time=   0.3s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1;, score=-664237.633 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1;, score=-664103.774 total time=   0.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l1;, score=-629539.483 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2;, score=-647881.070 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2;, score=-690301.860 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2;, score=-665474.467 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2;, score=-658188.928 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=l2;, score=-668106.476 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None;, score=-638951.871 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None;, score=-670288.236 total time=   0.3s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None;, score=-674080.270 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None;, score=-660462.680 total time=   0.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=0.01, eta0=0.001, learning_rate=constant, penalty=None;, score=-652855.409 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628150.781 total time=   7.0s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662352.815 total time=   6.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657273.869 total time=   6.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-647965.191 total time=   6.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632120.305 total time=   6.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-634773.937 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-669927.598 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-664101.686 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-655164.505 total time=   4.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-640189.280 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628156.310 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662314.493 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657168.636 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-647995.283 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632153.098 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-24766693064105.859 total time=   5.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-53294301623407.164 total time=   5.7s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-40755369272009.664 total time=   5.9s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-20176620796846.512 total time=   5.8s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-163786945002152.688 total time=   6.0s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-823046.657 total time=   3.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-707020.564 total time=   3.2s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-1133216.045 total time=   3.2s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-718077.746 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-813788.866 total time=   4.0s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-55086129703520.570 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-94507534512785.312 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-43398380996608.898 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-78765994990118.750 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-23109186015113.785 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-609534.791 total time=   0.7s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-646362.562 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-642184.533 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-632084.924 total time=   0.9s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-617573.079 total time=   0.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-615235.571 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-649438.142 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-647274.028 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-637165.202 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-624002.684 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=None;, score=-610286.726 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=None;, score=-647816.343 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=None;, score=-641278.468 total time=   0.5s\n",
            "[CV 4/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=None;, score=-632351.427 total time=   0.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=0.0001, learning_rate=constant, penalty=None;, score=-618287.929 total time=   0.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190793.261 total time=   6.3s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294822.549 total time=   6.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7314098.397 total time=   6.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7421041.913 total time=   6.2s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7268942.460 total time=   6.4s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7193850.183 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7297573.005 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7318495.880 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7425695.291 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7273004.036 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190654.554 total time=   4.0s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294540.033 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7314230.673 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421322.165 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269312.445 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-410074529078635.875 total time=   6.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-37250119347376.508 total time=   6.8s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-40281774866374.125 total time=   6.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-73522000755149.797 total time=   6.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-53736221852127.406 total time=   6.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-657575.921 total time=   3.7s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-842738.532 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-980154.554 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-764970.200 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-684801.447 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-69091777685875.703 total time=   3.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-77398420080006.625 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-59327323813228.016 total time=   3.9s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-179236270650753.094 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-57966789479514.789 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-608263.415 total time=   4.7s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-643433.954 total time=   5.0s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640406.839 total time=   5.5s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631361.732 total time=   5.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615535.230 total time=   5.5s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-613290.671 total time=   3.1s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-649229.256 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-646358.930 total time=   2.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-636131.520 total time=   3.1s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-621123.621 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607714.152 total time=   2.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=None;, score=-642873.747 total time=   3.1s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640805.839 total time=   2.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631424.020 total time=   3.0s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615594.227 total time=   2.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268084083.462 total time=   6.3s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268270516.149 total time=   6.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269127047.622 total time=   6.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269503726.157 total time=   6.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268033089.171 total time=   6.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268085708.572 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268271052.888 total time=   4.1s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269130891.095 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269506854.556 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268033990.238 total time=   3.8s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268082808.373 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268269399.066 total time=   4.0s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269127826.140 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269504601.379 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268034002.363 total time=   3.8s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-45771020239352.586 total time=   6.3s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-80204787050199.094 total time=   6.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-61170959981236.719 total time=   6.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-94900050556705.906 total time=   6.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-58558170084264.234 total time=   6.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-853690.113 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-764174.403 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-793948.967 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-751197.628 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-825488.558 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-62357344089133.578 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-307263752228522.625 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-103265230425185.406 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-100484971031858.375 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-36614952391635.586 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687669.364 total time=   6.4s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727542.810 total time=   6.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714739.381 total time=   6.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710662.112 total time=   6.4s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693399.016 total time=   6.6s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693955.933 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-733988.218 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-721448.687 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-717602.500 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-700480.317 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687668.407 total time=   3.9s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727535.947 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714729.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710650.495 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693421.252 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403945.106 total time=   7.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948479.632 total time=   6.2s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528960.012 total time=   6.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393665891.426 total time=   7.4s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391534071.864 total time=   6.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403512.243 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391948906.977 total time=   4.0s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393528397.250 total time=   4.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393665948.823 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391534342.583 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392404075.941 total time=   4.1s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949156.265 total time=   4.1s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528467.766 total time=   4.4s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393665499.681 total time=   4.0s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391534582.835 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-87643636473893.750 total time=   6.2s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-322009245290744.562 total time=   6.2s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-130619992029313.469 total time=   6.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-154596178735057.000 total time=   6.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-218345976196772.688 total time=   6.3s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-794568.387 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-724837.891 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-996378.321 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-917916.306 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-977917.404 total time=   3.9s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-93623194666753.516 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-53742594746664.570 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-52938578236877.367 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-70092815614113.977 total time=   3.9s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-53075823773509.008 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301239.510 total time=   6.4s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592263.219 total time=   6.4s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795663.855 total time=   6.7s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149449.464 total time=   6.3s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548812.970 total time=   6.1s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76303750.064 total time=   4.1s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76593642.521 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76798783.352 total time=   4.0s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77151985.815 total time=   3.8s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76550024.628 total time=   3.8s\n",
            "[CV 1/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301226.404 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592311.136 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795702.082 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149480.694 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.01, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548892.820 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-616788.339 total time=   0.8s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-646797.529 total time=   1.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-650685.202 total time=   0.8s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-642195.658 total time=   0.8s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-625899.368 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-617665.802 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-648024.276 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-649020.254 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-655668.469 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-632748.487 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-619796.148 total time=   0.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-653336.610 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-649288.686 total time=   0.5s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-644657.039 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-630729.588 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-610162.901 total time=   2.0s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-664912.093 total time=   1.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-646338.640 total time=   2.5s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-636745.095 total time=   2.7s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-624096.432 total time=   2.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-619196.534 total time=   1.5s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-648958.781 total time=   1.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-656168.135 total time=   1.5s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-638689.653 total time=   1.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-628108.479 total time=   1.1s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-612556.376 total time=   1.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-645294.775 total time=   1.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-655421.579 total time=   1.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-641660.966 total time=   1.1s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-625829.679 total time=   1.3s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1032866.988 total time=   1.2s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-2062390.793 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1159901.507 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-948157.055 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-969474.628 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1002715.549 total time=   0.2s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1643268.062 total time=   0.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1281269.905 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1736837.605 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1672720.895 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1256885.814 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1200895.372 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1190790.954 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None;, score=-910240.464 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1153195.723 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-607340.914 total time=   1.9s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-643376.039 total time=   1.7s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640506.115 total time=   2.7s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-632316.089 total time=   1.8s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615610.623 total time=   2.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-607073.490 total time=   1.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-643665.595 total time=   1.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-640468.386 total time=   1.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-631954.400 total time=   1.3s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-616236.529 total time=   1.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-606823.444 total time=   1.2s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-643156.069 total time=   1.2s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-641444.556 total time=   1.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-632379.040 total time=   1.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-615954.542 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-619583.269 total time=   2.1s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-657300.936 total time=   2.2s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-655390.099 total time=   1.6s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-642299.746 total time=   1.8s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-630809.181 total time=   1.7s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-615709.117 total time=   1.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-662513.871 total time=   0.7s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-653031.164 total time=   1.1s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-640645.991 total time=   1.0s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-628750.425 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-608625.685 total time=   1.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-667312.776 total time=   0.8s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-649096.444 total time=   1.4s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-638197.800 total time=   1.1s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-626556.736 total time=   1.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-639460.133 total time=   0.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-680912.304 total time=   0.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-658046.484 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-657877.475 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-630034.248 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-624260.871 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-678503.352 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-671111.576 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-653127.516 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-637976.650 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None;, score=-639530.675 total time=   0.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None;, score=-670526.721 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None;, score=-694341.356 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None;, score=-652578.315 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.001, learning_rate=constant, penalty=None;, score=-629461.502 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628201.795 total time=   6.6s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662246.678 total time=   6.9s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657169.687 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-648038.284 total time=   6.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632244.945 total time=   6.3s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628820.505 total time=   4.2s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-663023.648 total time=   4.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657713.306 total time=   4.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-648719.529 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632863.408 total time=   4.0s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628319.620 total time=   4.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662177.472 total time=   4.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657168.791 total time=   4.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-647868.818 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632195.753 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-618454.838 total time=   2.0s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-647669.895 total time=   2.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-651460.938 total time=   2.1s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-640004.228 total time=   1.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-623779.500 total time=   1.9s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-617975.423 total time=   0.9s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-650246.318 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-652770.086 total time=   0.8s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-640782.593 total time=   1.0s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-629416.108 total time=   0.8s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-621159.638 total time=   0.9s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-645868.998 total time=   1.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-652584.342 total time=   0.9s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-648465.337 total time=   0.9s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-623292.315 total time=   1.3s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-608502.243 total time=   0.8s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-645299.906 total time=   0.8s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-642435.733 total time=   0.8s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-631946.768 total time=   0.7s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-616963.001 total time=   0.7s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-608987.220 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-645312.600 total time=   0.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-642073.123 total time=   0.5s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-633661.383 total time=   0.6s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-618447.362 total time=   0.6s\n",
            "[CV 1/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-609711.844 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-646454.311 total time=   0.7s\n",
            "[CV 3/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-641286.656 total time=   0.5s\n",
            "[CV 4/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-633542.489 total time=   0.6s\n",
            "[CV 5/5] END alpha=0.001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-616726.057 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190936.740 total time=   6.1s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294587.412 total time=   5.8s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7314098.031 total time=   6.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7421282.664 total time=   6.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269497.761 total time=   5.9s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190803.926 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7295017.095 total time=   3.7s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7314081.328 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7421397.187 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269838.976 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190522.386 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294951.744 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7314226.434 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421322.209 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269228.508 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-613678.652 total time=   1.7s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-650149.584 total time=   2.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-656253.195 total time=   1.5s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-632755.280 total time=   2.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-631551.543 total time=   1.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-613030.658 total time=   0.9s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-656288.531 total time=   1.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-652492.002 total time=   1.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-638594.029 total time=   1.0s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-618914.646 total time=   1.1s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-613438.057 total time=   1.2s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-652332.948 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-650534.029 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-657414.915 total time=   1.0s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-622587.904 total time=   1.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-607701.163 total time=   4.9s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-643097.919 total time=   4.8s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640606.887 total time=   4.7s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631908.359 total time=   4.4s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615594.619 total time=   4.6s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-608014.869 total time=   2.5s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-643218.514 total time=   2.8s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640617.997 total time=   3.0s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-632075.143 total time=   2.3s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615797.810 total time=   2.7s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607934.882 total time=   2.6s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-643191.467 total time=   2.9s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640964.027 total time=   2.7s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631874.568 total time=   2.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615855.195 total time=   3.1s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268083319.786 total time=   6.1s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268271480.883 total time=   6.1s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269127470.254 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269502446.203 total time=   5.9s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268033011.812 total time=   5.8s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268082010.477 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268270379.467 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269128481.950 total time=   3.6s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269502698.464 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268034604.774 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268082853.964 total time=   3.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268269178.198 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269126977.925 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269502572.972 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268034017.358 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-620203.189 total time=   1.7s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-653974.472 total time=   1.9s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-649976.813 total time=   2.1s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-642703.816 total time=   2.4s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-625721.596 total time=   1.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-610397.762 total time=   1.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-647660.490 total time=   0.8s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-649399.437 total time=   1.1s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-643924.675 total time=   1.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-625785.337 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-610515.791 total time=   1.1s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-656065.718 total time=   1.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-648801.478 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-636098.540 total time=   1.1s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-627396.432 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687665.802 total time=   6.1s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727553.369 total time=   5.7s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714741.460 total time=   5.8s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710640.782 total time=   5.9s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693416.195 total time=   5.7s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-688267.503 total time=   3.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-728155.732 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-715392.541 total time=   3.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-711333.360 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-694088.944 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687665.009 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727545.935 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714748.521 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710655.783 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693403.320 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403372.002 total time=   6.0s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948777.318 total time=   6.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528422.960 total time=   5.8s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393666396.707 total time=   6.0s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391534144.047 total time=   5.8s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403524.287 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391949371.523 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393528759.710 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393665915.586 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391533882.002 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392404212.647 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949076.187 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528154.662 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393666062.678 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391534059.273 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-611488.385 total time=   2.4s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-666708.041 total time=   1.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-654440.227 total time=   2.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-644249.086 total time=   2.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-633195.756 total time=   1.7s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-614540.570 total time=   0.9s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-649382.462 total time=   1.0s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-656495.733 total time=   0.7s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-638715.185 total time=   1.3s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-619346.418 total time=   1.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-618883.131 total time=   1.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-649098.952 total time=   1.2s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-649033.062 total time=   0.9s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-639731.430 total time=   1.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-629600.869 total time=   1.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301247.560 total time=   6.0s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592364.348 total time=   6.1s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795657.351 total time=   5.7s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149470.702 total time=   5.7s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548815.992 total time=   6.2s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301467.121 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592462.471 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795988.295 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149735.142 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548933.104 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301247.919 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592330.156 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795694.711 total time=   3.2s\n",
            "[CV 4/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149458.723 total time=   3.2s\n",
            "[CV 5/5] END alpha=0.001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548846.553 total time=   3.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-610244.575 total time=   0.7s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-646676.493 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-647116.065 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-635905.202 total time=   0.6s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-622502.475 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-611578.673 total time=   0.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-653161.489 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-652601.729 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-639429.791 total time=   0.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-624668.884 total time=   0.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-614438.340 total time=   0.4s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-652656.612 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-648260.305 total time=   0.6s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-642933.941 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-626865.071 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-685638.685 total time=   4.7s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-707646.918 total time=   3.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-671020.558 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-668601.171 total time=   2.8s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-656140.305 total time=   5.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-688896.434 total time=   1.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-741629.998 total time=   1.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-739775.357 total time=   1.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-673129.399 total time=   1.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-674925.859 total time=   2.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-630154.758 total time=   2.4s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-692644.502 total time=   2.3s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-777376.734 total time=   1.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-700637.371 total time=   1.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=optimal, penalty=None;, score=-662750.474 total time=   2.0s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1242063.538 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1537620.850 total time=   0.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1229721.024 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1214579.270 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l1;, score=-825706.183 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1829204.849 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1428040.082 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1205973.075 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1015662.391 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1081495.717 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None;, score=-902009.101 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1080672.406 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1607682.983 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1272485.610 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.01, learning_rate=constant, penalty=None;, score=-1349904.844 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-608209.573 total time=   1.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-642979.813 total time=   2.0s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-641318.158 total time=   1.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-632410.271 total time=   1.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615623.242 total time=   2.0s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-607034.175 total time=   1.1s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-643107.076 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-640393.563 total time=   1.2s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-632237.780 total time=   1.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-615946.549 total time=   0.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-607909.387 total time=   1.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-643715.654 total time=   1.1s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-640794.210 total time=   1.0s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-631343.115 total time=   1.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-616118.747 total time=   1.0s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-693161.593 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-667397.344 total time=   4.7s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-713682.728 total time=   3.1s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-757921.525 total time=   3.0s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-674306.097 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-669154.774 total time=   1.7s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-725816.005 total time=   1.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-691821.746 total time=   2.2s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-690910.458 total time=   2.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-663261.438 total time=   2.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-698137.184 total time=   1.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-717810.178 total time=   2.2s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-711557.489 total time=   1.9s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-681026.798 total time=   1.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=optimal, penalty=None;, score=-746641.530 total time=   1.8s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-650551.528 total time=   0.3s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-673715.574 total time=   0.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-682245.562 total time=   0.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-658097.099 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l1;, score=-637627.641 total time=   0.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-638809.558 total time=   0.3s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-662263.240 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-673003.773 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-654265.285 total time=   0.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=l2;, score=-669609.014 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None;, score=-630846.169 total time=   0.2s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None;, score=-672570.221 total time=   0.2s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None;, score=-712554.793 total time=   0.2s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None;, score=-652954.209 total time=   0.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.001, learning_rate=constant, penalty=None;, score=-656366.248 total time=   0.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628198.810 total time=   6.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662240.828 total time=   6.0s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657118.144 total time=   5.9s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-647963.369 total time=   6.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632140.410 total time=   6.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628254.470 total time=   3.4s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-662421.862 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657204.044 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-647955.683 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632133.335 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628223.862 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662229.271 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657380.884 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-648101.785 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632175.962 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-705965.420 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-737850.141 total time=   3.6s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-653626.528 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-723032.372 total time=   3.6s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-691818.066 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-677846.592 total time=   1.8s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-671023.764 total time=   2.1s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-801347.084 total time=   1.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-718147.769 total time=   2.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-720172.698 total time=   2.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-663565.737 total time=   1.8s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-712454.000 total time=   2.0s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-694964.316 total time=   1.9s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-684544.110 total time=   1.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-728892.796 total time=   1.8s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-609225.319 total time=   0.7s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-645054.414 total time=   0.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-645248.676 total time=   0.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-634689.080 total time=   0.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-619847.699 total time=   0.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-607573.035 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-644005.511 total time=   0.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-643330.196 total time=   0.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-633943.243 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-616980.710 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-609562.656 total time=   0.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-644597.692 total time=   0.6s\n",
            "[CV 3/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-643452.012 total time=   0.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-632175.899 total time=   0.5s\n",
            "[CV 5/5] END alpha=0.0001, eta0=0.0001, learning_rate=constant, penalty=None;, score=-617707.748 total time=   0.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190654.253 total time=   6.1s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294663.697 total time=   6.1s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7313769.663 total time=   6.0s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7421185.561 total time=   6.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269185.467 total time=   5.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190720.026 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7294742.427 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7313703.084 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7421366.514 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269525.078 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190771.648 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294906.628 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7314130.791 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421060.537 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269300.664 total time=   3.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-787056.424 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-739482.732 total time=   4.1s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-683855.572 total time=   3.9s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-661485.140 total time=   4.2s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-671532.666 total time=   4.2s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-736886.843 total time=   1.8s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-683649.418 total time=   2.8s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-721764.303 total time=   1.6s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-698384.179 total time=   1.6s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-712712.017 total time=   1.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-687947.657 total time=   1.7s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-735036.561 total time=   1.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-706886.851 total time=   1.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-709916.467 total time=   1.8s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-673110.413 total time=   2.1s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-607261.013 total time=   4.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-643172.349 total time=   5.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640862.015 total time=   4.6s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631495.461 total time=   4.8s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615426.584 total time=   4.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-607255.442 total time=   2.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-643150.925 total time=   2.7s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640612.832 total time=   2.7s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-631777.282 total time=   3.1s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615935.276 total time=   2.4s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607290.384 total time=   3.0s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-643206.785 total time=   3.1s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640403.122 total time=   2.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631530.142 total time=   2.9s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615962.234 total time=   2.6s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268083079.736 total time=   5.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268270306.525 total time=   5.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269129057.415 total time=   5.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269503495.242 total time=   6.0s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268034182.435 total time=   5.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268082629.968 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268270552.036 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269127247.495 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269503580.321 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268033237.881 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268083090.873 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268270511.661 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269126622.421 total time=   3.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269503752.076 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268034075.437 total time=   3.6s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-676029.492 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-701283.794 total time=   2.8s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-806532.905 total time=   3.5s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-705279.570 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-654892.828 total time=   3.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-687706.484 total time=   1.8s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-758660.056 total time=   1.8s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-682563.434 total time=   1.6s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-720478.540 total time=   1.6s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-712158.853 total time=   1.6s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-699505.202 total time=   1.8s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-738310.847 total time=   1.7s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-717562.909 total time=   1.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-716208.419 total time=   1.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-686641.841 total time=   1.7s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687668.823 total time=   5.7s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727543.785 total time=   5.7s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714732.720 total time=   6.1s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710659.318 total time=   5.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693409.734 total time=   5.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-687719.452 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-727604.435 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-714803.983 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-710715.424 total time=   3.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693464.125 total time=   3.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687673.856 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727543.925 total time=   3.5s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714738.457 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710654.971 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693424.596 total time=   3.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403442.949 total time=   5.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391949350.247 total time=   5.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528356.515 total time=   6.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393665630.772 total time=   6.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391533804.919 total time=   6.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403626.418 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391948985.371 total time=   3.4s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393529280.429 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393665885.741 total time=   3.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391533907.214 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392403352.998 total time=   3.3s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949007.185 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528073.821 total time=   3.4s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393666367.643 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391533900.867 total time=   3.5s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-653219.907 total time=   3.8s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-680820.060 total time=   3.8s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-713403.702 total time=   3.7s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-694767.630 total time=   4.1s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-672978.398 total time=   3.7s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-650756.830 total time=   2.1s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-714614.570 total time=   1.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-740697.723 total time=   1.8s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-719977.570 total time=   1.7s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-740223.234 total time=   1.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-663159.754 total time=   1.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-713485.974 total time=   1.7s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-685691.983 total time=   2.2s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-719894.131 total time=   2.3s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-765518.129 total time=   1.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301178.142 total time=   5.9s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592316.064 total time=   5.9s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795721.593 total time=   5.7s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149508.421 total time=   5.9s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548849.385 total time=   5.9s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301289.575 total time=   3.6s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592361.434 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795733.168 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149537.785 total time=   3.5s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548872.391 total time=   3.3s\n",
            "[CV 1/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301228.779 total time=   3.5s\n",
            "[CV 2/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592269.179 total time=   3.3s\n",
            "[CV 3/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795690.574 total time=   3.3s\n",
            "[CV 4/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149481.689 total time=   3.4s\n",
            "[CV 5/5] END alpha=0.0001, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548866.719 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-616871.652 total time=   0.9s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-651968.944 total time=   0.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-650594.381 total time=   0.6s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-638789.947 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-630386.489 total time=   0.7s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-629362.344 total time=   0.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-655203.630 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-650457.339 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-637180.438 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-626317.820 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-609995.440 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-647868.064 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-643758.516 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-636382.524 total time=   0.6s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-620436.440 total time=   0.7s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-3356138.225 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-2089725.442 total time=   7.6s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-1364660.315 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-1431134.261 total time=   7.0s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-1058664.641 total time=   7.1s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-7033056.707 total time=   2.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-2038920.429 total time=   2.7s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-1585063.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-2947236.672 total time=   2.8s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-1780711.711 total time=   3.0s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=None;, score=-2436515.446 total time=   3.0s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=None;, score=-4096642.896 total time=   2.9s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=None;, score=-1637449.356 total time=   3.0s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=None;, score=-1716141.596 total time=   3.1s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=optimal, penalty=None;, score=-1431549.285 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1193909.990 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1552269.185 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1966266.632 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l1;, score=-2019614.454 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1172052.051 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1352363.043 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1869080.455 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1240972.896 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1515979.924 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1140191.515 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=None;, score=-1568265.499 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=None;, score=-1120287.114 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=None;, score=-1388889.101 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=None;, score=-1175936.156 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.01, learning_rate=constant, penalty=None;, score=-1423483.687 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-607319.175 total time=   2.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-643950.914 total time=   1.9s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-641291.340 total time=   1.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-631475.010 total time=   1.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-616226.280 total time=   1.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-607356.553 total time=   1.2s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-643820.778 total time=   0.8s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-640484.951 total time=   1.1s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-631591.032 total time=   1.0s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-616072.849 total time=   1.1s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-608425.089 total time=   0.8s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-642613.664 total time=   1.0s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-640737.555 total time=   1.1s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-632178.421 total time=   0.9s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-616187.932 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1546773.082 total time=   7.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1846797.166 total time=   7.7s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-1440213.388 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-2875887.344 total time=   6.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-4162299.603 total time=   6.1s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-3956309.991 total time=   3.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-1650285.901 total time=   3.2s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-1882997.595 total time=   3.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-2946535.496 total time=   2.6s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-1333573.503 total time=   3.2s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=None;, score=-2503952.259 total time=   2.6s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=None;, score=-5156214.205 total time=   2.8s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=None;, score=-3184286.021 total time=   3.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=None;, score=-5842768.685 total time=   2.6s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=optimal, penalty=None;, score=-2331124.648 total time=   3.4s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l1;, score=-629747.060 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l1;, score=-672008.549 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l1;, score=-664121.560 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l1;, score=-653924.230 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l1;, score=-636229.849 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l2;, score=-647732.422 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l2;, score=-671150.664 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l2;, score=-671286.258 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l2;, score=-680181.993 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=l2;, score=-626017.937 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=None;, score=-642113.053 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=None;, score=-662418.450 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=None;, score=-674550.086 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=None;, score=-659939.196 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.001, learning_rate=constant, penalty=None;, score=-643734.472 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628099.914 total time=   6.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662448.623 total time=   6.1s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657211.862 total time=   6.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-647915.587 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632288.521 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628168.937 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-662238.503 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657032.964 total time=   3.8s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-647918.692 total time=   3.8s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632149.999 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628175.544 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662397.042 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657091.684 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-647823.628 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632253.498 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-2549469.639 total time=   7.2s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-1373105.610 total time=   7.2s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-3687379.153 total time=   6.3s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-1745329.332 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-2038847.158 total time=   6.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-8505015.107 total time=   2.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-3460692.339 total time=   2.7s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-1326246.827 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-2551554.910 total time=   2.7s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-5484036.647 total time=   2.8s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-1677787.039 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-1307578.175 total time=   3.2s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-1859006.655 total time=   2.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-3374821.343 total time=   3.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-3344395.081 total time=   2.8s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-607372.487 total time=   0.8s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-645309.089 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-642408.338 total time=   1.0s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-632611.446 total time=   0.9s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-618125.049 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-607688.363 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-646267.928 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-644096.321 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-633793.234 total time=   0.6s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-617031.470 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=None;, score=-614298.345 total time=   0.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=None;, score=-645668.511 total time=   0.6s\n",
            "[CV 3/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=None;, score=-642973.987 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=None;, score=-632016.724 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-05, eta0=0.0001, learning_rate=constant, penalty=None;, score=-620236.334 total time=   0.7s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190621.696 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294536.965 total time=   6.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7313686.967 total time=   6.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7421026.476 total time=   6.9s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269412.186 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190824.848 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7294522.620 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7314314.871 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7420785.448 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269688.591 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190728.201 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294471.315 total time=   3.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7314050.430 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421430.861 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269092.608 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-3587673.295 total time=   6.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-2840619.161 total time=   8.0s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-3279526.558 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-1440796.948 total time=   7.1s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-4290354.826 total time=   5.8s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-1897153.010 total time=   3.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-1874620.812 total time=   3.0s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-1919828.258 total time=   2.8s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-9866083.932 total time=   2.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-1745162.850 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2072467.527 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2476116.534 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2859955.182 total time=   2.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2491223.796 total time=   3.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-2781837.141 total time=   3.0s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-608308.558 total time=   4.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-642882.586 total time=   5.0s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640682.903 total time=   4.9s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631741.372 total time=   4.8s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615591.053 total time=   4.7s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-607346.907 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-642884.378 total time=   3.0s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640216.363 total time=   3.0s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-631815.606 total time=   2.7s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615678.391 total time=   2.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607359.222 total time=   2.8s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=None;, score=-643561.103 total time=   2.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640537.256 total time=   2.8s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631715.081 total time=   2.7s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615607.407 total time=   2.9s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268082385.680 total time=   6.0s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268269723.764 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269126369.230 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269504832.956 total time=   6.2s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268034753.337 total time=   6.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268082513.911 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268269129.319 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269125271.385 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269505273.743 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268033167.080 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268083445.712 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268270819.885 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269128889.287 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269503533.762 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268033393.965 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1331899.488 total time=   7.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1583810.837 total time=   7.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1664156.838 total time=   6.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-3625216.673 total time=   5.7s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-1122727.974 total time=   7.6s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-1960920.153 total time=   2.6s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-1142788.762 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-1343647.914 total time=   3.1s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-3132177.412 total time=   2.8s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-2291312.850 total time=   3.0s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-2145075.951 total time=   2.7s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-2301470.205 total time=   2.7s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-3983951.983 total time=   2.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-4524701.899 total time=   2.5s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-1488518.228 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687657.073 total time=   6.0s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727566.552 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714740.379 total time=   5.8s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710663.296 total time=   5.7s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693408.656 total time=   6.6s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-687675.999 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-727568.180 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-714755.765 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-710657.415 total time=   3.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693398.355 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687674.324 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727555.327 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714746.374 total time=   3.2s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710660.167 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693408.748 total time=   3.2s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403638.269 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391949362.462 total time=   6.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528584.142 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393666028.023 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391533864.081 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403439.234 total time=   3.7s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391948700.411 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393529191.096 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393665729.890 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391533812.116 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392403450.205 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949107.135 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528947.286 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393665918.351 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391533819.229 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1591129.643 total time=   6.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1397175.956 total time=   6.6s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-3751815.434 total time=   7.7s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-2909912.140 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-1668208.192 total time=   7.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-2871104.565 total time=   3.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-1307606.998 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-4337555.146 total time=   2.9s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-2038582.549 total time=   2.9s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-5819391.278 total time=   2.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-1554743.915 total time=   3.1s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-2030481.474 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-2769606.205 total time=   2.8s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-1947130.997 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-39060953.830 total time=   2.6s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301245.560 total time=   5.8s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592283.625 total time=   6.2s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795649.989 total time=   5.8s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149499.021 total time=   5.8s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548817.762 total time=   5.7s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301229.685 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592264.968 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795640.445 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149503.925 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548847.864 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301206.272 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592290.358 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795626.562 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149453.666 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-05, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548829.879 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-617393.509 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-652011.795 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-651258.185 total time=   0.7s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-636573.184 total time=   1.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-623822.277 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-608751.341 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-653051.405 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-654442.084 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-638024.222 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-631166.573 total time=   0.6s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-617709.083 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-650600.353 total time=   0.8s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-644860.724 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-642374.314 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-622295.906 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-64568549596490013714939904.000 total time=   7.7s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-33855995067543171578724352.000 total time=   7.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-42937283572170765874757632.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-38363428030970637957726208.000 total time=   7.8s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-29288325922195040272121856.000 total time=   7.8s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-15509561966233435057946624.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-39934340875016607463636992.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-32441357805550575275737088.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-31713665777571838457544704.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-41313573786448929058979840.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=None;, score=-40158535274654133229977600.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=None;, score=-38757539749160508339519488.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=None;, score=-41831330450907776264699904.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=None;, score=-19867260970883752795308032.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=optimal, penalty=None;, score=-33620512573667156516929536.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1185403.267 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l1;, score=-943219.827 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1045343.959 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1108124.231 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1870149.992 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1503763.398 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l2;, score=-2453686.712 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1232800.768 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1761017.911 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1397801.564 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=None;, score=-1937967.368 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=None;, score=-1588386.334 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=None;, score=-1237939.486 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=None;, score=-1225160.547 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.01, learning_rate=constant, penalty=None;, score=-1131624.817 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-607183.585 total time=   1.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-643252.510 total time=   1.7s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640780.241 total time=   1.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-631471.531 total time=   1.7s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615642.104 total time=   1.9s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-607317.725 total time=   1.2s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-643827.801 total time=   1.0s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-641912.319 total time=   1.1s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-631772.727 total time=   1.2s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-616394.661 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-607298.580 total time=   0.9s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-642571.094 total time=   1.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-640586.133 total time=   1.0s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-631848.407 total time=   1.1s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-616441.751 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-21732453433756860509323264.000 total time=   8.1s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-21894111712288872987623424.000 total time=   7.7s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-33500718889033950621073408.000 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-43059146525975568602628096.000 total time=   7.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-56477436522609051676180480.000 total time=   7.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-63068232459871436448727040.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-42851232931538862025474048.000 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-103350014549953892189208576.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-63292746935265746137120768.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-39887955446473114685276160.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=None;, score=-120016805400657902190985216.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=None;, score=-40273326888453323957993472.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=None;, score=-43644797302635197309648896.000 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=None;, score=-30575692777589168578494464.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=optimal, penalty=None;, score=-63319577061683017575038976.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l1;, score=-633860.469 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l1;, score=-656180.557 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l1;, score=-663686.101 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l1;, score=-713447.735 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l1;, score=-663142.513 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l2;, score=-669729.780 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l2;, score=-688069.826 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l2;, score=-675503.156 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l2;, score=-701826.943 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=l2;, score=-641203.241 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=None;, score=-624563.785 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=None;, score=-652312.968 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=None;, score=-678244.292 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=None;, score=-663129.738 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.001, learning_rate=constant, penalty=None;, score=-646268.008 total time=   0.2s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628130.118 total time=   6.0s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662269.703 total time=   6.0s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657070.403 total time=   6.0s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-648002.560 total time=   6.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632107.742 total time=   6.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628288.342 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-662221.897 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657009.552 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-648073.749 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632190.912 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628145.568 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662310.765 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657170.656 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-648080.975 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632159.363 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-42296793272222881243201536.000 total time=   7.6s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-26530260155546671909961728.000 total time=   7.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-45730832316786135592337408.000 total time=   7.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-54057394768325393248157696.000 total time=   7.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-38792193976381203913113600.000 total time=   7.5s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-36057141826398503974207488.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-67743785898589189799149568.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-31523499078441381771870208.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-32082999010928820664401920.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-30612032456055600455876608.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-40604349486485063176552448.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-26213419579366600242888704.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-87291229699983279388622848.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-43568877772167343364898816.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-53437437369119358819762176.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-609915.057 total time=   0.9s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-644678.236 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-641374.916 total time=   0.7s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-633085.304 total time=   0.8s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-616788.064 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-608355.315 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-644540.731 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-641883.787 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-632042.114 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-616285.650 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=None;, score=-608542.725 total time=   0.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=None;, score=-648564.049 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=None;, score=-642658.375 total time=   0.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=None;, score=-635021.461 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=0.0001, learning_rate=constant, penalty=None;, score=-618542.600 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190792.235 total time=   6.0s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294841.623 total time=   6.1s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7313657.738 total time=   6.3s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7420969.568 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269401.481 total time=   5.9s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190432.934 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7294498.740 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7314385.632 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7421597.587 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269424.639 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190835.443 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294490.439 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7313764.880 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421194.944 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269535.770 total time=   3.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-49721867556658592023052288.000 total time=   7.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-80617654864928616505933824.000 total time=   7.5s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-35758784727371184986914816.000 total time=   7.7s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-39568131943835563987042304.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-32205598883524983476191232.000 total time=   7.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-27271535011743744238026752.000 total time=   3.7s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-42965212351369370843217920.000 total time=   3.7s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-31489862507483902314020864.000 total time=   3.7s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-51321192800715079021494272.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-30507774096416037784256512.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-62688001133904672340312064.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-41149655019407013117427712.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-66732105259026833070882816.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-18533354486726372951588864.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-30806371512776556181192704.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-607478.841 total time=   5.0s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-642679.968 total time=   5.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640794.505 total time=   4.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631935.819 total time=   4.9s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615814.904 total time=   5.1s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-607223.681 total time=   2.9s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-643089.527 total time=   2.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640712.169 total time=   2.9s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-631584.516 total time=   3.0s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615810.244 total time=   2.9s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607933.859 total time=   2.7s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=None;, score=-643254.210 total time=   2.9s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640558.205 total time=   2.7s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631815.362 total time=   2.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615758.152 total time=   2.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268082529.985 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268269744.268 total time=   6.0s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269126540.063 total time=   6.0s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269503873.411 total time=   6.2s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268032747.242 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268083380.187 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268268993.479 total time=   3.8s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269127145.022 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269503466.954 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268033844.354 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268083130.752 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268270488.132 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269126228.576 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269503376.856 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268032774.896 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-27374254230867414408495104.000 total time=   7.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-46481806208972994260238336.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-35969351685529762340536320.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-24596603795184862346870784.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-28520932652584695865802752.000 total time=   7.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-55168026681361451492311040.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-30422703363154038283567104.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-68908688185874604261113856.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-32804704388642190517075968.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-56993111105630617491996672.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-41611543991660329572499456.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-57421894480268918748348416.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-34521023079176062103453696.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-47708546137515656138981376.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-49938332976294330590298112.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687661.714 total time=   5.9s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727550.597 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714752.325 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710659.585 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693400.167 total time=   5.7s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-687667.922 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-727546.867 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-714747.402 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-710636.020 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693418.299 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687670.647 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727555.437 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714744.171 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710666.243 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693389.827 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403448.042 total time=   5.9s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948798.818 total time=   5.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528497.432 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393666167.625 total time=   6.6s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391533926.619 total time=   5.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403199.763 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391948731.254 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393528731.262 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393665943.412 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391534166.100 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392403914.710 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949507.259 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528132.847 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393665754.323 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391534150.787 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-54624108779635575128850432.000 total time=   7.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-53650228388309502168924160.000 total time=   7.7s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-26940743572669672168882176.000 total time=   7.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-29446465972976247038279680.000 total time=   7.8s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-92037753029035136846397440.000 total time=   7.8s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-49499628107007643915124736.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-42896585076702244191076352.000 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-36459901285491896685166592.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-42670143213414289680367616.000 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-79430475711092960916406272.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-36407418998617164193202176.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-49177382796194609832656896.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-27599517212179296015089664.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-44268765871632818053840896.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-63371400018900485475926016.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301188.017 total time=   5.8s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592307.973 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795685.462 total time=   6.2s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149435.113 total time=   5.7s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548849.571 total time=   5.7s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301259.324 total time=   3.8s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592296.948 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795628.723 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149505.201 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548833.043 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301227.347 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592305.633 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795650.652 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149532.189 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-06, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548822.226 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-611294.178 total time=   0.9s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-655610.102 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-657100.371 total time=   0.6s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-639707.258 total time=   0.7s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-629502.528 total time=   0.7s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-614322.439 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-650072.155 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-650368.731 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-639573.854 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-631380.238 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-609261.408 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-650523.251 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-648817.775 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-642724.400 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-621721.272 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-2539190251023115255671685120.000 total time=   7.8s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-2322879114427699001179504640.000 total time=   7.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-4406007461122911387730837504.000 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-3049162077550330915321806848.000 total time=   7.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-4474881191430109240217305088.000 total time=   8.1s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-4540074304933684674083946496.000 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-6203136794403639310434697216.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-7417065512180901231187722240.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-3856834560342560103980859392.000 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-4886241632132392011868143616.000 total time=   3.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=None;, score=-6923684031649237834907254784.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=None;, score=-7999319137805069360482484224.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=None;, score=-5785283595105250585727729664.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=None;, score=-5495107022872137230469562368.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=optimal, penalty=None;, score=-6530885214833753494153854976.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1719833.632 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1049323.009 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1348267.078 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1227317.232 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1085302.136 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l2;, score=-863879.144 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1833484.822 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1453275.675 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1268015.488 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1120687.813 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=None;, score=-1369773.378 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=None;, score=-1115611.738 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=None;, score=-3348498.342 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=None;, score=-1276257.302 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.01, learning_rate=constant, penalty=None;, score=-1149563.739 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-608716.991 total time=   1.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-643714.884 total time=   1.6s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640904.507 total time=   1.6s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-632008.376 total time=   2.0s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-616072.655 total time=   1.9s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-607474.477 total time=   1.2s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-643328.878 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-640266.948 total time=   1.0s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-631484.197 total time=   1.0s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-615519.994 total time=   1.1s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-607381.190 total time=   0.9s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-643799.961 total time=   1.0s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-641637.597 total time=   0.9s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-632034.243 total time=   0.9s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-617285.105 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-5048232562564981344329269248.000 total time=   8.1s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-3964246861373405251734863872.000 total time=   8.1s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-4945944234423453268762951680.000 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-6380874586096166655743754240.000 total time=   7.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-3181269678745465096310358016.000 total time=   7.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-6237847382053235969987969024.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-7652555638378607392107528192.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-4264676160424562902477832192.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-4968213220554785519141126144.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-6576388986926290237386653696.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=None;, score=-3434968037277635779927998464.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=None;, score=-6132250646996410936718262272.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=None;, score=-7138989262049649489040375808.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=None;, score=-3223218243610872453615583232.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=optimal, penalty=None;, score=-4360840833477755994921500672.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l1;, score=-642689.858 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l1;, score=-717077.828 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l1;, score=-670815.046 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l1;, score=-660619.226 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l1;, score=-642599.703 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l2;, score=-630308.300 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l2;, score=-677530.858 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l2;, score=-677293.399 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l2;, score=-652632.308 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=l2;, score=-644561.137 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=None;, score=-634970.007 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=None;, score=-728875.708 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=None;, score=-655495.890 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=None;, score=-647024.725 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.001, learning_rate=constant, penalty=None;, score=-638046.836 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628366.365 total time=   6.6s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662344.190 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657069.036 total time=   6.2s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-647965.492 total time=   6.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632095.502 total time=   6.7s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628292.819 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-662339.529 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657193.539 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-647914.615 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632196.996 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628084.677 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662291.531 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657275.826 total time=   3.8s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-647921.038 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632128.127 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-4136517646372533625662996480.000 total time=   7.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-5385298787707458558762156032.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-5447094098648756345611747328.000 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-6220946190952383594440425472.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-6384905995761142823242432512.000 total time=   8.2s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-1665229583781049739270684672.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-4333573639935762047512346624.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-4484362567872766025320628224.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-4693393696626135681273430016.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-3655097229049357198335934464.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-5384439815065537607328858112.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-6784857666078109365096677376.000 total time=   3.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-5071067661825170554564378624.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-5052666071771553638081626112.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-3776504950039379265705738240.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-607673.329 total time=   0.7s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-644032.222 total time=   0.8s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-641195.660 total time=   1.0s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-632186.239 total time=   0.8s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-617108.703 total time=   0.9s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-608871.144 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-644988.297 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-643588.653 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-634816.856 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-621882.996 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=None;, score=-608498.348 total time=   0.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=None;, score=-644210.717 total time=   0.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=None;, score=-642850.613 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=None;, score=-632415.825 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=0.0001, learning_rate=constant, penalty=None;, score=-616747.805 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190316.818 total time=   6.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7295215.277 total time=   6.1s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7314411.718 total time=   6.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7421288.755 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269006.799 total time=   6.1s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190767.728 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7294416.810 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7314145.735 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7421933.388 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269646.904 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190416.146 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294520.887 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7314204.264 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421296.041 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269287.819 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-6618507175667490679622205440.000 total time=   7.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-4556140729290910595878486016.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-5181151130808035335758413824.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-2760274465361848129476886528.000 total time=   7.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-5190687250050548947893616640.000 total time=   7.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-6605078097668614811069448192.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-3795586928644298042358366208.000 total time=   3.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-2902663736969983987321667584.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-4090504744854335743267438592.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-9434438288214510705733271552.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-6651439321661827610262372352.000 total time=   3.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-4655199239736762418461671424.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-4598742486301847154398855168.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-5760584149289617295008595968.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-4370924447571929359353643008.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-608448.593 total time=   4.1s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-643158.575 total time=   5.2s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640623.790 total time=   5.6s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-632133.073 total time=   4.8s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615454.085 total time=   4.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-607416.781 total time=   2.8s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-643249.800 total time=   2.9s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640911.966 total time=   2.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-631678.990 total time=   2.6s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615388.024 total time=   3.1s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607450.291 total time=   2.8s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=None;, score=-643259.963 total time=   2.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640373.382 total time=   3.0s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631655.451 total time=   2.8s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615670.214 total time=   2.8s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268083704.798 total time=   6.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268271076.940 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269127508.925 total time=   6.0s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269503646.710 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268033444.887 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268082723.483 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268270017.235 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269126827.512 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269504921.936 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268034165.127 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268084248.680 total time=   3.8s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268270162.160 total time=   4.1s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269126816.949 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269504583.062 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268034969.982 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-7370028770228686191371223040.000 total time=   7.6s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-7165677833415843352726732800.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-4886390693260806201985204224.000 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-4241416627472757886759731200.000 total time=   7.6s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-3133624094672167254569779200.000 total time=   7.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-4938470330245213083047821312.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-2390698162520630377874194432.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-2245805409157885882393427968.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-5486701368466999544733761536.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-4733660322251467706169556992.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-5863333840318106159582019584.000 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-5329793010217552086827008000.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-8164143260333367391114231808.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-3612698640822226962519949312.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-3718166527965519831285891072.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687671.678 total time=   5.7s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727551.406 total time=   5.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714749.602 total time=   5.8s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710651.892 total time=   6.1s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693397.958 total time=   6.1s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-687671.920 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-727565.002 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-714736.482 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-710670.753 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693404.829 total time=   3.8s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687665.914 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727554.799 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714749.353 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710678.022 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693396.597 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392404193.723 total time=   6.0s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948933.028 total time=   6.1s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528734.364 total time=   5.8s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393666029.522 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391533927.632 total time=   5.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403784.138 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391949907.317 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393528342.073 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393665965.620 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391533384.754 total time=   3.9s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392404180.417 total time=   3.8s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949303.937 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528654.652 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393666003.573 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391533936.674 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-7829962828275646685929013248.000 total time=   7.4s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-7124477603660165887802998784.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-2752708047061714238277943296.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-2485633556144117254562250752.000 total time=   7.8s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-5410770960909827593841147904.000 total time=   7.3s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-6651965813045180991606358016.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-8713150861824224809510240256.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-5417862360513788405702721536.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-6450465584717854229556887552.000 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-5460272834353705530393886720.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-4142669739652173716036517888.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-3647885240965757961666297856.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-5660627758811217044011220992.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-4125373537151536742365396992.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-5431898713586001529408585728.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301224.940 total time=   5.8s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592264.970 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795660.795 total time=   6.2s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149476.724 total time=   6.4s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548849.780 total time=   5.7s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301203.904 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592282.124 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795657.930 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149538.201 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548843.976 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301192.845 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592369.132 total time=   3.7s\n",
            "[CV 3/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795641.119 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149483.749 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-07, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548848.843 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-624374.243 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-644634.911 total time=   1.0s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-645571.951 total time=   0.9s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-639760.944 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-623983.529 total time=   0.9s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-619415.642 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-650492.095 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-645366.321 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-637268.735 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-624148.441 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-618576.005 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-649706.447 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-653239.566 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-636754.570 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-619631.866 total time=   0.6s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-240600234187547097041825955840.000 total time=   7.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-341853116759703527769270910976.000 total time=   7.9s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-519040253841094310225814487040.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-281063599794117950632320040960.000 total time=   7.6s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-327134998849760002921354756096.000 total time=   8.1s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-490023932166146935664907649024.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-520070423477616831095443030016.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-316035941966301220108117213184.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-234282180128092818144631980032.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-767133574863848586331698495488.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=None;, score=-425999341463527976783262515200.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=None;, score=-421569697662808267892381974528.000 total time=   3.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=None;, score=-889123107399869426295728242688.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=None;, score=-320483967685740501721340182528.000 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=optimal, penalty=None;, score=-487971320935292946535166246912.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1404997.963 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1536628.976 total time=   0.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1169423.987 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1140237.669 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1171008.099 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1094178.066 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1386330.703 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1244060.315 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1768731.181 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=l2;, score=-876152.969 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=None;, score=-1266669.462 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=None;, score=-1462587.185 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=None;, score=-1066563.053 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=None;, score=-1462932.789 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.01, learning_rate=constant, penalty=None;, score=-1144014.288 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-607437.137 total time=   1.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-643319.298 total time=   1.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640500.482 total time=   1.7s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-631789.994 total time=   1.6s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615610.275 total time=   1.9s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-607182.906 total time=   1.1s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-642937.556 total time=   1.1s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-640565.414 total time=   1.1s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-631907.363 total time=   0.9s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-615237.962 total time=   1.1s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-608310.692 total time=   0.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-642940.425 total time=   1.1s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-640519.808 total time=   1.2s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-631823.574 total time=   1.0s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-616356.221 total time=   0.9s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-567891890412232585198352465920.000 total time=   8.0s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-232243064656012003820833865728.000 total time=   7.6s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-265420815409212809520024125440.000 total time=   7.7s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-273370176537848040909122306048.000 total time=   7.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-586841281723386043325325770752.000 total time=   7.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-539918801022574411358138269696.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-476279219389457379809139097600.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-547514277164088091282056937472.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-474359660024612360923694235648.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-487373542849909484319919833088.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=None;, score=-565037726235166163882391633920.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=None;, score=-445091332246324558690959491072.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=None;, score=-413357681994233955660563742720.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=None;, score=-360054685966140961631768674304.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=optimal, penalty=None;, score=-512157143753305506442041622528.000 total time=   3.8s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l1;, score=-643589.985 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l1;, score=-664721.221 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l1;, score=-655325.861 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l1;, score=-654345.545 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l1;, score=-662620.285 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l2;, score=-639428.143 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l2;, score=-667865.070 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l2;, score=-666096.128 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l2;, score=-650192.089 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=l2;, score=-647592.111 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=None;, score=-629107.774 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=None;, score=-660503.196 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=None;, score=-678445.053 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=None;, score=-655189.568 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.001, learning_rate=constant, penalty=None;, score=-638337.260 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628183.535 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662304.284 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657103.462 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-648116.636 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632292.844 total time=   6.2s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628247.845 total time=   3.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-662282.193 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657112.141 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-647973.603 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632145.860 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628156.289 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662373.359 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657196.757 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-647991.992 total time=   3.9s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632117.073 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-418739595697654984085669412864.000 total time=   7.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-206171183430858483343630008320.000 total time=   7.6s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-626384704446664097864938946560.000 total time=   7.8s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-331483228596611555433826484224.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-418037234728968300803836084224.000 total time=   7.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-650022893830241438720990380032.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-362559449012126195923388727296.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-564018277534681151037562159104.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-521190377213370561597362143232.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-260447004205709363411706445824.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-287068809340158364525032112128.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-476797850356695245530761003008.000 total time=   3.8s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-267544057451864787840132972544.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-295476229915264087136023871488.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-260293226085705274956137365504.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-608439.874 total time=   0.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-647836.904 total time=   0.8s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-641329.192 total time=   0.9s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-631684.269 total time=   0.8s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-618618.921 total time=   0.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-609104.357 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-644303.394 total time=   0.6s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-640348.057 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-634722.792 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-618547.361 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=None;, score=-610138.151 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=None;, score=-644026.828 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=None;, score=-643203.335 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=None;, score=-634382.657 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=0.0001, learning_rate=constant, penalty=None;, score=-617310.939 total time=   0.6s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190824.994 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294434.675 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7314100.214 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7421100.110 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269138.137 total time=   6.2s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190746.478 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7294285.759 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7313547.712 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7421001.723 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269286.386 total time=   3.9s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190928.650 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294475.692 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7313809.244 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421016.150 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269365.616 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-543293296321569475315850280960.000 total time=   7.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-247779527071606376241131683840.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-266920281663852272438663446528.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-474702296740568174290100289536.000 total time=   8.1s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-442970638174974071880070201344.000 total time=   7.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-475403140479814327504983818240.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-629206687265811154986902487040.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-834356874218608096560984096768.000 total time=   3.9s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-344886085004294882677072330752.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-703863084206459360768564396032.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-261600967564980776180608139264.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-549702862839806252866091352064.000 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-315584357991011083707742158848.000 total time=   3.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-207173502337244827879196327936.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-668341052353878675405095829504.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-607894.399 total time=   4.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-643003.791 total time=   4.9s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640283.705 total time=   4.8s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-632021.840 total time=   4.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615893.441 total time=   4.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-607524.606 total time=   2.7s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-643033.884 total time=   2.7s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640333.188 total time=   3.1s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-631947.918 total time=   2.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615703.890 total time=   2.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607372.115 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=None;, score=-642623.366 total time=   3.2s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640607.051 total time=   2.7s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631633.466 total time=   2.7s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615619.226 total time=   3.0s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268081853.856 total time=   6.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268270771.037 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269127893.598 total time=   6.1s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269504619.102 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268033153.193 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268082965.526 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268270866.226 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269125908.293 total time=   3.8s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269503321.566 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268033777.554 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268084379.003 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268268763.663 total time=   3.8s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269127290.736 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269505135.067 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268032515.832 total time=   3.8s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-602211432815114319596154257408.000 total time=   7.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-211893380030594503867363229696.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-414621876027299167256161812480.000 total time=   7.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-299452059497527209136039657472.000 total time=   7.6s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-427376007561584831650668216320.000 total time=   7.8s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-620511187708348577918782275584.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-319294518234005273930077569024.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-486561521568651809546586554368.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-242565703425698932436718583808.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-393530759223875249062855835648.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-528570963344671819796504903680.000 total time=   3.7s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-402649988228930479608920276992.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-438641834259365497908588707840.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-324725257098870445451954356224.000 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-263612384879009497233877893120.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687686.245 total time=   6.1s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727558.175 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714732.773 total time=   6.1s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710669.554 total time=   5.8s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693412.448 total time=   5.9s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-687664.675 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-727555.599 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-714752.654 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-710654.743 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693408.900 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687666.997 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727535.261 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714745.036 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710653.774 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693395.680 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403738.449 total time=   5.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948845.254 total time=   6.0s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528095.468 total time=   6.2s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393665732.391 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391534441.905 total time=   6.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392403820.679 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391949045.466 total time=   3.4s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393528911.752 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393666107.679 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391534074.136 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392403694.771 total time=   3.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391949234.052 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393528473.854 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393665884.990 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391534616.062 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-364687302862990588889775734784.000 total time=   7.6s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-495104432767220368002240217088.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-375935925869179188597577744384.000 total time=   7.7s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-435532495223752895633379295232.000 total time=   8.0s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-282563415771215675228766076928.000 total time=   7.3s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-368258946905618992794760642560.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-547412201004384534687341608960.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-343338465406445987556052959232.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-690888793176609144966041567232.000 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-598322300985018737557634023424.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-792923097217157820104558247936.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-964126969005430069812210434048.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-790143237329828414107682865152.000 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-206668803661725995865624018944.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-207573474074421735906028290048.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301201.363 total time=   5.8s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592256.186 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795703.276 total time=   5.8s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149543.127 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548831.465 total time=   5.8s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301244.055 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592303.389 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795633.005 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149492.470 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548845.673 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301234.151 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592355.395 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795692.835 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149477.126 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-08, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548850.385 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-611507.584 total time=   0.8s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-653055.736 total time=   0.8s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-645440.379 total time=   0.7s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-639651.481 total time=   0.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l1;, score=-621755.674 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-610189.045 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-652093.364 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-648078.804 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-636919.461 total time=   0.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=l2;, score=-618458.288 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-622226.495 total time=   0.6s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-649085.893 total time=   0.7s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-644106.220 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-638958.584 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=invscaling, penalty=None;, score=-621749.717 total time=   0.8s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-21379657683242234010184577974272.000 total time=   7.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-14321508137133195518507749474304.000 total time=   7.6s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-37515213695391439470756139892736.000 total time=   7.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-22295479936168816019816402911232.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l1;, score=-9522610784650515035677299572736.000 total time=   8.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-13413774287425367132766012964864.000 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-19015650619173181230128411181056.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-21245745861829769635815814594560.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-33474703120680931226451454197760.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=l2;, score=-25227029255908196392449062469632.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=None;, score=-28748390296141044341840420012032.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=None;, score=-9104862885373959707710124457984.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=None;, score=-6692608917880304437362851577856.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=None;, score=-16798313938360259292681205710848.000 total time=   3.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=optimal, penalty=None;, score=-20944825601220102500202634543104.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1473142.290 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1193529.115 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1871338.918 total time=   0.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l1;, score=-1164744.754 total time=   0.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l1;, score=-937010.028 total time=   0.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1488124.028 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1473730.754 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l2;, score=-948151.457 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1197996.299 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=l2;, score=-1303447.205 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=None;, score=-1490806.239 total time=   0.2s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=None;, score=-1644061.662 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=None;, score=-1782590.569 total time=   0.2s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=None;, score=-1329877.279 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.01, learning_rate=constant, penalty=None;, score=-1725741.829 total time=   0.2s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-606175.026 total time=   2.2s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-642877.932 total time=   1.7s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-640778.432 total time=   1.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-632039.210 total time=   1.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l1;, score=-615935.637 total time=   1.8s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-608217.910 total time=   0.7s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-643279.730 total time=   1.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-640411.140 total time=   0.9s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-631401.292 total time=   1.0s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=l2;, score=-615964.634 total time=   1.0s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-607040.784 total time=   1.2s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-643767.801 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-639968.712 total time=   1.0s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-631311.784 total time=   1.1s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=invscaling, penalty=None;, score=-616557.780 total time=   0.9s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-19726683745807668581947720859648.000 total time=   7.8s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-31423388543587403687012094443520.000 total time=   7.9s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-14476463407255504024523679727616.000 total time=   7.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-9800355338923680762538575790080.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l1;, score=-6420803499627275291881413541888.000 total time=   7.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-33449235541503417451247652831232.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-17877214335935900689206532374528.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-20233568771322332661781024997376.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-17991897702147498858634354360320.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=l2;, score=-25387895587386198023896557420544.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=None;, score=-24456977662576308141170163187712.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=None;, score=-12070002972270253241435728904192.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=None;, score=-14417488404614330739526691979264.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=None;, score=-24302005844180504133642645143552.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=optimal, penalty=None;, score=-13751294193220562768493493092352.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l1;, score=-633952.421 total time=   0.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l1;, score=-658413.458 total time=   0.6s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l1;, score=-655140.485 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l1;, score=-654787.755 total time=   0.3s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l1;, score=-647474.163 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l2;, score=-616927.708 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l2;, score=-658404.979 total time=   0.2s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l2;, score=-668328.376 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l2;, score=-709044.110 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=l2;, score=-633334.089 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=None;, score=-635098.172 total time=   0.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=None;, score=-677402.655 total time=   0.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=None;, score=-707649.091 total time=   0.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=None;, score=-660175.193 total time=   0.2s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.001, learning_rate=constant, penalty=None;, score=-665925.997 total time=   0.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-628253.771 total time=   6.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-662358.039 total time=   6.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-657135.985 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-647893.888 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l1;, score=-632004.522 total time=   6.2s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-628175.197 total time=   4.0s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-662272.927 total time=   3.9s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-657109.142 total time=   4.2s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-648015.850 total time=   4.0s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=l2;, score=-632269.840 total time=   4.0s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-628183.323 total time=   4.0s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-662294.976 total time=   4.2s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-657364.647 total time=   4.2s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-648048.256 total time=   4.3s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=invscaling, penalty=None;, score=-632220.293 total time=   4.1s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-24653795054293324379430942932992.000 total time=   7.9s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-21638642208937518212733309288448.000 total time=   7.7s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-13851700407559108947307994808320.000 total time=   8.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-27173170844010139644535353901056.000 total time=   8.2s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l1;, score=-24241717371754904115505220550656.000 total time=   8.0s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-15646737961823087202406615744512.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-20590510973859399437231662825472.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-14127142561401394545608758394880.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-8313157277055880401808535846912.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=l2;, score=-15891226201997785270637385744384.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-24951368475283164191663090302976.000 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-16903975923770181055163804942336.000 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-15540581350147415450258065326080.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-26507017049908041651394602598400.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=optimal, penalty=None;, score=-10975802271643702124741186813952.000 total time=   3.6s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-607542.705 total time=   0.8s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-644117.150 total time=   0.9s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-642620.758 total time=   0.7s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-633424.945 total time=   0.9s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l1;, score=-615661.932 total time=   0.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-608751.125 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-644872.901 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-641704.713 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-633352.467 total time=   0.5s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=l2;, score=-616875.970 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=None;, score=-607582.247 total time=   0.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=None;, score=-645766.034 total time=   0.5s\n",
            "[CV 3/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=None;, score=-640776.710 total time=   0.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=None;, score=-633322.404 total time=   0.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=0.0001, learning_rate=constant, penalty=None;, score=-619622.832 total time=   0.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7190792.053 total time=   6.0s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7294885.275 total time=   6.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7314026.434 total time=   6.1s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7420819.796 total time=   6.1s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l1;, score=-7269523.204 total time=   6.0s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7190490.339 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7294140.824 total time=   3.7s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7313911.628 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7420992.097 total time=   3.9s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=l2;, score=-7269289.059 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7190629.022 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7294512.365 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7314161.889 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7421674.656 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=invscaling, penalty=None;, score=-7269547.214 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-31562658163254532556051638648832.000 total time=   7.9s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-21267311883884625580022468444160.000 total time=   7.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-21348092843034135395129662373888.000 total time=   7.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-27811390275992121264717470629888.000 total time=   7.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l1;, score=-29118471372202291108790279340032.000 total time=   7.8s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-24085882127260174038933142765568.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-17014823503386241071779791503360.000 total time=   3.8s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-14566768701280159880341715156992.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-17064744721943737789405837918208.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=l2;, score=-17487546660808189858826346823680.000 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-19228297509458958987280292773888.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-22578569884725953371285449867264.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-23120181062609729838327453974528.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-10980243082956933183947491246080.000 total time=   3.3s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=optimal, penalty=None;, score=-24747663006765072826593494171648.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-607603.940 total time=   5.0s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-643135.286 total time=   5.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-640452.313 total time=   5.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-631885.899 total time=   4.7s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l1;, score=-615304.068 total time=   5.0s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-607585.480 total time=   2.9s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-643073.433 total time=   2.8s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-640376.973 total time=   3.1s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-631954.208 total time=   3.0s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=l2;, score=-615922.216 total time=   2.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=None;, score=-607767.845 total time=   2.7s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=None;, score=-642921.947 total time=   3.0s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=None;, score=-640755.467 total time=   2.9s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=None;, score=-631765.280 total time=   2.8s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-05, learning_rate=constant, penalty=None;, score=-615886.582 total time=   3.2s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268081898.851 total time=   5.9s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268269072.162 total time=   6.0s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269128277.896 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-269503842.357 total time=   6.3s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l1;, score=-268033199.451 total time=   6.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268081978.331 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268269622.716 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269127177.738 total time=   3.7s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-269503457.045 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=l2;, score=-268033908.805 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268085587.748 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268268937.655 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269126470.498 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-269504415.495 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=invscaling, penalty=None;, score=-268033592.874 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-20108053796502315334730745118720.000 total time=   7.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-30267980254579132362180695949312.000 total time=   7.6s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-10593001307216171505084667002880.000 total time=   7.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-13935595187730937404603777417216.000 total time=   7.5s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l1;, score=-19781243871119669750475882758144.000 total time=   7.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-16444498802768198598861772554240.000 total time=   3.6s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-40058842177269558058707582976000.000 total time=   3.6s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-10989555406505026742514664603648.000 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-21855005932925766003600897605632.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=l2;, score=-22396978258507515473727147999232.000 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-20256322022374790259065239896064.000 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-14492949120864866037100313575424.000 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-17886567450709957869203967967232.000 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-14196341721081827990424399118336.000 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=optimal, penalty=None;, score=-18575381401048712917226363027456.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-687658.869 total time=   5.8s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-727552.814 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-714748.262 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-710643.721 total time=   5.9s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l1;, score=-693411.046 total time=   5.8s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-687666.684 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-727546.238 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-714740.310 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-710662.083 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=l2;, score=-693411.425 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=None;, score=-687664.064 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=None;, score=-727563.116 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=None;, score=-714734.020 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=None;, score=-710657.448 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-06, learning_rate=constant, penalty=None;, score=-693390.752 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-392403354.656 total time=   6.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391948538.255 total time=   5.8s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393528816.442 total time=   5.9s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-393665466.329 total time=   6.0s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l1;, score=-391534356.135 total time=   5.9s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-392402871.791 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391949000.392 total time=   3.8s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393528095.762 total time=   3.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-393666932.384 total time=   3.7s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=l2;, score=-391534344.271 total time=   3.7s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-392403266.573 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391948642.294 total time=   3.4s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393529045.854 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-393665810.374 total time=   3.5s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=invscaling, penalty=None;, score=-391533675.684 total time=   3.5s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-15760434689835849300248008589312.000 total time=   7.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-15780449415791030592871184990208.000 total time=   7.5s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-17335055174740777470046668587008.000 total time=   7.7s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-35468542405054327837635765075968.000 total time=   7.9s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l1;, score=-15750178382297211277675686002688.000 total time=   7.8s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-17028226648603202394773728002048.000 total time=   3.5s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-9778844327612895245458598789120.000 total time=   3.7s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-26362592234654667945333829402624.000 total time=   3.6s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-8911684630077486410797319979008.000 total time=   3.6s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=l2;, score=-13344975084459385202013076717568.000 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-23419840895890295721567864225792.000 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-14885768029014402566950056099840.000 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-15199444745876843035375104425984.000 total time=   3.3s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-18788861330674642405917252911104.000 total time=   3.3s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=optimal, penalty=None;, score=-11607588251437220046401699840000.000 total time=   3.3s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76301196.905 total time=   5.9s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76592289.253 total time=   5.9s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76795720.575 total time=   6.0s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-77149487.145 total time=   6.1s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l1;, score=-76548811.288 total time=   6.2s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76301228.383 total time=   3.4s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76592298.908 total time=   3.5s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76795673.431 total time=   3.5s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-77149495.963 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=l2;, score=-76548830.939 total time=   3.4s\n",
            "[CV 1/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76301219.934 total time=   3.3s\n",
            "[CV 2/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76592361.450 total time=   3.3s\n",
            "[CV 3/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76795658.251 total time=   3.4s\n",
            "[CV 4/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=None;, score=-77149483.377 total time=   3.4s\n",
            "[CV 5/5] END alpha=1e-09, eta0=1e-07, learning_rate=constant, penalty=None;, score=-76548816.839 total time=   3.3s\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "grid_cv = GridSearchCV(estimator=sgd, param_grid=params, scoring='neg_mean_squared_error', cv=kf, verbose=3)\n",
        "grid_score = grid_cv.fit(x_train, y_train)\n",
        "\n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089f9872",
      "metadata": {
        "id": "089f9872",
        "outputId": "96a80f45-51b7-4b1f-f90a-6359cfb25281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "792.1795189392149"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(-grid_score.best_score_ ) ** 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98bfee93",
      "metadata": {
        "scrolled": true,
        "id": "98bfee93",
        "outputId": "378c1f5c-e107-4529-bcd8-fdf86738383a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([7.96488059e+02, 8.74851014e+02, 7.96413056e+02, 4.37408707e+10,\n",
              "       3.27757779e+05, 4.47533560e+10, 8.77293361e+02, 1.18030380e+03,\n",
              "       1.08688131e+03, 7.92247613e+02, 8.67392953e+02, 7.92656473e+02,\n",
              "       4.15636073e+10, 3.24061360e+05, 4.22235203e+10, 8.10733512e+02,\n",
              "       8.80222091e+02, 8.17952960e+02, 8.03445665e+02, 8.68748843e+02,\n",
              "       8.03513792e+02, 4.55763651e+10, 3.52123725e+05, 5.16488978e+10,\n",
              "       7.93499324e+02, 8.67921025e+02, 7.93508489e+02, 2.70143823e+03,\n",
              "       2.70888753e+03, 2.70146910e+03, 4.38613991e+10, 3.27605282e+05,\n",
              "       4.59703526e+10, 7.92179519e+02, 8.67090341e+02, 7.92228873e+02,\n",
              "       1.63891530e+04, 1.63897173e+04, 1.63891415e+04, 4.43236250e+10,\n",
              "       3.44178253e+05, 4.26407262e+10, 8.40693478e+02, 8.87842606e+02,\n",
              "       8.40716237e+02, 1.98145480e+04, 1.98145750e+04, 1.98145489e+04,\n",
              "       3.86878550e+10, 3.23633839e+05, 4.58308794e+10, 8.75655817e+03,\n",
              "       8.75785505e+03, 8.75657094e+03, 7.98944607e+02, 8.00648392e+02,\n",
              "       7.96625223e+02, 1.04273914e+07, 9.10412195e+02, 1.15842216e+07,\n",
              "       9.37758293e+02, 1.14720024e+03, 1.23988263e+03, 7.92316698e+02,\n",
              "       7.96126688e+02, 7.92385016e+02, 1.01424467e+07, 8.81942213e+02,\n",
              "       1.03428704e+07, 8.04884397e+02, 8.16082447e+02, 8.11989959e+02,\n",
              "       8.03475321e+02, 8.07979827e+02, 8.03465969e+02, 7.78177267e+06,\n",
              "       9.15985795e+02, 7.67941699e+06, 7.93440595e+02, 7.96632365e+02,\n",
              "       7.93728026e+02, 2.70146992e+03, 2.70217018e+03, 2.70148329e+03,\n",
              "       1.10893160e+07, 8.86593554e+02, 9.41297596e+06, 7.92338459e+02,\n",
              "       7.95755490e+02, 7.92264095e+02, 1.63891334e+04, 1.63891946e+04,\n",
              "       1.63891344e+04, 8.25354455e+06, 8.93140490e+02, 1.10452365e+07,\n",
              "       8.40715491e+02, 8.44686410e+02, 8.40714589e+02, 1.98145469e+04,\n",
              "       1.98145457e+04, 1.98145491e+04, 1.35145479e+07, 9.39320851e+02,\n",
              "       8.04329543e+06, 8.75656815e+03, 8.75669100e+03, 8.75657025e+03,\n",
              "       7.97792717e+02, 8.00390816e+02, 7.99725962e+02, 7.97778812e+02,\n",
              "       7.98889427e+02, 7.97591797e+02, 1.11110674e+03, 1.21134735e+03,\n",
              "       1.06883192e+03, 7.92357215e+02, 7.92388592e+02, 7.92433928e+02,\n",
              "       8.00672621e+02, 8.00081317e+02, 7.98722660e+02, 8.08248804e+02,\n",
              "       8.08081675e+02, 8.10732825e+02, 8.03480104e+02, 8.03883125e+02,\n",
              "       8.03458830e+02, 7.97667775e+02, 7.98898057e+02, 7.98920601e+02,\n",
              "       7.93113819e+02, 7.93534081e+02, 7.93438259e+02, 2.70149598e+03,\n",
              "       2.70152322e+03, 2.70149038e+03, 7.98046146e+02, 7.97410793e+02,\n",
              "       7.99538348e+02, 7.92326820e+02, 7.92429723e+02, 7.92441813e+02,\n",
              "       1.63891289e+04, 1.63891316e+04, 1.63891159e+04, 7.99071947e+02,\n",
              "       7.97140853e+02, 7.97355374e+02, 8.40716077e+02, 8.41099053e+02,\n",
              "       8.40716191e+02, 1.98145457e+04, 1.98145474e+04, 1.98145480e+04,\n",
              "       8.01259196e+02, 7.97305508e+02, 7.98291606e+02, 8.75656960e+03,\n",
              "       8.75658137e+03, 8.75656985e+03, 7.95291747e+02, 7.97676697e+02,\n",
              "       7.98142126e+02, 8.23291885e+02, 8.38851244e+02, 8.32293679e+02,\n",
              "       1.09997190e+03, 1.14545852e+03, 1.11469771e+03, 7.92532783e+02,\n",
              "       7.92302864e+02, 7.92449508e+02, 8.37432897e+02, 8.29573917e+02,\n",
              "       8.43228697e+02, 8.12679199e+02, 8.12151571e+02, 8.15511084e+02,\n",
              "       8.03450255e+02, 8.03488568e+02, 8.03506287e+02, 8.38127977e+02,\n",
              "       8.47176240e+02, 8.34795898e+02, 7.94237394e+02, 7.93200189e+02,\n",
              "       7.93409857e+02, 2.70146104e+03, 2.70148319e+03, 2.70148738e+03,\n",
              "       8.41832826e+02, 8.43018002e+02, 8.38200209e+02, 7.92239537e+02,\n",
              "       7.92304456e+02, 7.92261657e+02, 1.63891435e+04, 1.63891260e+04,\n",
              "       1.63891309e+04, 8.41904815e+02, 8.43986655e+02, 8.43591040e+02,\n",
              "       8.40715693e+02, 8.40750548e+02, 8.40718241e+02, 1.98145431e+04,\n",
              "       1.98145486e+04, 1.98145437e+04, 8.26461094e+02, 8.44543655e+02,\n",
              "       8.42347906e+02, 8.75656980e+03, 8.75657232e+03, 8.75656938e+03,\n",
              "       7.98575158e+02, 7.99815175e+02, 7.94788146e+02, 1.36384184e+03,\n",
              "       1.75413731e+03, 1.50454635e+03, 1.25730762e+03, 1.19319637e+03,\n",
              "       1.15558310e+03, 7.92497662e+02, 7.92379475e+02, 7.92482512e+02,\n",
              "       1.54090691e+03, 1.53425568e+03, 1.95029976e+03, 8.06973512e+02,\n",
              "       8.11956806e+02, 8.10278379e+02, 8.03487960e+02, 8.03431278e+02,\n",
              "       8.03460191e+02, 1.50957815e+03, 2.06531091e+03, 1.52076220e+03,\n",
              "       7.93199396e+02, 7.93583936e+02, 7.94379494e+02, 2.70145458e+03,\n",
              "       2.70148612e+03, 2.70147269e+03, 1.75721204e+03, 1.86026067e+03,\n",
              "       1.59258282e+03, 7.92364370e+02, 7.92204727e+02, 7.92310554e+02,\n",
              "       1.63891309e+04, 1.63891144e+04, 1.63891433e+04, 1.36585591e+03,\n",
              "       1.40505139e+03, 1.69963045e+03, 8.40718259e+02, 8.40720609e+02,\n",
              "       8.40719328e+02, 1.98145476e+04, 1.98145445e+04, 1.98145464e+04,\n",
              "       1.50454255e+03, 1.80965414e+03, 3.07775621e+03, 8.75656892e+03,\n",
              "       8.75656881e+03, 8.75656790e+03, 7.97628855e+02, 7.98177377e+02,\n",
              "       7.97225235e+02, 6.46550203e+12, 5.67296219e+12, 5.90313779e+12,\n",
              "       1.10925572e+03, 1.29221286e+03, 1.19340509e+03, 7.92253743e+02,\n",
              "       7.92619106e+02, 7.92306249e+02, 5.94413774e+12, 7.90506398e+12,\n",
              "       7.71790385e+12, 8.16127119e+02, 8.21746061e+02, 8.08024602e+02,\n",
              "       8.03440169e+02, 8.03465550e+02, 8.03475865e+02, 6.44061293e+12,\n",
              "       6.29316229e+12, 7.08682318e+12, 7.93201308e+02, 7.92856557e+02,\n",
              "       7.94144724e+02, 2.70146859e+03, 2.70149364e+03, 2.70147447e+03,\n",
              "       6.89742036e+12, 6.05896983e+12, 6.63188491e+12, 7.92300958e+02,\n",
              "       7.92265125e+02, 7.92378671e+02, 1.63891149e+04, 1.63891234e+04,\n",
              "       1.63891183e+04, 5.70864167e+12, 6.98995327e+12, 6.80001972e+12,\n",
              "       8.40716883e+02, 8.40715946e+02, 8.40717114e+02, 1.98145443e+04,\n",
              "       1.98145440e+04, 1.98145475e+04, 7.16518387e+12, 7.08458515e+12,\n",
              "       6.64566753e+12, 8.75656858e+03, 8.75656923e+03, 8.75656940e+03,\n",
              "       7.99151354e+02, 7.98212680e+02, 7.96623889e+02, 5.79519113e+13,\n",
              "       7.33530542e+13, 8.09126430e+13, 1.13402320e+03, 1.14362082e+03,\n",
              "       1.28527853e+03, 7.92643352e+02, 7.92221496e+02, 7.92734268e+02,\n",
              "       6.85865408e+13, 7.70709821e+13, 6.96997375e+13, 8.16553937e+02,\n",
              "       8.10225401e+02, 8.12946882e+02, 8.03472537e+02, 8.03484598e+02,\n",
              "       8.03455188e+02, 7.42627265e+13, 6.13704436e+13, 7.22073904e+13,\n",
              "       7.92741591e+02, 7.94247814e+02, 7.93060314e+02, 2.70148994e+03,\n",
              "       2.70151478e+03, 2.70147090e+03, 6.97233974e+13, 7.32506270e+13,\n",
              "       7.21621641e+13, 7.92441558e+02, 7.92293577e+02, 7.92263757e+02,\n",
              "       1.63891390e+04, 1.63891345e+04, 1.63891475e+04, 7.32081116e+13,\n",
              "       6.29211182e+13, 7.30590655e+13, 8.40716663e+02, 8.40719809e+02,\n",
              "       8.40719298e+02, 1.98145493e+04, 1.98145471e+04, 1.98145506e+04,\n",
              "       7.15591406e+13, 8.08624974e+13, 6.78357649e+13, 8.75656870e+03,\n",
              "       8.75656926e+03, 8.75656937e+03, 7.97286094e+02, 7.97081079e+02,\n",
              "       7.97233774e+02, 5.84755026e+14, 6.82282354e+14, 7.13463024e+14,\n",
              "       1.13333990e+03, 1.12866764e+03, 1.13161537e+03, 7.92295044e+02,\n",
              "       7.92190785e+02, 7.92458292e+02, 6.20607320e+14, 7.10696208e+14,\n",
              "       6.77598490e+14, 8.10012703e+02, 8.08847766e+02, 8.07661173e+02,\n",
              "       8.03492472e+02, 8.03462711e+02, 8.03471900e+02, 6.32584531e+14,\n",
              "       6.86766045e+14, 5.63414621e+14, 7.93461928e+02, 7.93350611e+02,\n",
              "       7.93607196e+02, 2.70146620e+03, 2.70143917e+03, 2.70146610e+03,\n",
              "       6.28596220e+14, 7.73009168e+14, 6.32835325e+14, 7.92350576e+02,\n",
              "       7.92280693e+02, 7.92193818e+02, 1.63891323e+04, 1.63891235e+04,\n",
              "       1.63891311e+04, 6.25388640e+14, 6.42255975e+14, 6.25811542e+14,\n",
              "       8.40721023e+02, 8.40718332e+02, 8.40713595e+02, 1.98145444e+04,\n",
              "       1.98145500e+04, 1.98145497e+04, 6.25111762e+14, 7.13893649e+14,\n",
              "       7.69601921e+14, 8.75656937e+03, 8.75656918e+03, 8.75657022e+03,\n",
              "       7.96418339e+02, 7.95705845e+02, 7.97010277e+02, 4.58332784e+15,\n",
              "       4.74082067e+15, 4.05682146e+15, 1.15236844e+03, 1.13238242e+03,\n",
              "       1.26278087e+03, 7.92187634e+02, 7.92372981e+02, 7.92293741e+02,\n",
              "       4.04592868e+15, 4.79457635e+15, 4.21895174e+15, 8.06197033e+02,\n",
              "       8.10683571e+02, 8.18077149e+02, 8.03448344e+02, 8.03472832e+02,\n",
              "       8.03506253e+02, 4.72353736e+15, 3.86183311e+15, 4.35611628e+15,\n",
              "       7.92889335e+02, 7.93165453e+02, 7.93356191e+02, 2.70148281e+03,\n",
              "       2.70143754e+03, 2.70150051e+03, 5.12070160e+15, 4.24781746e+15,\n",
              "       4.48675728e+15, 7.92260248e+02, 7.92327244e+02, 7.92350569e+02,\n",
              "       1.63891201e+04, 1.63891192e+04, 1.63891367e+04, 4.35168644e+15,\n",
              "       4.72747037e+15, 4.13297863e+15, 8.40715732e+02, 8.40717163e+02,\n",
              "       8.40715100e+02, 1.98145428e+04, 1.98145464e+04, 1.98145423e+04,\n",
              "       4.47425212e+15, 3.88397536e+15, 4.09637653e+15, 8.75656902e+03,\n",
              "       8.75656928e+03, 8.75656942e+03])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# переводим score в rmse\n",
        "grid_score.cv_results_['mean_test_score'] = (-1 * grid_score.cv_results_['mean_test_score']) ** 0.5\n",
        "grid_score.cv_results_['mean_test_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deca680b",
      "metadata": {
        "id": "deca680b",
        "outputId": "5cf419c2-87d5-4013-839f-9aa065dfe857"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': 'l1'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b94de6",
      "metadata": {
        "id": "b7b94de6",
        "outputId": "247b8893-f524-4018-faf2-e787f28b7c04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.75836887, 0.48647685, 0.48384309, 6.75556188, 4.37273717,\n",
              "        3.39109383, 0.56091199, 0.18526583, 0.19389601, 1.90304751,\n",
              "        0.51140771, 1.04461617, 5.96534982, 3.37837071, 3.41933351,\n",
              "        0.59103599, 0.21471167, 0.20603518, 7.73421578, 3.75896597,\n",
              "        3.56897645, 6.9439188 , 3.99856668, 3.97069569, 0.97622371,\n",
              "        0.39290843, 0.52783351, 6.70503526, 3.97028689, 3.80228925,\n",
              "        6.31631508, 3.76405921, 3.7009975 , 5.88258023, 1.55915542,\n",
              "        3.19672699, 6.49431047, 3.83118997, 3.79690762, 6.4861764 ,\n",
              "        3.67436357, 3.63357806, 6.38264766, 3.642202  , 3.65275364,\n",
              "        6.47857757, 3.93990536, 3.77005525, 6.53968763, 3.81370077,\n",
              "        3.71395383, 6.38263507, 3.63927436, 3.93202801, 0.90372987,\n",
              "        0.52549419, 0.52917237, 6.4268333 , 3.57150407, 3.6752665 ,\n",
              "        1.3261909 , 0.19503279, 0.23779154, 1.8164392 , 0.9759798 ,\n",
              "        0.96690869, 6.38625932, 3.64875484, 3.78883033, 0.39839025,\n",
              "        0.27222881, 0.25416861, 6.6356895 , 3.91842523, 3.52944155,\n",
              "        5.82645278, 3.40713472, 3.69522691, 0.8920682 , 0.48658161,\n",
              "        0.5672276 , 6.42294617, 3.73613162, 3.76682234, 6.58844733,\n",
              "        3.65820808, 3.72265086, 5.19135947, 3.15541525, 2.85974855,\n",
              "        6.52685199, 3.83805332, 3.75478425, 6.46200194, 3.68100004,\n",
              "        3.69232001, 6.47556405, 3.67599721, 3.60640154, 6.85353894,\n",
              "        3.92507834, 4.09124088, 6.38249426, 3.75251207, 3.69272261,\n",
              "        6.38192153, 3.83250656, 3.63179679, 0.90486159, 0.52406769,\n",
              "        0.46529136, 2.22597842, 1.41366596, 1.3176393 , 0.58515749,\n",
              "        0.22668896, 0.19920397, 2.04430337, 1.32379565, 1.1255507 ,\n",
              "        1.90019817, 1.02441754, 1.18282266, 0.38248906, 0.26127725,\n",
              "        0.23142748, 6.45611029, 4.10672793, 4.02288022, 2.0031754 ,\n",
              "        0.85966825, 1.01262574, 0.74331427, 0.53573337, 0.54286313,\n",
              "        6.04644771, 3.60844908, 3.35623922, 1.83908658, 1.04995551,\n",
              "        1.11891193, 4.69028025, 2.67260146, 2.74591837, 6.02347808,\n",
              "        3.49693136, 3.4631197 , 1.85541301, 1.09414544, 1.09456043,\n",
              "        5.81615343, 3.39473634, 3.30440779, 5.94474807, 3.47184558,\n",
              "        3.45111861, 1.99590883, 1.0320179 , 1.12541614, 5.96817808,\n",
              "        3.3767755 , 3.27262607, 0.67476764, 0.43849025, 0.47260151,\n",
              "        4.02864137, 1.75827861, 2.0501411 , 0.36858001, 0.2468195 ,\n",
              "        0.22419887, 1.73143229, 1.07664914, 1.056214  , 3.51955471,\n",
              "        1.91769261, 1.81810398, 0.3523015 , 0.2369915 , 0.20825791,\n",
              "        6.20622311, 3.460463  , 3.48057179, 3.52520413, 1.95928054,\n",
              "        1.84131579, 0.78911109, 0.48368807, 0.50775857, 6.0762445 ,\n",
              "        3.45061574, 3.43733954, 3.97748456, 1.95017233, 1.76258502,\n",
              "        4.8994648 , 2.77803664, 2.87658987, 5.89006834, 3.45633364,\n",
              "        3.59778314, 3.43614793, 1.66790423, 1.7346725 , 5.82541747,\n",
              "        3.34381552, 3.34480109, 6.14654961, 3.51848159, 3.42609468,\n",
              "        3.80943265, 1.75174246, 1.80758629, 5.84842844, 3.38665171,\n",
              "        3.41161056, 0.71177506, 0.42960048, 0.54126058, 7.04409347,\n",
              "        2.90620985, 3.04227667, 0.4291595 , 0.20654349, 0.21058846,\n",
              "        1.73857837, 1.03273606, 0.92806034, 6.94842229, 3.07467513,\n",
              "        2.91140127, 0.40863242, 0.25656242, 0.22589688, 6.12102585,\n",
              "        3.59944682, 3.49937029, 6.64968266, 2.83077865, 3.04462023,\n",
              "        0.87358775, 0.49945955, 0.50763459, 6.38676066, 3.47612128,\n",
              "        3.54567127, 6.67302547, 3.01030951, 3.1657382 , 4.68691096,\n",
              "        2.8861567 , 2.74383087, 6.07935333, 3.54175706, 3.50442576,\n",
              "        6.88450413, 3.01167026, 2.77066655, 5.99756885, 3.37642951,\n",
              "        3.31920791, 6.02893467, 3.59199748, 3.44290781, 6.79691443,\n",
              "        2.94194193, 3.01595511, 5.85781646, 3.45117235, 3.30971336,\n",
              "        0.88974156, 0.48294482, 0.50944571, 7.64516768, 3.42865782,\n",
              "        3.38456111, 0.39719934, 0.20387239, 0.21560316, 1.65002537,\n",
              "        1.05822473, 1.02009721, 7.62083378, 3.46753736, 3.44659886,\n",
              "        0.37585359, 0.26742797, 0.22638531, 6.08816218, 3.56962662,\n",
              "        3.58437705, 7.58021817, 3.4818851 , 3.45455422, 0.80323367,\n",
              "        0.48116293, 0.48333254, 6.03624692, 3.61339521, 3.53422494,\n",
              "        7.47075949, 3.59198699, 3.34831386, 4.95519042, 2.84740067,\n",
              "        2.62631106, 6.04120913, 3.56357737, 3.47415557, 7.43869724,\n",
              "        3.49553318, 3.44924688, 5.84766607, 3.39082112, 3.37541528,\n",
              "        6.0419508 , 3.55356855, 3.49449725, 7.64485188, 3.49935684,\n",
              "        3.36813726, 5.84146461, 3.61902118, 3.38122973, 0.76352029,\n",
              "        0.37456875, 0.41805463, 7.69346871, 3.49547024, 3.37482395,\n",
              "        0.40896969, 0.19595585, 0.17893858, 1.67903304, 1.02220616,\n",
              "        0.88203387, 7.70320439, 3.46567793, 3.41966925, 0.40249181,\n",
              "        0.28036013, 0.26149168, 6.33044715, 3.49516788, 3.55992017,\n",
              "        7.57692237, 3.39539256, 3.50713305, 0.85130944, 0.47492256,\n",
              "        0.52341452, 6.20690422, 3.61134944, 3.44663291, 7.43140321,\n",
              "        3.51152325, 3.41513934, 4.82745008, 2.74557762, 2.80415602,\n",
              "        6.0509573 , 3.4984859 , 3.66298952, 7.4816349 , 3.47593026,\n",
              "        3.44707823, 5.88179708, 3.45802445, 3.39807076, 5.96238141,\n",
              "        3.52773857, 3.51518455, 7.48777556, 3.50999022, 3.36640668,\n",
              "        5.97438145, 3.42111673, 3.40959435, 0.7620048 , 0.4272459 ,\n",
              "        0.50084295, 7.71845288, 3.44400764, 3.43356638, 0.30997238,\n",
              "        0.21215343, 0.19970765, 1.65445228, 1.03656139, 0.99864635,\n",
              "        7.66013041, 3.41770554, 3.41999383, 0.36453004, 0.22914271,\n",
              "        0.21784415, 5.99626222, 3.58333631, 3.58183804, 7.56476789,\n",
              "        3.46537528, 3.48445134, 0.81885991, 0.52775383, 0.53469343,\n",
              "        6.03251219, 3.58862624, 3.47629404, 7.60165191, 3.51905947,\n",
              "        3.36452928, 4.63814931, 2.71147103, 2.99328012, 6.07996798,\n",
              "        3.52677088, 3.63686609, 7.59879079, 3.53319006, 3.4434998 ,\n",
              "        5.92879052, 3.38886547, 3.41766725, 6.06971388, 3.48395033,\n",
              "        3.52667933, 7.60579467, 3.50063472, 3.43808327, 5.84035764,\n",
              "        3.45215292, 3.34597368, 0.66874599, 0.45140443, 0.57699366,\n",
              "        7.66555796, 3.50673318, 3.33820009, 0.34550357, 0.19366741,\n",
              "        0.18941426, 1.73382683, 0.99768496, 1.01605997, 7.60740733,\n",
              "        3.42388458, 3.44892488, 0.3844152 , 0.24722977, 0.26893806,\n",
              "        6.1481319 , 4.01936259, 4.15285559, 8.01693263, 3.51861129,\n",
              "        3.51081414, 0.78959885, 0.50114589, 0.4978055 , 6.09480019,\n",
              "        3.63229814, 3.4587944 , 7.61183796, 3.51699533, 3.34603209,\n",
              "        5.06164474, 2.81379704, 2.90852704, 6.12018909, 3.55884247,\n",
              "        3.45339122, 7.50088415, 3.58611574, 3.38377891, 5.82173643,\n",
              "        3.46393404, 3.34147968, 6.02995772, 3.63274183, 3.45960846,\n",
              "        7.69469986, 3.55079103, 3.33634338, 6.03222704, 3.43869333,\n",
              "        3.33646364]),\n",
              " 'std_fit_time': array([0.11246311, 0.08593921, 0.1204308 , 0.32928808, 0.6073439 ,\n",
              "        0.07500739, 0.04660833, 0.01262377, 0.03655506, 0.18962581,\n",
              "        0.09144013, 0.15282162, 0.08490404, 0.04127336, 0.08741298,\n",
              "        0.1344289 , 0.05996313, 0.02606738, 0.71846892, 0.11391912,\n",
              "        0.10973289, 0.27214201, 0.3136025 , 0.16721208, 0.09212868,\n",
              "        0.04401122, 0.06293232, 0.286797  , 0.27832939, 0.04793428,\n",
              "        0.15308386, 0.10812702, 0.08331817, 0.46691667, 0.16686371,\n",
              "        0.3455709 , 0.14731696, 0.10151049, 0.08652533, 0.13986458,\n",
              "        0.09964724, 0.14296455, 0.27823252, 0.07496923, 0.08220134,\n",
              "        0.14348284, 0.16132403, 0.12232387, 0.36687826, 0.30033576,\n",
              "        0.12249964, 0.17744142, 0.14487787, 0.14403931, 0.35864758,\n",
              "        0.13725933, 0.07616741, 0.08278563, 0.04268464, 0.13085256,\n",
              "        0.30929881, 0.02255289, 0.06029847, 0.21522035, 0.06121392,\n",
              "        0.14042151, 0.11075758, 0.11356684, 0.10015936, 0.09858793,\n",
              "        0.0817636 , 0.02656847, 0.20263421, 0.19763203, 0.13239207,\n",
              "        0.09226637, 0.3050959 , 0.1154465 , 0.08954305, 0.05723851,\n",
              "        0.02228178, 0.16017671, 0.07707982, 0.12486563, 0.13261432,\n",
              "        0.06316447, 0.14731929, 0.29084448, 0.32609248, 0.17001305,\n",
              "        0.13061321, 0.13929144, 0.13617183, 0.17911542, 0.06158275,\n",
              "        0.13699742, 0.09203723, 0.04928068, 0.1314416 , 0.43942921,\n",
              "        0.08496243, 0.15789415, 0.22133249, 0.12351646, 0.17290381,\n",
              "        0.19292545, 0.2073708 , 0.10681924, 0.13870327, 0.07731745,\n",
              "        0.03787442, 0.44145676, 0.13662627, 0.14254589, 0.29146441,\n",
              "        0.05078004, 0.01581032, 0.36102922, 0.05180496, 0.07768387,\n",
              "        0.21799645, 0.19371907, 0.21654991, 0.10136849, 0.05536097,\n",
              "        0.03059271, 0.23394389, 0.25412248, 0.47040697, 0.06285049,\n",
              "        0.05941047, 0.18111365, 0.05437276, 0.06298062, 0.09010701,\n",
              "        0.1628183 , 0.13348745, 0.04805158, 0.37631509, 0.11712657,\n",
              "        0.19259461, 0.1922894 , 0.23357066, 0.18930957, 0.17605076,\n",
              "        0.08761069, 0.10388633, 0.40550049, 0.14592017, 0.13530697,\n",
              "        0.1484547 , 0.09016344, 0.05877729, 0.11053814, 0.07617256,\n",
              "        0.10807921, 0.39147198, 0.21487086, 0.15259504, 0.21203408,\n",
              "        0.13734925, 0.05578856, 0.13599503, 0.11809739, 0.05135679,\n",
              "        0.86151855, 0.35902121, 0.2650274 , 0.07058064, 0.06999404,\n",
              "        0.03355723, 0.20853458, 0.15746741, 0.09404942, 0.61973185,\n",
              "        0.32887551, 0.30774274, 0.06483947, 0.03767716, 0.01216049,\n",
              "        0.21496869, 0.09870068, 0.07985241, 0.09478894, 0.33433686,\n",
              "        0.09314117, 0.07565617, 0.02428902, 0.04523266, 0.09854766,\n",
              "        0.05150029, 0.09636715, 0.27324106, 0.46167868, 0.18672091,\n",
              "        0.26173057, 0.22047721, 0.1710515 , 0.07291536, 0.06194635,\n",
              "        0.13079618, 0.38696853, 0.09280778, 0.04324655, 0.15550767,\n",
              "        0.08040715, 0.07111335, 0.21051383, 0.12723349, 0.07248353,\n",
              "        0.16747705, 0.29771869, 0.36766597, 0.0722843 , 0.12406815,\n",
              "        0.09150216, 0.1608128 , 0.05616172, 0.0899699 , 0.50641726,\n",
              "        0.33718881, 0.11667046, 0.07953753, 0.01946135, 0.03835229,\n",
              "        0.34076896, 0.12660766, 0.10231019, 0.67295768, 0.23511319,\n",
              "        0.33393557, 0.09142955, 0.03338743, 0.03058774, 0.09329203,\n",
              "        0.1394029 , 0.10512729, 0.4559173 , 0.35407419, 0.27891073,\n",
              "        0.08284461, 0.03937396, 0.09419234, 0.35487248, 0.0249679 ,\n",
              "        0.12462888, 0.83819256, 0.35518917, 0.25940723, 0.31858156,\n",
              "        0.26071354, 0.14363581, 0.15754484, 0.08768981, 0.06870949,\n",
              "        0.68584306, 0.30350872, 0.2816572 , 0.33203058, 0.08261234,\n",
              "        0.08546427, 0.15523613, 0.09166711, 0.03282118, 0.68232817,\n",
              "        0.26679191, 0.28374881, 0.16168317, 0.09371908, 0.02672132,\n",
              "        0.29200004, 0.12024571, 0.15799713, 0.13172117, 0.07606419,\n",
              "        0.06465558, 0.07856161, 0.01979779, 0.04092579, 0.12188253,\n",
              "        0.13549803, 0.15872635, 0.23928973, 0.07881535, 0.1269698 ,\n",
              "        0.07910671, 0.06062528, 0.0301713 , 0.14995389, 0.04872936,\n",
              "        0.10783276, 0.03772219, 0.06656752, 0.0677771 , 0.08026482,\n",
              "        0.043512  , 0.03894629, 0.15449038, 0.10759333, 0.15568955,\n",
              "        0.12892807, 0.12071077, 0.02837547, 0.34477136, 0.132816  ,\n",
              "        0.14588698, 0.08915462, 0.14556372, 0.05883936, 0.06138972,\n",
              "        0.11754111, 0.11468682, 0.08173305, 0.12180804, 0.1139631 ,\n",
              "        0.2854546 , 0.12482649, 0.10753178, 0.13912185, 0.12147545,\n",
              "        0.04413354, 0.16651385, 0.1016948 , 0.0780153 , 0.13617498,\n",
              "        0.08724304, 0.10611587, 0.25263343, 0.12241353, 0.04228398,\n",
              "        0.06781652, 0.02463437, 0.00898717, 0.23836041, 0.07918155,\n",
              "        0.05768899, 0.33513245, 0.07240177, 0.11421939, 0.07577112,\n",
              "        0.03525992, 0.04357792, 0.27130815, 0.06838295, 0.13763066,\n",
              "        0.32343202, 0.06661421, 0.1112959 , 0.10094511, 0.02314195,\n",
              "        0.07828851, 0.19055462, 0.10358219, 0.06012909, 0.06361288,\n",
              "        0.14356428, 0.08464638, 0.53266968, 0.27849665, 0.10603197,\n",
              "        0.19665044, 0.07063449, 0.26359843, 0.08392536, 0.15621479,\n",
              "        0.14760254, 0.15053993, 0.19493308, 0.06411392, 0.10962816,\n",
              "        0.16125077, 0.13810806, 0.17094352, 0.15348235, 0.02736409,\n",
              "        0.25813931, 0.16622368, 0.14692456, 0.18702093, 0.10481906,\n",
              "        0.06607959, 0.21993273, 0.14394563, 0.1499532 , 0.05791138,\n",
              "        0.03892257, 0.02300203, 0.15482391, 0.09112912, 0.14410881,\n",
              "        0.17302612, 0.0959507 , 0.16766434, 0.08242455, 0.03221338,\n",
              "        0.04786521, 0.09919569, 0.04116036, 0.2189227 , 0.1289386 ,\n",
              "        0.10461797, 0.17002528, 0.04934465, 0.05913598, 0.05614629,\n",
              "        0.11045258, 0.17401447, 0.06987671, 0.23404403, 0.20975466,\n",
              "        0.04358146, 0.20095682, 0.18744454, 0.24878469, 0.13690123,\n",
              "        0.15150429, 0.17563154, 0.17869061, 0.102995  , 0.15721647,\n",
              "        0.15850296, 0.13848456, 0.09193583, 0.20670663, 0.11876275,\n",
              "        0.1418723 , 0.24335086, 0.13928492, 0.13934388, 0.05940592,\n",
              "        0.09397591, 0.10605691, 0.11551411, 0.10988463, 0.16041901,\n",
              "        0.31920815, 0.11148273, 0.00811488, 0.05574291, 0.01748375,\n",
              "        0.02207147, 0.26405456, 0.18512863, 0.11452523, 0.21259834,\n",
              "        0.08807628, 0.1221246 , 0.09899805, 0.03984097, 0.03126011,\n",
              "        0.21279623, 0.08179897, 0.12012225, 0.21218866, 0.12360636,\n",
              "        0.10316536, 0.08805689, 0.02252502, 0.04536792, 0.16340249,\n",
              "        0.12520674, 0.02896739, 0.18636455, 0.13165208, 0.00978764,\n",
              "        0.23585957, 0.23514019, 0.18193395, 0.21844583, 0.12482703,\n",
              "        0.0469476 , 0.08258584, 0.05109819, 0.03336193, 0.04724062,\n",
              "        0.08355199, 0.06509848, 0.23437645, 0.12686096, 0.04106901,\n",
              "        0.15064092, 0.10271683, 0.00455294, 0.13566368, 0.05895873,\n",
              "        0.05801817]),\n",
              " 'mean_score_time': array([0.00172586, 0.00179157, 0.00213537, 0.00255613, 0.00172181,\n",
              "        0.00155425, 0.00184579, 0.00167069, 0.00155482, 0.0018899 ,\n",
              "        0.0018949 , 0.00157642, 0.00191898, 0.00158639, 0.00164366,\n",
              "        0.00173216, 0.00160518, 0.00193233, 0.00207701, 0.00161934,\n",
              "        0.00169482, 0.00186396, 0.00187755, 0.0018383 , 0.00248933,\n",
              "        0.00173988, 0.00189037, 0.00193453, 0.00175462, 0.00187359,\n",
              "        0.00187516, 0.00204396, 0.0018013 , 0.00231113, 0.00175982,\n",
              "        0.00161691, 0.00217957, 0.00172544, 0.0017767 , 0.00202932,\n",
              "        0.00181417, 0.00177069, 0.0018641 , 0.00167384, 0.00168662,\n",
              "        0.00187898, 0.00174055, 0.0015748 , 0.00198827, 0.00264912,\n",
              "        0.00164495, 0.00176682, 0.00161047, 0.00182133, 0.0018569 ,\n",
              "        0.00172958, 0.0016696 , 0.00182495, 0.00165973, 0.00176911,\n",
              "        0.00211334, 0.00205803, 0.00179753, 0.00185828, 0.00173221,\n",
              "        0.00217199, 0.00173106, 0.00170798, 0.00178165, 0.00175848,\n",
              "        0.00184112, 0.00175996, 0.00191355, 0.00190105, 0.00174718,\n",
              "        0.00190444, 0.00153518, 0.00157938, 0.00208807, 0.0020236 ,\n",
              "        0.0020843 , 0.00175014, 0.00206079, 0.00218115, 0.00207195,\n",
              "        0.00171247, 0.00173874, 0.00219564, 0.00156116, 0.00207534,\n",
              "        0.00181222, 0.00166287, 0.00167203, 0.00179267, 0.00180392,\n",
              "        0.00178232, 0.00213027, 0.00249014, 0.00177255, 0.0019608 ,\n",
              "        0.0018609 , 0.0018621 , 0.00180931, 0.00156841, 0.00176511,\n",
              "        0.00205064, 0.00194693, 0.00160313, 0.00195303, 0.00233583,\n",
              "        0.00182862, 0.00179667, 0.00289121, 0.00174007, 0.00228167,\n",
              "        0.00204353, 0.00175691, 0.00177984, 0.00175939, 0.00178919,\n",
              "        0.00198245, 0.00170321, 0.00196409, 0.00169973, 0.00178814,\n",
              "        0.00156631, 0.00184655, 0.00169272, 0.00168104, 0.0019907 ,\n",
              "        0.00158167, 0.00160532, 0.00191545, 0.00166602, 0.00190549,\n",
              "        0.00203371, 0.0016748 , 0.00160303, 0.00186501, 0.00158992,\n",
              "        0.00158739, 0.00165672, 0.00155568, 0.00159283, 0.00183945,\n",
              "        0.00164313, 0.00162044, 0.00170302, 0.00164485, 0.00172482,\n",
              "        0.00218768, 0.00164838, 0.00154352, 0.00189857, 0.00157132,\n",
              "        0.00157166, 0.00198441, 0.00155411, 0.00158362, 0.00201054,\n",
              "        0.00158682, 0.00156851, 0.00201659, 0.00162292, 0.00162706,\n",
              "        0.00193367, 0.00162797, 0.00166302, 0.00199261, 0.00164728,\n",
              "        0.00156598, 0.00193076, 0.00161743, 0.00157809, 0.002039  ,\n",
              "        0.00154409, 0.00158668, 0.00200233, 0.0017457 , 0.00176687,\n",
              "        0.00192266, 0.00161605, 0.00177202, 0.0019702 , 0.00155659,\n",
              "        0.0015512 , 0.00193548, 0.00162535, 0.00156198, 0.00196066,\n",
              "        0.00165429, 0.00157008, 0.0019959 , 0.00168581, 0.00159874,\n",
              "        0.00185952, 0.00169396, 0.00155549, 0.00215178, 0.0015955 ,\n",
              "        0.00165477, 0.00183644, 0.0015954 , 0.00164165, 0.00165763,\n",
              "        0.00157342, 0.00166221, 0.00189981, 0.001581  , 0.00162959,\n",
              "        0.00185232, 0.00166178, 0.00168443, 0.00214186, 0.00158415,\n",
              "        0.00160093, 0.0019731 , 0.00159264, 0.0015645 , 0.00179276,\n",
              "        0.0015696 , 0.00164528, 0.00189033, 0.00167699, 0.00159001,\n",
              "        0.00175514, 0.00158801, 0.00167108, 0.00211606, 0.00155635,\n",
              "        0.00156302, 0.00212312, 0.00161543, 0.00179949, 0.00191035,\n",
              "        0.00158691, 0.00160427, 0.00197005, 0.00160928, 0.00158648,\n",
              "        0.00203137, 0.00158486, 0.00157557, 0.00158772, 0.00163784,\n",
              "        0.00167212, 0.00214939, 0.00153513, 0.00159106, 0.00193405,\n",
              "        0.00165677, 0.0016242 , 0.00179219, 0.00164461, 0.00163784,\n",
              "        0.00176244, 0.00155725, 0.00157475, 0.00171819, 0.00159039,\n",
              "        0.00157051, 0.00196943, 0.00158162, 0.00162635, 0.00199986,\n",
              "        0.0015449 , 0.00154181, 0.00215764, 0.00161901, 0.0015449 ,\n",
              "        0.00210915, 0.00155821, 0.00171227, 0.00233474, 0.00167012,\n",
              "        0.00155311, 0.00177183, 0.00161557, 0.00159521, 0.0021338 ,\n",
              "        0.00153036, 0.00157571, 0.00199871, 0.00154696, 0.00162358,\n",
              "        0.00200014, 0.00169735, 0.00177722, 0.00197048, 0.00167603,\n",
              "        0.00161738, 0.00214686, 0.00162902, 0.00160236, 0.00182834,\n",
              "        0.00158997, 0.00155659, 0.0017705 , 0.00157576, 0.00162964,\n",
              "        0.00210333, 0.00159059, 0.00157285, 0.00182886, 0.00166116,\n",
              "        0.00158463, 0.00174723, 0.00160427, 0.00161719, 0.00190153,\n",
              "        0.00162096, 0.0017684 , 0.00217352, 0.00171485, 0.00155644,\n",
              "        0.00164189, 0.0015677 , 0.00163908, 0.00215611, 0.0015728 ,\n",
              "        0.00156636, 0.00190468, 0.00170708, 0.0015625 , 0.00180001,\n",
              "        0.00157995, 0.00154653, 0.00227275, 0.00166211, 0.00156736,\n",
              "        0.00194173, 0.0017612 , 0.0015862 , 0.00191727, 0.00159516,\n",
              "        0.00158935, 0.00221643, 0.00158067, 0.00157957, 0.00200853,\n",
              "        0.00220556, 0.00173764, 0.00210299, 0.00155387, 0.00158958,\n",
              "        0.00215292, 0.00162115, 0.0015717 , 0.00190616, 0.00173039,\n",
              "        0.00180554, 0.00182881, 0.00162516, 0.00153594, 0.00163732,\n",
              "        0.00166922, 0.00161948, 0.00199766, 0.00158463, 0.00154934,\n",
              "        0.00203586, 0.00165391, 0.0016408 , 0.0018208 , 0.00163898,\n",
              "        0.00166125, 0.00200443, 0.00169067, 0.00175266, 0.00174966,\n",
              "        0.0015892 , 0.00156446, 0.00189486, 0.00163302, 0.00165005,\n",
              "        0.00175281, 0.00158167, 0.00163894, 0.00186687, 0.00178509,\n",
              "        0.00164733, 0.00223875, 0.00163279, 0.00156894, 0.00187097,\n",
              "        0.00164666, 0.00173831, 0.0019145 , 0.00157833, 0.00157094,\n",
              "        0.00202923, 0.00158887, 0.00161314, 0.00178499, 0.0017725 ,\n",
              "        0.00160446, 0.00183353, 0.00159554, 0.00162697, 0.00227818,\n",
              "        0.00162568, 0.00156345, 0.00198712, 0.0015532 , 0.00152984,\n",
              "        0.0016788 , 0.00187755, 0.00157385, 0.00211744, 0.00165358,\n",
              "        0.00155759, 0.00193043, 0.0015523 , 0.0016211 , 0.00197959,\n",
              "        0.00155568, 0.00170374, 0.00209441, 0.00168376, 0.00157242,\n",
              "        0.00202703, 0.00167117, 0.00154972, 0.00182214, 0.00157075,\n",
              "        0.00155272, 0.00198855, 0.00157843, 0.00163064, 0.0017972 ,\n",
              "        0.00167933, 0.00162067, 0.00180669, 0.00161481, 0.00158496,\n",
              "        0.00220842, 0.00162454, 0.00167489, 0.00205064, 0.0016757 ,\n",
              "        0.00156841, 0.00180111, 0.0017416 , 0.00158892, 0.00210943,\n",
              "        0.00156722, 0.0016418 , 0.00176873, 0.00173769, 0.00183749,\n",
              "        0.00189786, 0.00178132, 0.00213928, 0.00171452, 0.00159006,\n",
              "        0.00172448, 0.0018259 , 0.0016284 , 0.00160737, 0.00164332,\n",
              "        0.00166984, 0.00159507, 0.00224295, 0.00170484, 0.00156603,\n",
              "        0.00226912, 0.00177822, 0.0015594 , 0.0019309 , 0.00162106,\n",
              "        0.00158005, 0.00205145, 0.00163093, 0.00155883, 0.00175295,\n",
              "        0.00163417, 0.00159469, 0.00192428, 0.00157871, 0.0015727 ,\n",
              "        0.00228009, 0.00166345, 0.00154376, 0.00186381, 0.0015563 ,\n",
              "        0.00161734]),\n",
              " 'std_score_time': array([2.00486091e-04, 3.23593535e-04, 1.09899766e-03, 7.68526266e-04,\n",
              "        2.38288720e-04, 7.19864619e-05, 3.81655451e-04, 2.85900665e-04,\n",
              "        4.15370328e-05, 3.35181127e-04, 7.01495066e-04, 5.66993268e-05,\n",
              "        3.27295183e-04, 1.01400450e-04, 1.95439026e-04, 1.72997835e-04,\n",
              "        5.63186067e-05, 5.50055966e-04, 4.76248511e-04, 1.17830408e-04,\n",
              "        1.61090286e-04, 2.93464741e-04, 2.82436570e-04, 2.52649305e-04,\n",
              "        7.03742362e-04, 1.90524498e-04, 4.98009134e-04, 4.94502158e-04,\n",
              "        2.45622870e-04, 3.65537674e-04, 3.26860099e-04, 6.14846801e-04,\n",
              "        2.52442672e-04, 1.06874912e-03, 1.51334728e-04, 5.82525241e-05,\n",
              "        4.96950673e-04, 2.30207182e-04, 1.66144532e-04, 4.38969477e-04,\n",
              "        3.56859539e-04, 3.56485558e-04, 2.31328277e-04, 1.97863899e-04,\n",
              "        1.33167429e-04, 1.90056070e-04, 1.94497698e-04, 7.05202339e-05,\n",
              "        2.27667278e-04, 1.89185162e-03, 1.36251043e-04, 2.64885492e-04,\n",
              "        8.70743560e-05, 3.11139129e-04, 3.77525521e-04, 1.26784769e-04,\n",
              "        1.65907224e-04, 4.20757055e-04, 1.71264853e-04, 2.26804696e-04,\n",
              "        6.13306129e-04, 8.72689280e-04, 2.76336518e-04, 2.67462820e-04,\n",
              "        1.93632973e-04, 1.13648634e-03, 3.33173449e-04, 2.14134736e-04,\n",
              "        2.21404453e-04, 1.95228139e-04, 1.13945365e-04, 1.60158266e-04,\n",
              "        3.92681755e-04, 4.78927266e-04, 2.50948184e-04, 4.06383706e-04,\n",
              "        6.36199775e-05, 5.72382581e-05, 7.53436314e-04, 3.49508190e-04,\n",
              "        8.43652036e-04, 1.79089055e-04, 8.43448937e-04, 1.06737983e-03,\n",
              "        4.72996494e-04, 1.20632884e-04, 2.52518544e-04, 3.81451708e-04,\n",
              "        4.59362754e-05, 4.54502274e-04, 2.33049204e-04, 1.21216245e-04,\n",
              "        1.88419776e-04, 2.45217281e-04, 2.46953288e-04, 2.34640059e-04,\n",
              "        6.58585864e-04, 1.60263908e-03, 3.47273421e-04, 2.07245651e-04,\n",
              "        3.39329343e-04, 2.11368816e-04, 3.52407740e-04, 3.53813814e-05,\n",
              "        1.73059873e-04, 7.65730476e-04, 3.58140197e-04, 7.29495401e-05,\n",
              "        2.28113763e-04, 7.17339633e-04, 1.53631382e-04, 1.16147333e-04,\n",
              "        1.03949891e-03, 1.85512367e-04, 4.99525717e-04, 3.93375119e-04,\n",
              "        2.10093283e-04, 2.16603530e-04, 1.54074240e-04, 1.40557618e-04,\n",
              "        3.85110174e-04, 1.62069816e-04, 5.74557157e-04, 1.77201657e-04,\n",
              "        3.58305512e-04, 3.85635990e-05, 2.28181652e-04, 1.48766270e-04,\n",
              "        1.90266953e-04, 2.71567641e-04, 4.28233220e-05, 1.63361543e-04,\n",
              "        2.96572166e-04, 1.56360670e-04, 4.23649445e-04, 2.52352433e-04,\n",
              "        1.10310544e-04, 1.51246598e-04, 2.71490309e-04, 7.32833094e-05,\n",
              "        1.06696190e-04, 2.18574861e-04, 3.45511217e-05, 8.16580536e-05,\n",
              "        3.46064330e-04, 1.13861525e-04, 8.48041983e-05, 2.17710708e-04,\n",
              "        1.85044215e-04, 1.62041769e-04, 5.56218639e-05, 1.25179826e-04,\n",
              "        4.34230862e-05, 4.50174670e-04, 5.68576277e-05, 5.01736025e-05,\n",
              "        3.05763119e-04, 5.63452465e-05, 8.74165419e-05, 4.38023647e-04,\n",
              "        7.50243226e-05, 5.31829289e-05, 3.64000990e-04, 5.42625182e-05,\n",
              "        5.41063322e-05, 3.24340611e-04, 1.61474580e-04, 1.80995690e-04,\n",
              "        5.13884368e-04, 1.26173977e-04, 4.64338531e-05, 3.26878338e-04,\n",
              "        7.59305345e-05, 3.82953975e-05, 3.75183476e-04, 3.82899347e-05,\n",
              "        8.29345804e-05, 3.59053681e-04, 1.75065098e-04, 1.03955575e-04,\n",
              "        1.82823023e-04, 5.91761253e-05, 3.44244151e-04, 3.30646175e-04,\n",
              "        3.90323764e-05, 5.31981041e-05, 2.48471191e-04, 1.29357792e-04,\n",
              "        3.02897082e-05, 3.89484508e-04, 1.13174022e-04, 7.77407787e-05,\n",
              "        3.05710473e-04, 1.83092951e-04, 4.77548545e-05, 3.89535381e-04,\n",
              "        1.43060604e-04, 3.70243097e-05, 4.27051467e-04, 5.67370098e-05,\n",
              "        9.94836598e-05, 3.28425919e-04, 1.16481138e-04, 1.57398276e-04,\n",
              "        1.25731986e-04, 5.31262936e-05, 1.09178571e-04, 1.75739860e-04,\n",
              "        5.43130710e-05, 1.35068786e-04, 2.92696864e-04, 8.97040834e-05,\n",
              "        1.47401696e-04, 2.87128539e-04, 4.90590128e-05, 8.09878681e-05,\n",
              "        2.29956043e-04, 1.04405235e-04, 2.55208129e-05, 3.45584440e-04,\n",
              "        3.67673917e-05, 1.40909487e-04, 2.86225992e-04, 1.33772617e-04,\n",
              "        4.86925224e-05, 2.82833363e-04, 7.12458545e-05, 1.09834862e-04,\n",
              "        3.04171527e-04, 1.99631394e-05, 3.32155332e-05, 3.03745157e-04,\n",
              "        1.19148964e-04, 1.61094873e-04, 2.59523791e-04, 1.99013116e-05,\n",
              "        7.37206902e-05, 2.98458697e-04, 1.38342811e-04, 4.06125318e-05,\n",
              "        2.00207075e-04, 7.07185999e-05, 5.84193829e-05, 2.66058897e-05,\n",
              "        1.61814084e-04, 1.85862390e-04, 2.52349252e-04, 3.18295143e-05,\n",
              "        5.00796168e-05, 2.11896154e-04, 1.48704403e-04, 1.37430111e-04,\n",
              "        2.92853414e-04, 1.26203492e-04, 1.25045088e-04, 3.56234838e-04,\n",
              "        5.07312160e-05, 4.48738983e-05, 2.79659875e-04, 1.33170263e-04,\n",
              "        3.14789820e-05, 2.66234870e-04, 2.81412308e-05, 7.13730776e-05,\n",
              "        2.70214702e-04, 3.95367884e-05, 1.93976043e-05, 3.27565832e-04,\n",
              "        1.00930399e-04, 1.54890626e-05, 1.56463387e-04, 4.36717761e-05,\n",
              "        1.28008478e-04, 4.18586784e-04, 1.88370655e-04, 4.09009557e-05,\n",
              "        2.55791141e-04, 8.84666458e-05, 1.76580468e-05, 4.80959558e-04,\n",
              "        2.68160601e-05, 2.69229371e-05, 6.90000668e-05, 3.36974447e-05,\n",
              "        7.59614315e-05, 3.02151484e-04, 5.36808101e-05, 2.04459211e-04,\n",
              "        3.56718637e-04, 1.09492174e-04, 8.97180739e-05, 3.02917297e-04,\n",
              "        7.07055772e-05, 6.36505987e-05, 4.00977030e-04, 7.44573855e-05,\n",
              "        4.81124676e-05, 2.93849211e-04, 5.92396816e-05, 1.52430110e-04,\n",
              "        1.67455271e-04, 5.90278565e-05, 8.18740648e-05, 2.97772088e-04,\n",
              "        1.07787518e-04, 3.37749518e-05, 1.98039181e-04, 7.34730710e-05,\n",
              "        7.83925219e-05, 2.69470403e-04, 6.74887829e-05, 2.57390868e-04,\n",
              "        4.97778315e-04, 1.38473264e-04, 2.43286972e-05, 1.77117284e-04,\n",
              "        4.93959569e-05, 1.30978184e-04, 3.57480038e-04, 8.46362987e-05,\n",
              "        7.45172149e-05, 3.28561128e-04, 2.26444790e-04, 4.76392539e-05,\n",
              "        4.39132628e-04, 6.65985334e-05, 2.16578576e-05, 4.86625345e-05,\n",
              "        1.29099343e-04, 4.34845685e-05, 3.86638887e-04, 1.69270139e-04,\n",
              "        2.34805385e-05, 2.31108297e-04, 5.72193861e-05, 2.27143721e-05,\n",
              "        2.02040276e-04, 3.35401305e-05, 6.40655877e-05, 3.94000898e-04,\n",
              "        8.36939418e-04, 1.35675750e-04, 2.64641684e-04, 2.78672872e-05,\n",
              "        8.06446253e-05, 2.75458265e-04, 8.65297986e-05, 3.49783377e-05,\n",
              "        3.40469912e-04, 1.21432591e-04, 2.46632007e-04, 3.37644637e-04,\n",
              "        4.90209935e-05, 1.90749168e-05, 9.62960333e-05, 8.65025714e-05,\n",
              "        3.73855537e-05, 2.80533857e-04, 3.58436306e-05, 2.76559797e-05,\n",
              "        3.67632698e-04, 8.24813890e-05, 6.10999419e-05, 2.62974072e-04,\n",
              "        1.44554394e-04, 1.53330723e-04, 2.78746882e-04, 1.48842823e-04,\n",
              "        2.33038540e-04, 2.53715396e-04, 7.29144046e-05, 5.27491394e-05,\n",
              "        4.30066557e-04, 8.35593142e-05, 1.66422071e-04, 1.90650886e-04,\n",
              "        7.46745868e-05, 8.90932173e-05, 4.56381599e-04, 4.26256132e-04,\n",
              "        1.96919175e-04, 9.69747751e-05, 8.30553424e-05, 4.12761989e-05,\n",
              "        3.16578887e-04, 1.10550415e-04, 1.91678989e-04, 3.37448699e-04,\n",
              "        3.70515052e-05, 4.62519754e-05, 3.43857766e-04, 5.60520812e-05,\n",
              "        9.48645925e-05, 2.36307785e-04, 1.58802639e-04, 6.25236257e-05,\n",
              "        2.71204612e-04, 5.50323381e-05, 7.57605564e-05, 2.34290462e-04,\n",
              "        1.20144759e-04, 5.80276929e-05, 2.44332802e-04, 2.12171639e-05,\n",
              "        2.21427076e-05, 2.33850325e-04, 3.49573936e-04, 3.34940005e-05,\n",
              "        3.67440869e-04, 1.79803564e-04, 4.10971670e-05, 2.42088276e-04,\n",
              "        1.65781681e-05, 1.36484889e-04, 4.72701761e-04, 3.72586532e-05,\n",
              "        1.57232597e-04, 8.24352846e-05, 9.69570009e-05, 6.62795760e-05,\n",
              "        4.20670811e-04, 1.33111089e-04, 3.49313731e-05, 3.03926504e-04,\n",
              "        8.23386632e-05, 2.44216050e-05, 2.75202120e-04, 6.10762324e-05,\n",
              "        6.15222686e-05, 2.03881484e-04, 1.63432317e-04, 1.07030198e-04,\n",
              "        3.26723079e-04, 6.63926183e-05, 5.01716538e-05, 1.03412491e-04,\n",
              "        7.19694669e-05, 1.64805573e-04, 2.52308640e-04, 1.45241781e-04,\n",
              "        2.33375622e-05, 2.66545002e-04, 1.63741700e-04, 1.21556314e-04,\n",
              "        8.05579088e-05, 4.77441405e-05, 1.21161441e-04, 2.91840570e-04,\n",
              "        8.22643744e-05, 3.35102415e-04, 2.67348413e-04, 1.35796375e-04,\n",
              "        4.94492718e-04, 2.32427346e-04, 6.70339434e-05, 2.59486177e-04,\n",
              "        3.49778976e-04, 1.52903395e-04, 1.35626051e-04, 1.28013292e-04,\n",
              "        1.10293414e-04, 1.04227964e-04, 2.79609503e-04, 2.45737214e-04,\n",
              "        6.37379152e-05, 1.26874406e-04, 3.35229701e-04, 8.26201531e-05,\n",
              "        1.06449813e-04, 6.25626706e-05, 4.01083264e-05, 4.85898997e-04,\n",
              "        4.47890988e-05, 5.21947008e-05, 2.41652237e-04, 1.26644341e-04,\n",
              "        1.14896048e-04, 2.63741128e-04, 3.27912146e-05, 5.08241309e-05,\n",
              "        7.31544354e-05, 1.27099332e-04, 2.49112620e-05, 3.02274653e-04,\n",
              "        3.18963072e-05, 9.77712239e-05]),\n",
              " 'param_alpha': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08,\n",
              "                    1e-08, 1e-08, 1e-08, 1e-08, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09, 1e-09,\n",
              "                    1e-09, 1e-09],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_eta0': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001,\n",
              "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
              "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
              "                    0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05,\n",
              "                    1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06,\n",
              "                    1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07,\n",
              "                    1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_learning_rate': masked_array(data=['invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant', 'invscaling', 'invscaling',\n",
              "                    'invscaling', 'optimal', 'optimal', 'optimal',\n",
              "                    'constant', 'constant', 'constant', 'invscaling',\n",
              "                    'invscaling', 'invscaling', 'optimal', 'optimal',\n",
              "                    'optimal', 'constant', 'constant', 'constant',\n",
              "                    'invscaling', 'invscaling', 'invscaling', 'optimal',\n",
              "                    'optimal', 'optimal', 'constant', 'constant',\n",
              "                    'constant', 'invscaling', 'invscaling', 'invscaling',\n",
              "                    'optimal', 'optimal', 'optimal', 'constant',\n",
              "                    'constant', 'constant'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_penalty': masked_array(data=['l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None,\n",
              "                    'l1', 'l2', None, 'l1', 'l2', None, 'l1', 'l2', None],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'alpha': 0.1,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 0.0001, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 1e-06, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-06, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-06, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.1,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.1, 'eta0': 1e-07, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-07, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.1, 'eta0': 1e-07, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 0.0001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 0.0001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 0.0001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-05, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 1e-06, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-06, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-06, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.01,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.01, 'eta0': 1e-07, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-07, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.01, 'eta0': 1e-07, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.001, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.001, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.001, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.001, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.001, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.001, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.001, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.001, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.001, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 0.0001,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-05, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-05, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-05, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-05, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-05, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-05, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-05, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-05, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-05,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-06, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-06, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-06, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-06, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-06, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-06, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-06, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-06, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-06,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-07, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-07, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-07, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-07, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-07, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-07, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-07, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-07, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-07,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-08, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-08, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-08, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-08, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-08, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-08, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-08, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-08, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-08,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.01,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-09, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-09, 'eta0': 0.01, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-09, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-09, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-09, 'eta0': 0.01, 'learning_rate': 'constant', 'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-09, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-09, 'eta0': 0.001, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'optimal',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 0.0001,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-05, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-05,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-06, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-06,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'invscaling',\n",
              "   'penalty': None},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l1'},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': 'l2'},\n",
              "  {'alpha': 1e-09, 'eta0': 1e-07, 'learning_rate': 'optimal', 'penalty': None},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l1'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': 'l2'},\n",
              "  {'alpha': 1e-09,\n",
              "   'eta0': 1e-07,\n",
              "   'learning_rate': 'constant',\n",
              "   'penalty': None}],\n",
              " 'split0_test_score': array([-6.12316378e+05, -7.43701469e+05, -6.15496168e+05, -2.51656968e+21,\n",
              "        -1.35490274e+11, -2.37362532e+21, -7.02128850e+05, -1.13143847e+06,\n",
              "        -8.18505583e+05, -6.07718257e+05, -7.30374936e+05, -6.08204227e+05,\n",
              "        -1.02122625e+21, -8.25736115e+10, -1.97135396e+21, -6.42321254e+05,\n",
              "        -7.45574552e+05, -6.48502219e+05, -6.28101160e+05, -7.32487776e+05,\n",
              "        -6.28257530e+05, -1.02263509e+21, -5.72976920e+10, -2.59750519e+21,\n",
              "        -6.09585465e+05, -7.32738672e+05, -6.08226809e+05, -7.18997497e+06,\n",
              "        -7.22717421e+06, -7.19072532e+06, -1.98285049e+21, -9.69348591e+10,\n",
              "        -2.52210476e+21, -6.07333752e+05, -7.29873311e+05, -6.07433994e+05,\n",
              "        -2.68084824e+08, -2.68106603e+08, -2.68083070e+08, -1.79130638e+21,\n",
              "        -9.07698813e+10, -2.33369310e+21, -6.87670695e+05, -7.65722063e+05,\n",
              "        -6.87661922e+05, -3.92403945e+08, -3.92405362e+08, -3.92403164e+08,\n",
              "        -1.85801570e+21, -6.65171161e+10, -2.01517270e+21, -7.63010163e+07,\n",
              "        -7.63278081e+07, -7.63012244e+07, -6.16155015e+05, -6.25672375e+05,\n",
              "        -6.12856443e+05, -7.35004036e+13, -7.79726461e+05, -7.76907482e+13,\n",
              "        -6.78869170e+05, -1.13001686e+06, -3.10664516e+06, -6.07547237e+05,\n",
              "        -6.13743169e+05, -6.08173258e+05, -3.37950105e+13, -7.28583145e+05,\n",
              "        -1.01508325e+14, -6.15613864e+05, -6.47881070e+05, -6.38951871e+05,\n",
              "        -6.28150781e+05, -6.34773937e+05, -6.28156310e+05, -2.47666931e+13,\n",
              "        -8.23046657e+05, -5.50861297e+13, -6.09534791e+05, -6.15235571e+05,\n",
              "        -6.10286726e+05, -7.19079326e+06, -7.19385018e+06, -7.19065455e+06,\n",
              "        -4.10074529e+14, -6.57575921e+05, -6.90917777e+13, -6.08263415e+05,\n",
              "        -6.13290671e+05, -6.07714152e+05, -2.68084083e+08, -2.68085709e+08,\n",
              "        -2.68082808e+08, -4.57710202e+13, -8.53690113e+05, -6.23573441e+13,\n",
              "        -6.87669364e+05, -6.93955933e+05, -6.87668407e+05, -3.92403945e+08,\n",
              "        -3.92403512e+08, -3.92404076e+08, -8.76436365e+13, -7.94568387e+05,\n",
              "        -9.36231947e+13, -7.63012395e+07, -7.63037501e+07, -7.63012264e+07,\n",
              "        -6.16788339e+05, -6.17665802e+05, -6.19796148e+05, -6.10162901e+05,\n",
              "        -6.19196534e+05, -6.12556376e+05, -1.03286699e+06, -1.00271555e+06,\n",
              "        -1.25688581e+06, -6.07340914e+05, -6.07073490e+05, -6.06823444e+05,\n",
              "        -6.19583269e+05, -6.15709117e+05, -6.08625685e+05, -6.39460133e+05,\n",
              "        -6.24260871e+05, -6.39530675e+05, -6.28201795e+05, -6.28820505e+05,\n",
              "        -6.28319620e+05, -6.18454838e+05, -6.17975423e+05, -6.21159638e+05,\n",
              "        -6.08502243e+05, -6.08987220e+05, -6.09711844e+05, -7.19093674e+06,\n",
              "        -7.19080393e+06, -7.19052239e+06, -6.13678652e+05, -6.13030658e+05,\n",
              "        -6.13438057e+05, -6.07701163e+05, -6.08014869e+05, -6.07934882e+05,\n",
              "        -2.68083320e+08, -2.68082010e+08, -2.68082854e+08, -6.20203189e+05,\n",
              "        -6.10397762e+05, -6.10515791e+05, -6.87665802e+05, -6.88267503e+05,\n",
              "        -6.87665009e+05, -3.92403372e+08, -3.92403524e+08, -3.92404213e+08,\n",
              "        -6.11488385e+05, -6.14540570e+05, -6.18883131e+05, -7.63012476e+07,\n",
              "        -7.63014671e+07, -7.63012479e+07, -6.10244575e+05, -6.11578673e+05,\n",
              "        -6.14438340e+05, -6.85638685e+05, -6.88896434e+05, -6.30154758e+05,\n",
              "        -1.24206354e+06, -1.82920485e+06, -9.02009101e+05, -6.08209573e+05,\n",
              "        -6.07034175e+05, -6.07909387e+05, -6.93161593e+05, -6.69154774e+05,\n",
              "        -6.98137184e+05, -6.50551528e+05, -6.38809558e+05, -6.30846169e+05,\n",
              "        -6.28198810e+05, -6.28254470e+05, -6.28223862e+05, -7.05965420e+05,\n",
              "        -6.77846592e+05, -6.63565737e+05, -6.09225319e+05, -6.07573035e+05,\n",
              "        -6.09562656e+05, -7.19065425e+06, -7.19072003e+06, -7.19077165e+06,\n",
              "        -7.87056424e+05, -7.36886843e+05, -6.87947657e+05, -6.07261013e+05,\n",
              "        -6.07255442e+05, -6.07290384e+05, -2.68083080e+08, -2.68082630e+08,\n",
              "        -2.68083091e+08, -6.76029492e+05, -6.87706484e+05, -6.99505202e+05,\n",
              "        -6.87668823e+05, -6.87719452e+05, -6.87673856e+05, -3.92403443e+08,\n",
              "        -3.92403626e+08, -3.92403353e+08, -6.53219907e+05, -6.50756830e+05,\n",
              "        -6.63159754e+05, -7.63011781e+07, -7.63012896e+07, -7.63012288e+07,\n",
              "        -6.16871652e+05, -6.29362344e+05, -6.09995440e+05, -3.35613822e+06,\n",
              "        -7.03305671e+06, -2.43651545e+06, -1.19390999e+06, -1.35236304e+06,\n",
              "        -1.56826550e+06, -6.07319175e+05, -6.07356553e+05, -6.08425089e+05,\n",
              "        -1.54677308e+06, -3.95630999e+06, -2.50395226e+06, -6.29747060e+05,\n",
              "        -6.47732422e+05, -6.42113053e+05, -6.28099914e+05, -6.28168937e+05,\n",
              "        -6.28175544e+05, -2.54946964e+06, -8.50501511e+06, -1.67778704e+06,\n",
              "        -6.07372487e+05, -6.07688363e+05, -6.14298345e+05, -7.19062170e+06,\n",
              "        -7.19082485e+06, -7.19072820e+06, -3.58767329e+06, -1.89715301e+06,\n",
              "        -2.07246753e+06, -6.08308558e+05, -6.07346907e+05, -6.07359222e+05,\n",
              "        -2.68082386e+08, -2.68082514e+08, -2.68083446e+08, -1.33189949e+06,\n",
              "        -1.96092015e+06, -2.14507595e+06, -6.87657073e+05, -6.87675999e+05,\n",
              "        -6.87674324e+05, -3.92403638e+08, -3.92403439e+08, -3.92403450e+08,\n",
              "        -1.59112964e+06, -2.87110457e+06, -1.55474392e+06, -7.63012456e+07,\n",
              "        -7.63012297e+07, -7.63012063e+07, -6.17393509e+05, -6.08751341e+05,\n",
              "        -6.17709083e+05, -6.45685496e+25, -1.55095620e+25, -4.01585353e+25,\n",
              "        -1.18540327e+06, -1.50376340e+06, -1.93796737e+06, -6.07183585e+05,\n",
              "        -6.07317725e+05, -6.07298580e+05, -2.17324534e+25, -6.30682325e+25,\n",
              "        -1.20016805e+26, -6.33860469e+05, -6.69729780e+05, -6.24563785e+05,\n",
              "        -6.28130118e+05, -6.28288342e+05, -6.28145568e+05, -4.22967933e+25,\n",
              "        -3.60571418e+25, -4.06043495e+25, -6.09915057e+05, -6.08355315e+05,\n",
              "        -6.08542725e+05, -7.19079224e+06, -7.19043293e+06, -7.19083544e+06,\n",
              "        -4.97218676e+25, -2.72715350e+25, -6.26880011e+25, -6.07478841e+05,\n",
              "        -6.07223681e+05, -6.07933859e+05, -2.68082530e+08, -2.68083380e+08,\n",
              "        -2.68083131e+08, -2.73742542e+25, -5.51680267e+25, -4.16115440e+25,\n",
              "        -6.87661714e+05, -6.87667922e+05, -6.87670647e+05, -3.92403448e+08,\n",
              "        -3.92403200e+08, -3.92403915e+08, -5.46241088e+25, -4.94996281e+25,\n",
              "        -3.64074190e+25, -7.63011880e+07, -7.63012593e+07, -7.63012273e+07,\n",
              "        -6.11294178e+05, -6.14322439e+05, -6.09261408e+05, -2.53919025e+27,\n",
              "        -4.54007430e+27, -6.92368403e+27, -1.71983363e+06, -8.63879144e+05,\n",
              "        -1.36977338e+06, -6.08716991e+05, -6.07474477e+05, -6.07381190e+05,\n",
              "        -5.04823256e+27, -6.23784738e+27, -3.43496804e+27, -6.42689858e+05,\n",
              "        -6.30308300e+05, -6.34970007e+05, -6.28366365e+05, -6.28292819e+05,\n",
              "        -6.28084677e+05, -4.13651765e+27, -1.66522958e+27, -5.38443982e+27,\n",
              "        -6.07673329e+05, -6.08871144e+05, -6.08498348e+05, -7.19031682e+06,\n",
              "        -7.19076773e+06, -7.19041615e+06, -6.61850718e+27, -6.60507810e+27,\n",
              "        -6.65143932e+27, -6.08448593e+05, -6.07416781e+05, -6.07450291e+05,\n",
              "        -2.68083705e+08, -2.68082723e+08, -2.68084249e+08, -7.37002877e+27,\n",
              "        -4.93847033e+27, -5.86333384e+27, -6.87671678e+05, -6.87671920e+05,\n",
              "        -6.87665914e+05, -3.92404194e+08, -3.92403784e+08, -3.92404180e+08,\n",
              "        -7.82996283e+27, -6.65196581e+27, -4.14266974e+27, -7.63012249e+07,\n",
              "        -7.63012039e+07, -7.63011928e+07, -6.24374243e+05, -6.19415642e+05,\n",
              "        -6.18576005e+05, -2.40600234e+29, -4.90023932e+29, -4.25999341e+29,\n",
              "        -1.40499796e+06, -1.09417807e+06, -1.26666946e+06, -6.07437137e+05,\n",
              "        -6.07182906e+05, -6.08310692e+05, -5.67891890e+29, -5.39918801e+29,\n",
              "        -5.65037726e+29, -6.43589985e+05, -6.39428143e+05, -6.29107774e+05,\n",
              "        -6.28183535e+05, -6.28247845e+05, -6.28156289e+05, -4.18739596e+29,\n",
              "        -6.50022894e+29, -2.87068809e+29, -6.08439874e+05, -6.09104357e+05,\n",
              "        -6.10138151e+05, -7.19082499e+06, -7.19074648e+06, -7.19092865e+06,\n",
              "        -5.43293296e+29, -4.75403140e+29, -2.61600968e+29, -6.07894399e+05,\n",
              "        -6.07524606e+05, -6.07372115e+05, -2.68081854e+08, -2.68082966e+08,\n",
              "        -2.68084379e+08, -6.02211433e+29, -6.20511188e+29, -5.28570963e+29,\n",
              "        -6.87686245e+05, -6.87664675e+05, -6.87666997e+05, -3.92403738e+08,\n",
              "        -3.92403821e+08, -3.92403695e+08, -3.64687303e+29, -3.68258947e+29,\n",
              "        -7.92923097e+29, -7.63012014e+07, -7.63012441e+07, -7.63012342e+07,\n",
              "        -6.11507584e+05, -6.10189045e+05, -6.22226495e+05, -2.13796577e+31,\n",
              "        -1.34137743e+31, -2.87483903e+31, -1.47314229e+06, -1.48812403e+06,\n",
              "        -1.49080624e+06, -6.06175026e+05, -6.08217910e+05, -6.07040784e+05,\n",
              "        -1.97266837e+31, -3.34492355e+31, -2.44569777e+31, -6.33952421e+05,\n",
              "        -6.16927708e+05, -6.35098172e+05, -6.28253771e+05, -6.28175197e+05,\n",
              "        -6.28183323e+05, -2.46537951e+31, -1.56467380e+31, -2.49513685e+31,\n",
              "        -6.07542705e+05, -6.08751125e+05, -6.07582247e+05, -7.19079205e+06,\n",
              "        -7.19049034e+06, -7.19062902e+06, -3.15626582e+31, -2.40858821e+31,\n",
              "        -1.92282975e+31, -6.07603940e+05, -6.07585480e+05, -6.07767845e+05,\n",
              "        -2.68081899e+08, -2.68081978e+08, -2.68085588e+08, -2.01080538e+31,\n",
              "        -1.64444988e+31, -2.02563220e+31, -6.87658869e+05, -6.87666684e+05,\n",
              "        -6.87664064e+05, -3.92403355e+08, -3.92402872e+08, -3.92403267e+08,\n",
              "        -1.57604347e+31, -1.70282266e+31, -2.34198409e+31, -7.63011969e+07,\n",
              "        -7.63012284e+07, -7.63012199e+07]),\n",
              " 'split1_test_score': array([-6.45243510e+05, -7.78387591e+05, -6.47145923e+05, -1.75042114e+21,\n",
              "        -9.06472840e+10, -1.01830110e+21, -8.85301382e+05, -1.58651155e+06,\n",
              "        -1.21213857e+06, -6.42785641e+05, -7.70467239e+05, -6.43226573e+05,\n",
              "        -1.98046595e+21, -6.59221564e+10, -1.90245040e+21, -6.74086377e+05,\n",
              "        -8.06859054e+05, -7.17963086e+05, -6.62426889e+05, -7.72180286e+05,\n",
              "        -6.62460159e+05, -1.96362400e+21, -1.82702117e+11, -1.55491569e+21,\n",
              "        -6.45442883e+05, -7.71765424e+05, -6.44642275e+05, -7.29434433e+06,\n",
              "        -7.32745148e+06, -7.29436495e+06, -1.80674390e+21, -1.55317233e+11,\n",
              "        -3.19552358e+21, -6.42880424e+05, -7.69260927e+05, -6.42756486e+05,\n",
              "        -2.68269179e+08, -2.68286033e+08, -2.68271199e+08, -2.24253913e+21,\n",
              "        -1.18188117e+11, -2.38065621e+21, -7.27476940e+05, -8.06265842e+05,\n",
              "        -7.27551824e+05, -3.91948922e+08, -3.91950042e+08, -3.91949228e+08,\n",
              "        -1.58466241e+21, -1.80724823e+11, -2.25966378e+21, -7.65920293e+07,\n",
              "        -7.66072945e+07, -7.65923584e+07, -6.57454337e+05, -6.56014192e+05,\n",
              "        -6.48064018e+05, -3.11956579e+14, -1.11258910e+06, -1.02798611e+14,\n",
              "        -9.62080996e+05, -1.34888068e+06, -1.38592625e+06, -6.42903715e+05,\n",
              "        -6.50663937e+05, -6.43180887e+05, -2.98578534e+13, -8.22211883e+05,\n",
              "        -8.99019015e+13, -6.65699709e+05, -6.90301860e+05, -6.70288236e+05,\n",
              "        -6.62352815e+05, -6.69927598e+05, -6.62314493e+05, -5.32943016e+13,\n",
              "        -7.07020564e+05, -9.45075345e+13, -6.46362562e+05, -6.49438142e+05,\n",
              "        -6.47816343e+05, -7.29482255e+06, -7.29757300e+06, -7.29454003e+06,\n",
              "        -3.72501193e+13, -8.42738532e+05, -7.73984201e+13, -6.43433954e+05,\n",
              "        -6.49229256e+05, -6.42873747e+05, -2.68270516e+08, -2.68271053e+08,\n",
              "        -2.68269399e+08, -8.02047871e+13, -7.64174403e+05, -3.07263752e+14,\n",
              "        -7.27542810e+05, -7.33988218e+05, -7.27535947e+05, -3.91948480e+08,\n",
              "        -3.91948907e+08, -3.91949156e+08, -3.22009245e+14, -7.24837891e+05,\n",
              "        -5.37425947e+13, -7.65922632e+07, -7.65936425e+07, -7.65923111e+07,\n",
              "        -6.46797529e+05, -6.48024276e+05, -6.53336610e+05, -6.64912093e+05,\n",
              "        -6.48958781e+05, -6.45294775e+05, -2.06239079e+06, -1.64326806e+06,\n",
              "        -1.20089537e+06, -6.43376039e+05, -6.43665595e+05, -6.43156069e+05,\n",
              "        -6.57300936e+05, -6.62513871e+05, -6.67312776e+05, -6.80912304e+05,\n",
              "        -6.78503352e+05, -6.70526721e+05, -6.62246678e+05, -6.63023648e+05,\n",
              "        -6.62177472e+05, -6.47669895e+05, -6.50246318e+05, -6.45868998e+05,\n",
              "        -6.45299906e+05, -6.45312600e+05, -6.46454311e+05, -7.29458741e+06,\n",
              "        -7.29501709e+06, -7.29495174e+06, -6.50149584e+05, -6.56288531e+05,\n",
              "        -6.52332948e+05, -6.43097919e+05, -6.43218514e+05, -6.43191467e+05,\n",
              "        -2.68271481e+08, -2.68270379e+08, -2.68269178e+08, -6.53974472e+05,\n",
              "        -6.47660490e+05, -6.56065718e+05, -7.27553369e+05, -7.28155732e+05,\n",
              "        -7.27545935e+05, -3.91948777e+08, -3.91949372e+08, -3.91949076e+08,\n",
              "        -6.66708041e+05, -6.49382462e+05, -6.49098952e+05, -7.65923643e+07,\n",
              "        -7.65924625e+07, -7.65923302e+07, -6.46676493e+05, -6.53161489e+05,\n",
              "        -6.52656612e+05, -7.07646918e+05, -7.41629998e+05, -6.92644502e+05,\n",
              "        -1.53762085e+06, -1.42804008e+06, -1.08067241e+06, -6.42979813e+05,\n",
              "        -6.43107076e+05, -6.43715654e+05, -6.67397344e+05, -7.25816005e+05,\n",
              "        -7.17810178e+05, -6.73715574e+05, -6.62263240e+05, -6.72570221e+05,\n",
              "        -6.62240828e+05, -6.62421862e+05, -6.62229271e+05, -7.37850141e+05,\n",
              "        -6.71023764e+05, -7.12454000e+05, -6.45054414e+05, -6.44005511e+05,\n",
              "        -6.44597692e+05, -7.29466370e+06, -7.29474243e+06, -7.29490663e+06,\n",
              "        -7.39482732e+05, -6.83649418e+05, -7.35036561e+05, -6.43172349e+05,\n",
              "        -6.43150925e+05, -6.43206785e+05, -2.68270307e+08, -2.68270552e+08,\n",
              "        -2.68270512e+08, -7.01283794e+05, -7.58660056e+05, -7.38310847e+05,\n",
              "        -7.27543785e+05, -7.27604435e+05, -7.27543925e+05, -3.91949350e+08,\n",
              "        -3.91948985e+08, -3.91949007e+08, -6.80820060e+05, -7.14614570e+05,\n",
              "        -7.13485974e+05, -7.65923161e+07, -7.65923614e+07, -7.65922692e+07,\n",
              "        -6.51968944e+05, -6.55203630e+05, -6.47868064e+05, -2.08972544e+06,\n",
              "        -2.03892043e+06, -4.09664290e+06, -1.55226918e+06, -1.86908046e+06,\n",
              "        -1.12028711e+06, -6.43950914e+05, -6.43820778e+05, -6.42613664e+05,\n",
              "        -1.84679717e+06, -1.65028590e+06, -5.15621420e+06, -6.72008549e+05,\n",
              "        -6.71150664e+05, -6.62418450e+05, -6.62448623e+05, -6.62238503e+05,\n",
              "        -6.62397042e+05, -1.37310561e+06, -3.46069234e+06, -1.30757817e+06,\n",
              "        -6.45309089e+05, -6.46267928e+05, -6.45668511e+05, -7.29453697e+06,\n",
              "        -7.29452262e+06, -7.29447132e+06, -2.84061916e+06, -1.87462081e+06,\n",
              "        -2.47611653e+06, -6.42882586e+05, -6.42884378e+05, -6.43561103e+05,\n",
              "        -2.68269724e+08, -2.68269129e+08, -2.68270820e+08, -1.58381084e+06,\n",
              "        -1.14278876e+06, -2.30147020e+06, -7.27566552e+05, -7.27568180e+05,\n",
              "        -7.27555327e+05, -3.91949362e+08, -3.91948700e+08, -3.91949107e+08,\n",
              "        -1.39717596e+06, -1.30760700e+06, -2.03048147e+06, -7.65922836e+07,\n",
              "        -7.65922650e+07, -7.65922904e+07, -6.52011795e+05, -6.53051405e+05,\n",
              "        -6.50600353e+05, -3.38559951e+25, -3.99343409e+25, -3.87575397e+25,\n",
              "        -9.43219827e+05, -2.45368671e+06, -1.58838633e+06, -6.43252510e+05,\n",
              "        -6.43827801e+05, -6.42571094e+05, -2.18941117e+25, -4.28512329e+25,\n",
              "        -4.02733269e+25, -6.56180557e+05, -6.88069826e+05, -6.52312968e+05,\n",
              "        -6.62269703e+05, -6.62221897e+05, -6.62310765e+05, -2.65302602e+25,\n",
              "        -6.77437859e+25, -2.62134196e+25, -6.44678236e+05, -6.44540731e+05,\n",
              "        -6.48564049e+05, -7.29484162e+06, -7.29449874e+06, -7.29449044e+06,\n",
              "        -8.06176549e+25, -4.29652124e+25, -4.11496550e+25, -6.42679968e+05,\n",
              "        -6.43089527e+05, -6.43254210e+05, -2.68269744e+08, -2.68268993e+08,\n",
              "        -2.68270488e+08, -4.64818062e+25, -3.04227034e+25, -5.74218945e+25,\n",
              "        -7.27550597e+05, -7.27546867e+05, -7.27555437e+05, -3.91948799e+08,\n",
              "        -3.91948731e+08, -3.91949507e+08, -5.36502284e+25, -4.28965851e+25,\n",
              "        -4.91773828e+25, -7.65923080e+07, -7.65922969e+07, -7.65923056e+07,\n",
              "        -6.55610102e+05, -6.50072155e+05, -6.50523251e+05, -2.32287911e+27,\n",
              "        -6.20313679e+27, -7.99931914e+27, -1.04932301e+06, -1.83348482e+06,\n",
              "        -1.11561174e+06, -6.43714884e+05, -6.43328878e+05, -6.43799961e+05,\n",
              "        -3.96424686e+27, -7.65255564e+27, -6.13225065e+27, -7.17077828e+05,\n",
              "        -6.77530858e+05, -7.28875708e+05, -6.62344190e+05, -6.62339529e+05,\n",
              "        -6.62291531e+05, -5.38529879e+27, -4.33357364e+27, -6.78485767e+27,\n",
              "        -6.44032222e+05, -6.44988297e+05, -6.44210717e+05, -7.29521528e+06,\n",
              "        -7.29441681e+06, -7.29452089e+06, -4.55614073e+27, -3.79558693e+27,\n",
              "        -4.65519924e+27, -6.43158575e+05, -6.43249800e+05, -6.43259963e+05,\n",
              "        -2.68271077e+08, -2.68270017e+08, -2.68270162e+08, -7.16567783e+27,\n",
              "        -2.39069816e+27, -5.32979301e+27, -7.27551406e+05, -7.27565002e+05,\n",
              "        -7.27554799e+05, -3.91948933e+08, -3.91949907e+08, -3.91949304e+08,\n",
              "        -7.12447760e+27, -8.71315086e+27, -3.64788524e+27, -7.65922650e+07,\n",
              "        -7.65922821e+07, -7.65923691e+07, -6.44634911e+05, -6.50492095e+05,\n",
              "        -6.49706447e+05, -3.41853117e+29, -5.20070423e+29, -4.21569698e+29,\n",
              "        -1.53662898e+06, -1.38633070e+06, -1.46258718e+06, -6.43319298e+05,\n",
              "        -6.42937556e+05, -6.42940425e+05, -2.32243065e+29, -4.76279219e+29,\n",
              "        -4.45091332e+29, -6.64721221e+05, -6.67865070e+05, -6.60503196e+05,\n",
              "        -6.62304284e+05, -6.62282193e+05, -6.62373359e+05, -2.06171183e+29,\n",
              "        -3.62559449e+29, -4.76797850e+29, -6.47836904e+05, -6.44303394e+05,\n",
              "        -6.44026828e+05, -7.29443468e+06, -7.29428576e+06, -7.29447569e+06,\n",
              "        -2.47779527e+29, -6.29206687e+29, -5.49702863e+29, -6.43003791e+05,\n",
              "        -6.43033884e+05, -6.42623366e+05, -2.68270771e+08, -2.68270866e+08,\n",
              "        -2.68268764e+08, -2.11893380e+29, -3.19294518e+29, -4.02649988e+29,\n",
              "        -7.27558175e+05, -7.27555599e+05, -7.27535261e+05, -3.91948845e+08,\n",
              "        -3.91949045e+08, -3.91949234e+08, -4.95104433e+29, -5.47412201e+29,\n",
              "        -9.64126969e+29, -7.65922562e+07, -7.65923034e+07, -7.65923554e+07,\n",
              "        -6.53055736e+05, -6.52093364e+05, -6.49085893e+05, -1.43215081e+31,\n",
              "        -1.90156506e+31, -9.10486289e+30, -1.19352911e+06, -1.47373075e+06,\n",
              "        -1.64406166e+06, -6.42877932e+05, -6.43279730e+05, -6.43767801e+05,\n",
              "        -3.14233885e+31, -1.78772143e+31, -1.20700030e+31, -6.58413458e+05,\n",
              "        -6.58404979e+05, -6.77402655e+05, -6.62358039e+05, -6.62272927e+05,\n",
              "        -6.62294976e+05, -2.16386422e+31, -2.05905110e+31, -1.69039759e+31,\n",
              "        -6.44117150e+05, -6.44872901e+05, -6.45766034e+05, -7.29488528e+06,\n",
              "        -7.29414082e+06, -7.29451237e+06, -2.12673119e+31, -1.70148235e+31,\n",
              "        -2.25785699e+31, -6.43135286e+05, -6.43073433e+05, -6.42921947e+05,\n",
              "        -2.68269072e+08, -2.68269623e+08, -2.68268938e+08, -3.02679803e+31,\n",
              "        -4.00588422e+31, -1.44929491e+31, -7.27552814e+05, -7.27546238e+05,\n",
              "        -7.27563116e+05, -3.91948538e+08, -3.91949000e+08, -3.91948642e+08,\n",
              "        -1.57804494e+31, -9.77884433e+30, -1.48857680e+31, -7.65922893e+07,\n",
              "        -7.65922989e+07, -7.65923614e+07]),\n",
              " 'split2_test_score': array([-6.44907007e+05, -7.89043028e+05, -6.47472059e+05, -1.58844248e+21,\n",
              "        -9.42071993e+10, -2.09332034e+21, -7.62767000e+05, -1.55640768e+06,\n",
              "        -1.13616814e+06, -6.40408537e+05, -7.61303530e+05, -6.40571790e+05,\n",
              "        -1.28827059e+21, -1.28882749e+11, -1.68672134e+21, -6.77230753e+05,\n",
              "        -7.76357834e+05, -6.85308238e+05, -6.56904860e+05, -7.65500898e+05,\n",
              "        -6.57199461e+05, -2.33926655e+21, -8.04913720e+10, -2.78193989e+21,\n",
              "        -6.41899086e+05, -7.63813194e+05, -6.43200912e+05, -7.31414797e+06,\n",
              "        -7.36100341e+06, -7.31387647e+06, -7.41444690e+20, -7.23368322e+10,\n",
              "        -2.03710605e+21, -6.40399333e+05, -7.63371218e+05, -6.40399505e+05,\n",
              "        -2.69127911e+08, -2.69149930e+08, -2.69127414e+08, -2.11915292e+21,\n",
              "        -1.27735071e+11, -1.35852392e+21, -7.14668231e+05, -7.97059371e+05,\n",
              "        -7.14744042e+05, -3.93528747e+08, -3.93529689e+08, -3.93529442e+08,\n",
              "        -9.66859617e+20, -7.60086419e+10, -2.41208866e+21, -7.67955220e+07,\n",
              "        -7.68278046e+07, -7.67956670e+07, -6.47328906e+05, -6.49174018e+05,\n",
              "        -6.46214044e+05, -5.25955302e+13, -7.09578218e+05, -2.96588120e+14,\n",
              "        -9.65943899e+05, -2.04605661e+06, -1.02796514e+06, -6.40318613e+05,\n",
              "        -6.45911200e+05, -6.40781170e+05, -2.93198172e+13, -9.68344451e+05,\n",
              "        -2.90733753e+13, -6.64237633e+05, -6.65474467e+05, -6.74080270e+05,\n",
              "        -6.57273869e+05, -6.64101686e+05, -6.57168636e+05, -4.07553693e+13,\n",
              "        -1.13321605e+06, -4.33983810e+13, -6.42184533e+05, -6.47274028e+05,\n",
              "        -6.41278468e+05, -7.31409840e+06, -7.31849588e+06, -7.31423067e+06,\n",
              "        -4.02817749e+13, -9.80154554e+05, -5.93273238e+13, -6.40406839e+05,\n",
              "        -6.46358930e+05, -6.40805839e+05, -2.69127048e+08, -2.69130891e+08,\n",
              "        -2.69127826e+08, -6.11709600e+13, -7.93948967e+05, -1.03265230e+14,\n",
              "        -7.14739381e+05, -7.21448687e+05, -7.14729000e+05, -3.93528960e+08,\n",
              "        -3.93528397e+08, -3.93528468e+08, -1.30619992e+14, -9.96378321e+05,\n",
              "        -5.29385782e+13, -7.67956639e+07, -7.67987834e+07, -7.67957021e+07,\n",
              "        -6.50685202e+05, -6.49020254e+05, -6.49288686e+05, -6.46338640e+05,\n",
              "        -6.56168135e+05, -6.55421579e+05, -1.15990151e+06, -1.28126990e+06,\n",
              "        -1.19079095e+06, -6.40506115e+05, -6.40468386e+05, -6.41444556e+05,\n",
              "        -6.55390099e+05, -6.53031164e+05, -6.49096444e+05, -6.58046484e+05,\n",
              "        -6.71111576e+05, -6.94341356e+05, -6.57169687e+05, -6.57713306e+05,\n",
              "        -6.57168791e+05, -6.51460938e+05, -6.52770086e+05, -6.52584342e+05,\n",
              "        -6.42435733e+05, -6.42073123e+05, -6.41286656e+05, -7.31409803e+06,\n",
              "        -7.31408133e+06, -7.31422643e+06, -6.56253195e+05, -6.52492002e+05,\n",
              "        -6.50534029e+05, -6.40606887e+05, -6.40617997e+05, -6.40964027e+05,\n",
              "        -2.69127470e+08, -2.69128482e+08, -2.69126978e+08, -6.49976813e+05,\n",
              "        -6.49399437e+05, -6.48801478e+05, -7.14741460e+05, -7.15392541e+05,\n",
              "        -7.14748521e+05, -3.93528423e+08, -3.93528760e+08, -3.93528155e+08,\n",
              "        -6.54440227e+05, -6.56495733e+05, -6.49033062e+05, -7.67956574e+07,\n",
              "        -7.67959883e+07, -7.67956947e+07, -6.47116065e+05, -6.52601729e+05,\n",
              "        -6.48260305e+05, -6.71020558e+05, -7.39775357e+05, -7.77376734e+05,\n",
              "        -1.22972102e+06, -1.20597307e+06, -1.60768298e+06, -6.41318158e+05,\n",
              "        -6.40393563e+05, -6.40794210e+05, -7.13682728e+05, -6.91821746e+05,\n",
              "        -7.11557489e+05, -6.82245562e+05, -6.73003773e+05, -7.12554793e+05,\n",
              "        -6.57118144e+05, -6.57204044e+05, -6.57380884e+05, -6.53626528e+05,\n",
              "        -8.01347084e+05, -6.94964316e+05, -6.45248676e+05, -6.43330196e+05,\n",
              "        -6.43452012e+05, -7.31376966e+06, -7.31370308e+06, -7.31413079e+06,\n",
              "        -6.83855572e+05, -7.21764303e+05, -7.06886851e+05, -6.40862015e+05,\n",
              "        -6.40612832e+05, -6.40403122e+05, -2.69129057e+08, -2.69127247e+08,\n",
              "        -2.69126622e+08, -8.06532905e+05, -6.82563434e+05, -7.17562909e+05,\n",
              "        -7.14732720e+05, -7.14803983e+05, -7.14738457e+05, -3.93528357e+08,\n",
              "        -3.93529280e+08, -3.93528074e+08, -7.13403702e+05, -7.40697723e+05,\n",
              "        -6.85691983e+05, -7.67957216e+07, -7.67957332e+07, -7.67956906e+07,\n",
              "        -6.50594381e+05, -6.50457339e+05, -6.43758516e+05, -1.36466031e+06,\n",
              "        -1.58506300e+06, -1.63744936e+06, -1.96626663e+06, -1.24097290e+06,\n",
              "        -1.38888910e+06, -6.41291340e+05, -6.40484951e+05, -6.40737555e+05,\n",
              "        -1.44021339e+06, -1.88299760e+06, -3.18428602e+06, -6.64121560e+05,\n",
              "        -6.71286258e+05, -6.74550086e+05, -6.57211862e+05, -6.57032964e+05,\n",
              "        -6.57091684e+05, -3.68737915e+06, -1.32624683e+06, -1.85900665e+06,\n",
              "        -6.42408338e+05, -6.44096321e+05, -6.42973987e+05, -7.31368697e+06,\n",
              "        -7.31431487e+06, -7.31405043e+06, -3.27952656e+06, -1.91982826e+06,\n",
              "        -2.85995518e+06, -6.40682903e+05, -6.40216363e+05, -6.40537256e+05,\n",
              "        -2.69126369e+08, -2.69125271e+08, -2.69128889e+08, -1.66415684e+06,\n",
              "        -1.34364791e+06, -3.98395198e+06, -7.14740379e+05, -7.14755765e+05,\n",
              "        -7.14746374e+05, -3.93528584e+08, -3.93529191e+08, -3.93528947e+08,\n",
              "        -3.75181543e+06, -4.33755515e+06, -2.76960620e+06, -7.67956500e+07,\n",
              "        -7.67956404e+07, -7.67956266e+07, -6.51258185e+05, -6.54442084e+05,\n",
              "        -6.44860724e+05, -4.29372836e+25, -3.24413578e+25, -4.18313305e+25,\n",
              "        -1.04534396e+06, -1.23280077e+06, -1.23793949e+06, -6.40780241e+05,\n",
              "        -6.41912319e+05, -6.40586133e+05, -3.35007189e+25, -1.03350015e+26,\n",
              "        -4.36447973e+25, -6.63686101e+05, -6.75503156e+05, -6.78244292e+05,\n",
              "        -6.57070403e+05, -6.57009552e+05, -6.57170656e+05, -4.57308323e+25,\n",
              "        -3.15234991e+25, -8.72912297e+25, -6.41374916e+05, -6.41883787e+05,\n",
              "        -6.42658375e+05, -7.31365774e+06, -7.31438563e+06, -7.31376488e+06,\n",
              "        -3.57587847e+25, -3.14898625e+25, -6.67321053e+25, -6.40794505e+05,\n",
              "        -6.40712169e+05, -6.40558205e+05, -2.69126540e+08, -2.69127145e+08,\n",
              "        -2.69126229e+08, -3.59693517e+25, -6.89086882e+25, -3.45210231e+25,\n",
              "        -7.14752325e+05, -7.14747402e+05, -7.14744171e+05, -3.93528497e+08,\n",
              "        -3.93528731e+08, -3.93528133e+08, -2.69407436e+25, -3.64599013e+25,\n",
              "        -2.75995172e+25, -7.67956855e+07, -7.67956287e+07, -7.67956507e+07,\n",
              "        -6.57100371e+05, -6.50368731e+05, -6.48817775e+05, -4.40600746e+27,\n",
              "        -7.41706551e+27, -5.78528360e+27, -1.34826708e+06, -1.45327567e+06,\n",
              "        -3.34849834e+06, -6.40904507e+05, -6.40266948e+05, -6.41637597e+05,\n",
              "        -4.94594423e+27, -4.26467616e+27, -7.13898926e+27, -6.70815046e+05,\n",
              "        -6.77293399e+05, -6.55495890e+05, -6.57069036e+05, -6.57193539e+05,\n",
              "        -6.57275826e+05, -5.44709410e+27, -4.48436257e+27, -5.07106766e+27,\n",
              "        -6.41195660e+05, -6.43588653e+05, -6.42850613e+05, -7.31441172e+06,\n",
              "        -7.31414574e+06, -7.31420426e+06, -5.18115113e+27, -2.90266374e+27,\n",
              "        -4.59874249e+27, -6.40623790e+05, -6.40911966e+05, -6.40373382e+05,\n",
              "        -2.69127509e+08, -2.69126828e+08, -2.69126817e+08, -4.88639069e+27,\n",
              "        -2.24580541e+27, -8.16414326e+27, -7.14749602e+05, -7.14736482e+05,\n",
              "        -7.14749353e+05, -3.93528734e+08, -3.93528342e+08, -3.93528655e+08,\n",
              "        -2.75270805e+27, -5.41786236e+27, -5.66062776e+27, -7.67956608e+07,\n",
              "        -7.67956579e+07, -7.67956411e+07, -6.45571951e+05, -6.45366321e+05,\n",
              "        -6.53239566e+05, -5.19040254e+29, -3.16035942e+29, -8.89123107e+29,\n",
              "        -1.16942399e+06, -1.24406032e+06, -1.06656305e+06, -6.40500482e+05,\n",
              "        -6.40565414e+05, -6.40519808e+05, -2.65420815e+29, -5.47514277e+29,\n",
              "        -4.13357682e+29, -6.55325861e+05, -6.66096128e+05, -6.78445053e+05,\n",
              "        -6.57103462e+05, -6.57112141e+05, -6.57196757e+05, -6.26384704e+29,\n",
              "        -5.64018278e+29, -2.67544057e+29, -6.41329192e+05, -6.40348057e+05,\n",
              "        -6.43203335e+05, -7.31410021e+06, -7.31354771e+06, -7.31380924e+06,\n",
              "        -2.66920282e+29, -8.34356874e+29, -3.15584358e+29, -6.40283705e+05,\n",
              "        -6.40333188e+05, -6.40607051e+05, -2.69127894e+08, -2.69125908e+08,\n",
              "        -2.69127291e+08, -4.14621876e+29, -4.86561522e+29, -4.38641834e+29,\n",
              "        -7.14732773e+05, -7.14752654e+05, -7.14745036e+05, -3.93528095e+08,\n",
              "        -3.93528912e+08, -3.93528474e+08, -3.75935926e+29, -3.43338465e+29,\n",
              "        -7.90143237e+29, -7.67957033e+07, -7.67956330e+07, -7.67956928e+07,\n",
              "        -6.45440379e+05, -6.48078804e+05, -6.44106220e+05, -3.75152137e+31,\n",
              "        -2.12457459e+31, -6.69260892e+30, -1.87133892e+06, -9.48151457e+05,\n",
              "        -1.78259057e+06, -6.40778432e+05, -6.40411140e+05, -6.39968712e+05,\n",
              "        -1.44764634e+31, -2.02335688e+31, -1.44174884e+31, -6.55140485e+05,\n",
              "        -6.68328376e+05, -7.07649091e+05, -6.57135985e+05, -6.57109142e+05,\n",
              "        -6.57364647e+05, -1.38517004e+31, -1.41271426e+31, -1.55405814e+31,\n",
              "        -6.42620758e+05, -6.41704713e+05, -6.40776710e+05, -7.31402643e+06,\n",
              "        -7.31391163e+06, -7.31416189e+06, -2.13480928e+31, -1.45667687e+31,\n",
              "        -2.31201811e+31, -6.40452313e+05, -6.40376973e+05, -6.40755467e+05,\n",
              "        -2.69128278e+08, -2.69127178e+08, -2.69126470e+08, -1.05930013e+31,\n",
              "        -1.09895554e+31, -1.78865675e+31, -7.14748262e+05, -7.14740310e+05,\n",
              "        -7.14734020e+05, -3.93528816e+08, -3.93528096e+08, -3.93529046e+08,\n",
              "        -1.73350552e+31, -2.63625922e+31, -1.51994447e+31, -7.67957206e+07,\n",
              "        -7.67956734e+07, -7.67956583e+07]),\n",
              " 'split3_test_score': array([-6.42200693e+05, -7.59049051e+05, -6.39692679e+05, -2.08016330e+21,\n",
              "        -9.04831376e+10, -2.43534170e+21, -7.47672733e+05, -1.44406253e+06,\n",
              "        -1.41636100e+06, -6.31882599e+05, -7.55777922e+05, -6.32951681e+05,\n",
              "        -1.90949710e+21, -1.76885543e+11, -1.82253977e+21, -6.54543818e+05,\n",
              "        -7.70165591e+05, -6.46066001e+05, -6.48062352e+05, -7.58729660e+05,\n",
              "        -6.47951278e+05, -2.91831773e+21, -1.26145721e+11, -2.61325639e+21,\n",
              "        -6.36502275e+05, -7.54279258e+05, -6.35180972e+05, -7.42097036e+06,\n",
              "        -7.46820360e+06, -7.42132515e+06, -3.05598528e+21, -1.33115706e+11,\n",
              "        -1.27509489e+21, -6.31536413e+05, -7.55421768e+05, -6.31692531e+05,\n",
              "        -2.69505449e+08, -2.69523279e+08, -2.69503814e+08, -1.92904193e+21,\n",
              "        -1.11027788e+11, -1.42951743e+21, -7.10651509e+05, -7.94423118e+05,\n",
              "        -7.10650966e+05, -3.93665897e+08, -3.93666429e+08, -3.93665864e+08,\n",
              "        -1.72612519e+21, -9.00392286e+10, -1.71121010e+21, -7.71493675e+07,\n",
              "        -7.71754521e+07, -7.71495758e+07, -6.43455043e+05, -6.45349890e+05,\n",
              "        -6.39763313e+05, -5.33613825e+13, -7.94137694e+05, -1.25895313e+14,\n",
              "        -8.66334480e+05, -1.00360212e+06, -1.11243120e+06, -6.32082854e+05,\n",
              "        -6.36796103e+05, -6.31540845e+05, -7.04283709e+13, -7.30011701e+05,\n",
              "        -7.51972755e+13, -6.64103774e+05, -6.58188928e+05, -6.60462680e+05,\n",
              "        -6.47965191e+05, -6.55164505e+05, -6.47995283e+05, -2.01766208e+13,\n",
              "        -7.18077746e+05, -7.87659950e+13, -6.32084924e+05, -6.37165202e+05,\n",
              "        -6.32351427e+05, -7.42104191e+06, -7.42569529e+06, -7.42132216e+06,\n",
              "        -7.35220008e+13, -7.64970200e+05, -1.79236271e+14, -6.31361732e+05,\n",
              "        -6.36131520e+05, -6.31424020e+05, -2.69503726e+08, -2.69506855e+08,\n",
              "        -2.69504601e+08, -9.49000506e+13, -7.51197628e+05, -1.00484971e+14,\n",
              "        -7.10662112e+05, -7.17602500e+05, -7.10650495e+05, -3.93665891e+08,\n",
              "        -3.93665949e+08, -3.93665500e+08, -1.54596179e+14, -9.17916306e+05,\n",
              "        -7.00928156e+13, -7.71494495e+07, -7.71519858e+07, -7.71494807e+07,\n",
              "        -6.42195658e+05, -6.55668469e+05, -6.44657039e+05, -6.36745095e+05,\n",
              "        -6.38689653e+05, -6.41660966e+05, -9.48157055e+05, -1.73683761e+06,\n",
              "        -9.10240464e+05, -6.32316089e+05, -6.31954400e+05, -6.32379040e+05,\n",
              "        -6.42299746e+05, -6.40645991e+05, -6.38197800e+05, -6.57877475e+05,\n",
              "        -6.53127516e+05, -6.52578315e+05, -6.48038284e+05, -6.48719529e+05,\n",
              "        -6.47868818e+05, -6.40004228e+05, -6.40782593e+05, -6.48465337e+05,\n",
              "        -6.31946768e+05, -6.33661383e+05, -6.33542489e+05, -7.42128266e+06,\n",
              "        -7.42139719e+06, -7.42132221e+06, -6.32755280e+05, -6.38594029e+05,\n",
              "        -6.57414915e+05, -6.31908359e+05, -6.32075143e+05, -6.31874568e+05,\n",
              "        -2.69502446e+08, -2.69502698e+08, -2.69502573e+08, -6.42703816e+05,\n",
              "        -6.43924675e+05, -6.36098540e+05, -7.10640782e+05, -7.11333360e+05,\n",
              "        -7.10655783e+05, -3.93666397e+08, -3.93665916e+08, -3.93666063e+08,\n",
              "        -6.44249086e+05, -6.38715185e+05, -6.39731430e+05, -7.71494707e+07,\n",
              "        -7.71497351e+07, -7.71494587e+07, -6.35905202e+05, -6.39429791e+05,\n",
              "        -6.42933941e+05, -6.68601171e+05, -6.73129399e+05, -7.00637371e+05,\n",
              "        -1.21457927e+06, -1.01566239e+06, -1.27248561e+06, -6.32410271e+05,\n",
              "        -6.32237780e+05, -6.31343115e+05, -7.57921525e+05, -6.90910458e+05,\n",
              "        -6.81026798e+05, -6.58097099e+05, -6.54265285e+05, -6.52954209e+05,\n",
              "        -6.47963369e+05, -6.47955683e+05, -6.48101785e+05, -7.23032372e+05,\n",
              "        -7.18147769e+05, -6.84544110e+05, -6.34689080e+05, -6.33943243e+05,\n",
              "        -6.32175899e+05, -7.42118556e+06, -7.42136651e+06, -7.42106054e+06,\n",
              "        -6.61485140e+05, -6.98384179e+05, -7.09916467e+05, -6.31495461e+05,\n",
              "        -6.31777282e+05, -6.31530142e+05, -2.69503495e+08, -2.69503580e+08,\n",
              "        -2.69503752e+08, -7.05279570e+05, -7.20478540e+05, -7.16208419e+05,\n",
              "        -7.10659318e+05, -7.10715424e+05, -7.10654971e+05, -3.93665631e+08,\n",
              "        -3.93665886e+08, -3.93666368e+08, -6.94767630e+05, -7.19977570e+05,\n",
              "        -7.19894131e+05, -7.71495084e+07, -7.71495378e+07, -7.71494817e+07,\n",
              "        -6.38789947e+05, -6.37180438e+05, -6.36382524e+05, -1.43113426e+06,\n",
              "        -2.94723667e+06, -1.71614160e+06, -2.01961445e+06, -1.51597992e+06,\n",
              "        -1.17593616e+06, -6.31475010e+05, -6.31591032e+05, -6.32178421e+05,\n",
              "        -2.87588734e+06, -2.94653550e+06, -5.84276868e+06, -6.53924230e+05,\n",
              "        -6.80181993e+05, -6.59939196e+05, -6.47915587e+05, -6.47918692e+05,\n",
              "        -6.47823628e+05, -1.74532933e+06, -2.55155491e+06, -3.37482134e+06,\n",
              "        -6.32611446e+05, -6.33793234e+05, -6.32016724e+05, -7.42102648e+06,\n",
              "        -7.42078545e+06, -7.42143086e+06, -1.44079695e+06, -9.86608393e+06,\n",
              "        -2.49122380e+06, -6.31741372e+05, -6.31815606e+05, -6.31715081e+05,\n",
              "        -2.69504833e+08, -2.69505274e+08, -2.69503534e+08, -3.62521667e+06,\n",
              "        -3.13217741e+06, -4.52470190e+06, -7.10663296e+05, -7.10657415e+05,\n",
              "        -7.10660167e+05, -3.93666028e+08, -3.93665730e+08, -3.93665918e+08,\n",
              "        -2.90991214e+06, -2.03858255e+06, -1.94713100e+06, -7.71494990e+07,\n",
              "        -7.71495039e+07, -7.71494537e+07, -6.36573184e+05, -6.38024222e+05,\n",
              "        -6.42374314e+05, -3.83634280e+25, -3.17136658e+25, -1.98672610e+25,\n",
              "        -1.10812423e+06, -1.76101791e+06, -1.22516055e+06, -6.31471531e+05,\n",
              "        -6.31772727e+05, -6.31848407e+05, -4.30591465e+25, -6.32927469e+25,\n",
              "        -3.05756928e+25, -7.13447735e+05, -7.01826943e+05, -6.63129738e+05,\n",
              "        -6.48002560e+05, -6.48073749e+05, -6.48080975e+05, -5.40573948e+25,\n",
              "        -3.20829990e+25, -4.35688778e+25, -6.33085304e+05, -6.32042114e+05,\n",
              "        -6.35021461e+05, -7.42096957e+06, -7.42159759e+06, -7.42119494e+06,\n",
              "        -3.95681319e+25, -5.13211928e+25, -1.85333545e+25, -6.31935819e+05,\n",
              "        -6.31584516e+05, -6.31815362e+05, -2.69503873e+08, -2.69503467e+08,\n",
              "        -2.69503377e+08, -2.45966038e+25, -3.28047044e+25, -4.77085461e+25,\n",
              "        -7.10659585e+05, -7.10636020e+05, -7.10666243e+05, -3.93666168e+08,\n",
              "        -3.93665943e+08, -3.93665754e+08, -2.94464660e+25, -4.26701432e+25,\n",
              "        -4.42687659e+25, -7.71494351e+07, -7.71495052e+07, -7.71495322e+07,\n",
              "        -6.39707258e+05, -6.39573854e+05, -6.42724400e+05, -3.04916208e+27,\n",
              "        -3.85683456e+27, -5.49510702e+27, -1.22731723e+06, -1.26801549e+06,\n",
              "        -1.27625730e+06, -6.32008376e+05, -6.31484197e+05, -6.32034243e+05,\n",
              "        -6.38087459e+27, -4.96821322e+27, -3.22321824e+27, -6.60619226e+05,\n",
              "        -6.52632308e+05, -6.47024725e+05, -6.47965492e+05, -6.47914615e+05,\n",
              "        -6.47921038e+05, -6.22094619e+27, -4.69339370e+27, -5.05266607e+27,\n",
              "        -6.32186239e+05, -6.34816856e+05, -6.32415825e+05, -7.42128876e+06,\n",
              "        -7.42193339e+06, -7.42129604e+06, -2.76027447e+27, -4.09050474e+27,\n",
              "        -5.76058415e+27, -6.32133073e+05, -6.31678990e+05, -6.31655451e+05,\n",
              "        -2.69503647e+08, -2.69504922e+08, -2.69504583e+08, -4.24141663e+27,\n",
              "        -5.48670137e+27, -3.61269864e+27, -7.10651892e+05, -7.10670753e+05,\n",
              "        -7.10678022e+05, -3.93666030e+08, -3.93665966e+08, -3.93666004e+08,\n",
              "        -2.48563356e+27, -6.45046558e+27, -4.12537354e+27, -7.71494767e+07,\n",
              "        -7.71495382e+07, -7.71494837e+07, -6.39760944e+05, -6.37268735e+05,\n",
              "        -6.36754570e+05, -2.81063600e+29, -2.34282180e+29, -3.20483968e+29,\n",
              "        -1.14023767e+06, -1.76873118e+06, -1.46293279e+06, -6.31789994e+05,\n",
              "        -6.31907363e+05, -6.31823574e+05, -2.73370177e+29, -4.74359660e+29,\n",
              "        -3.60054686e+29, -6.54345545e+05, -6.50192089e+05, -6.55189568e+05,\n",
              "        -6.48116636e+05, -6.47973603e+05, -6.47991992e+05, -3.31483229e+29,\n",
              "        -5.21190377e+29, -2.95476230e+29, -6.31684269e+05, -6.34722792e+05,\n",
              "        -6.34382657e+05, -7.42110011e+06, -7.42100172e+06, -7.42101615e+06,\n",
              "        -4.74702297e+29, -3.44886085e+29, -2.07173502e+29, -6.32021840e+05,\n",
              "        -6.31947918e+05, -6.31633466e+05, -2.69504619e+08, -2.69503322e+08,\n",
              "        -2.69505135e+08, -2.99452059e+29, -2.42565703e+29, -3.24725257e+29,\n",
              "        -7.10669554e+05, -7.10654743e+05, -7.10653774e+05, -3.93665732e+08,\n",
              "        -3.93666108e+08, -3.93665885e+08, -4.35532495e+29, -6.90888793e+29,\n",
              "        -2.06668804e+29, -7.71495431e+07, -7.71494925e+07, -7.71494771e+07,\n",
              "        -6.39651481e+05, -6.36919461e+05, -6.38958584e+05, -2.22954799e+31,\n",
              "        -3.34747031e+31, -1.67983139e+31, -1.16474475e+06, -1.19799630e+06,\n",
              "        -1.32987728e+06, -6.32039210e+05, -6.31401292e+05, -6.31311784e+05,\n",
              "        -9.80035534e+30, -1.79918977e+31, -2.43020058e+31, -6.54787755e+05,\n",
              "        -7.09044110e+05, -6.60175193e+05, -6.47893888e+05, -6.48015850e+05,\n",
              "        -6.48048256e+05, -2.71731708e+31, -8.31315728e+30, -2.65070170e+31,\n",
              "        -6.33424945e+05, -6.33352467e+05, -6.33322404e+05, -7.42081980e+06,\n",
              "        -7.42099210e+06, -7.42167466e+06, -2.78113903e+31, -1.70647447e+31,\n",
              "        -1.09802431e+31, -6.31885899e+05, -6.31954208e+05, -6.31765280e+05,\n",
              "        -2.69503842e+08, -2.69503457e+08, -2.69504415e+08, -1.39355952e+31,\n",
              "        -2.18550059e+31, -1.41963417e+31, -7.10643721e+05, -7.10662083e+05,\n",
              "        -7.10657448e+05, -3.93665466e+08, -3.93666932e+08, -3.93665810e+08,\n",
              "        -3.54685424e+31, -8.91168463e+30, -1.87888613e+31, -7.71494871e+07,\n",
              "        -7.71494960e+07, -7.71494834e+07]),\n",
              " 'split4_test_score': array([-6.27298552e+05, -7.56640348e+05, -6.21561953e+05, -1.63072224e+21,\n",
              "        -1.26297915e+11, -2.09372592e+21, -7.50348243e+05, -1.24716508e+06,\n",
              "        -1.32338160e+06, -6.15486368e+05, -7.43929044e+05, -6.16567148e+05,\n",
              "        -2.43820739e+21, -7.08147645e+10, -1.53106285e+21, -6.38261937e+05,\n",
              "        -7.74997614e+05, -6.47395677e+05, -6.32129424e+05, -7.44724138e+05,\n",
              "        -6.32303638e+05, -2.14218189e+21, -1.73318686e+11, -3.79042605e+21,\n",
              "        -6.14776176e+05, -7.43837984e+05, -6.17027646e+05, -7.26940493e+06,\n",
              "        -7.30652547e+06, -7.26938459e+06, -2.03208728e+21, -7.89214730e+10,\n",
              "        -1.53653732e+21, -6.15592029e+05, -7.41301077e+05, -6.15850422e+05,\n",
              "        -2.68034310e+08, -2.68048324e+08, -2.68034293e+08, -1.74087831e+21,\n",
              "        -1.44572491e+11, -1.58876697e+21, -6.93360245e+05, -7.77852070e+05,\n",
              "        -6.93410202e+05, -3.91534054e+08, -3.91535386e+08, -3.91534046e+08,\n",
              "        -1.34808772e+21, -1.10404498e+11, -2.10421228e+21, -7.65486199e+07,\n",
              "        -7.65617665e+07, -7.65488480e+07, -6.27169121e+05, -6.28978766e+05,\n",
              "        -6.26160915e+05, -5.22385587e+13, -7.48220352e+05, -6.79981604e+13,\n",
              "        -9.23724538e+05, -1.05178573e+06, -1.05357689e+06, -6.15976329e+05,\n",
              "        -6.21974109e+05, -6.15693906e+05, -3.50945073e+14, -6.39959153e+05,\n",
              "        -2.39193962e+14, -6.29539483e+05, -6.68106476e+05, -6.52855409e+05,\n",
              "        -6.32120305e+05, -6.40189280e+05, -6.32153098e+05, -1.63786945e+14,\n",
              "        -8.13788866e+05, -2.31091860e+13, -6.17573079e+05, -6.24002684e+05,\n",
              "        -6.18287929e+05, -7.26894246e+06, -7.27300404e+06, -7.26931244e+06,\n",
              "        -5.37362219e+13, -6.84801447e+05, -5.79667895e+13, -6.15535230e+05,\n",
              "        -6.21123621e+05, -6.15594227e+05, -2.68033089e+08, -2.68033990e+08,\n",
              "        -2.68034002e+08, -5.85581701e+13, -8.25488558e+05, -3.66149524e+13,\n",
              "        -6.93399016e+05, -7.00480317e+05, -6.93421252e+05, -3.91534072e+08,\n",
              "        -3.91534343e+08, -3.91534583e+08, -2.18345976e+14, -9.77917404e+05,\n",
              "        -5.30758238e+13, -7.65488130e+07, -7.65500246e+07, -7.65488928e+07,\n",
              "        -6.25899368e+05, -6.32748487e+05, -6.30729588e+05, -6.24096432e+05,\n",
              "        -6.28108479e+05, -6.25829679e+05, -9.69474628e+05, -1.67272090e+06,\n",
              "        -1.15319572e+06, -6.15610623e+05, -6.16236529e+05, -6.15954542e+05,\n",
              "        -6.30809181e+05, -6.28750425e+05, -6.26556736e+05, -6.30034248e+05,\n",
              "        -6.37976650e+05, -6.29461502e+05, -6.32244945e+05, -6.32863408e+05,\n",
              "        -6.32195753e+05, -6.23779500e+05, -6.29416108e+05, -6.23292315e+05,\n",
              "        -6.16963001e+05, -6.18447362e+05, -6.16726057e+05, -7.26949776e+06,\n",
              "        -7.26983898e+06, -7.26922851e+06, -6.31551543e+05, -6.18914646e+05,\n",
              "        -6.22587904e+05, -6.15594619e+05, -6.15797810e+05, -6.15855195e+05,\n",
              "        -2.68033012e+08, -2.68034605e+08, -2.68034017e+08, -6.25721596e+05,\n",
              "        -6.25785337e+05, -6.27396432e+05, -6.93416195e+05, -6.94088944e+05,\n",
              "        -6.93403320e+05, -3.91534144e+08, -3.91533882e+08, -3.91534059e+08,\n",
              "        -6.33195756e+05, -6.19346418e+05, -6.29600869e+05, -7.65488160e+07,\n",
              "        -7.65489331e+07, -7.65488466e+07, -6.22502475e+05, -6.24668884e+05,\n",
              "        -6.26865071e+05, -6.56140305e+05, -6.74925859e+05, -6.62750474e+05,\n",
              "        -8.25706183e+05, -1.08149572e+06, -1.34990484e+06, -6.15623242e+05,\n",
              "        -6.15946549e+05, -6.16118747e+05, -6.74306097e+05, -6.63261438e+05,\n",
              "        -7.46641530e+05, -6.37627641e+05, -6.69609014e+05, -6.56366248e+05,\n",
              "        -6.32140410e+05, -6.32133335e+05, -6.32175962e+05, -6.91818066e+05,\n",
              "        -7.20172698e+05, -7.28892796e+05, -6.19847699e+05, -6.16980710e+05,\n",
              "        -6.17707748e+05, -7.26918547e+06, -7.26952508e+06, -7.26930066e+06,\n",
              "        -6.71532666e+05, -7.12712017e+05, -6.73110413e+05, -6.15426584e+05,\n",
              "        -6.15935276e+05, -6.15962234e+05, -2.68034182e+08, -2.68033238e+08,\n",
              "        -2.68034075e+08, -6.54892828e+05, -7.12158853e+05, -6.86641841e+05,\n",
              "        -6.93409734e+05, -6.93464125e+05, -6.93424596e+05, -3.91533805e+08,\n",
              "        -3.91533907e+08, -3.91533901e+08, -6.72978398e+05, -7.40223234e+05,\n",
              "        -7.65518129e+05, -7.65488494e+07, -7.65488724e+07, -7.65488667e+07,\n",
              "        -6.30386489e+05, -6.26317820e+05, -6.20436440e+05, -1.05866464e+06,\n",
              "        -1.78071171e+06, -1.43154928e+06, -1.17205205e+06, -1.14019152e+06,\n",
              "        -1.42348369e+06, -6.16226280e+05, -6.16072849e+05, -6.16187932e+05,\n",
              "        -4.16229960e+06, -1.33357350e+06, -2.33112465e+06, -6.36229849e+05,\n",
              "        -6.26017937e+05, -6.43734472e+05, -6.32288521e+05, -6.32149999e+05,\n",
              "        -6.32253498e+05, -2.03884716e+06, -5.48403665e+06, -3.34439508e+06,\n",
              "        -6.18125049e+05, -6.17031470e+05, -6.20236334e+05, -7.26941219e+06,\n",
              "        -7.26968859e+06, -7.26909261e+06, -4.29035483e+06, -1.74516285e+06,\n",
              "        -2.78183714e+06, -6.15591053e+05, -6.15678391e+05, -6.15607407e+05,\n",
              "        -2.68034753e+08, -2.68033167e+08, -2.68033394e+08, -1.12272797e+06,\n",
              "        -2.29131285e+06, -1.48851823e+06, -6.93408656e+05, -6.93398355e+05,\n",
              "        -6.93408748e+05, -3.91533864e+08, -3.91533812e+08, -3.91533819e+08,\n",
              "        -1.66820819e+06, -5.81939128e+06, -3.90609538e+07, -7.65488178e+07,\n",
              "        -7.65488479e+07, -7.65488299e+07, -6.23822277e+05, -6.31166573e+05,\n",
              "        -6.22295906e+05, -2.92883259e+25, -4.13135738e+25, -3.36205126e+25,\n",
              "        -1.87014999e+06, -1.39780156e+06, -1.13162482e+06, -6.15642104e+05,\n",
              "        -6.16394661e+05, -6.16441751e+05, -5.64774365e+25, -3.98879554e+25,\n",
              "        -6.33195771e+25, -6.63142513e+05, -6.41203241e+05, -6.46268008e+05,\n",
              "        -6.32107742e+05, -6.32190912e+05, -6.32159363e+05, -3.87921940e+25,\n",
              "        -3.06120325e+25, -5.34374374e+25, -6.16788064e+05, -6.16285650e+05,\n",
              "        -6.18542600e+05, -7.26940148e+06, -7.26942464e+06, -7.26953577e+06,\n",
              "        -3.22055989e+25, -3.05077741e+25, -3.08063715e+25, -6.15814904e+05,\n",
              "        -6.15810244e+05, -6.15758152e+05, -2.68032747e+08, -2.68033844e+08,\n",
              "        -2.68032775e+08, -2.85209327e+25, -5.69931111e+25, -4.99383330e+25,\n",
              "        -6.93400167e+05, -6.93418299e+05, -6.93389827e+05, -3.91533927e+08,\n",
              "        -3.91534166e+08, -3.91534151e+08, -9.20377530e+25, -7.94304757e+25,\n",
              "        -6.33714000e+25, -7.65488496e+07, -7.65488330e+07, -7.65488222e+07,\n",
              "        -6.29502528e+05, -6.31380238e+05, -6.21721272e+05, -4.47488119e+27,\n",
              "        -4.88624163e+27, -6.53088521e+27, -1.08530214e+06, -1.12068781e+06,\n",
              "        -1.14956374e+06, -6.16072655e+05, -6.15519994e+05, -6.17285105e+05,\n",
              "        -3.18126968e+27, -6.57638899e+27, -4.36084083e+27, -6.42599703e+05,\n",
              "        -6.44561137e+05, -6.38046836e+05, -6.32095502e+05, -6.32196996e+05,\n",
              "        -6.32128127e+05, -6.38490600e+27, -3.65509723e+27, -3.77650495e+27,\n",
              "        -6.17108703e+05, -6.21882996e+05, -6.16747805e+05, -7.26900680e+06,\n",
              "        -7.26964690e+06, -7.26928782e+06, -5.19068725e+27, -9.43443829e+27,\n",
              "        -4.37092445e+27, -6.15454085e+05, -6.15388024e+05, -6.15670214e+05,\n",
              "        -2.68033445e+08, -2.68034165e+08, -2.68034970e+08, -3.13362409e+27,\n",
              "        -4.73366032e+27, -3.71816653e+27, -6.93397958e+05, -6.93404829e+05,\n",
              "        -6.93396597e+05, -3.91533928e+08, -3.91533385e+08, -3.91533937e+08,\n",
              "        -5.41077096e+27, -5.46027283e+27, -5.43189871e+27, -7.65488498e+07,\n",
              "        -7.65488440e+07, -7.65488488e+07, -6.23983529e+05, -6.24148441e+05,\n",
              "        -6.19631866e+05, -3.27134999e+29, -7.67133575e+29, -4.87971321e+29,\n",
              "        -1.17100810e+06, -8.76152969e+05, -1.14401429e+06, -6.15610275e+05,\n",
              "        -6.15237962e+05, -6.16356221e+05, -5.86841282e+29, -4.87373543e+29,\n",
              "        -5.12157144e+29, -6.62620285e+05, -6.47592111e+05, -6.38337260e+05,\n",
              "        -6.32292844e+05, -6.32145860e+05, -6.32117073e+05, -4.18037235e+29,\n",
              "        -2.60447004e+29, -2.60293226e+29, -6.18618921e+05, -6.18547361e+05,\n",
              "        -6.17310939e+05, -7.26913814e+06, -7.26928639e+06, -7.26936562e+06,\n",
              "        -4.42970638e+29, -7.03863084e+29, -6.68341052e+29, -6.15893441e+05,\n",
              "        -6.15703890e+05, -6.15619226e+05, -2.68033153e+08, -2.68033778e+08,\n",
              "        -2.68032516e+08, -4.27376008e+29, -3.93530759e+29, -2.63612385e+29,\n",
              "        -6.93412448e+05, -6.93408900e+05, -6.93395680e+05, -3.91534442e+08,\n",
              "        -3.91534074e+08, -3.91534616e+08, -2.82563416e+29, -5.98322301e+29,\n",
              "        -2.07573474e+29, -7.65488315e+07, -7.65488457e+07, -7.65488504e+07,\n",
              "        -6.21755674e+05, -6.18458288e+05, -6.21749717e+05, -9.52261078e+30,\n",
              "        -2.52270293e+31, -2.09448256e+31, -9.37010028e+05, -1.30344721e+06,\n",
              "        -1.72574183e+06, -6.15935637e+05, -6.15964634e+05, -6.16557780e+05,\n",
              "        -6.42080350e+30, -2.53878956e+31, -1.37512942e+31, -6.47474163e+05,\n",
              "        -6.33334089e+05, -6.65925997e+05, -6.32004522e+05, -6.32269840e+05,\n",
              "        -6.32220293e+05, -2.42417174e+31, -1.58912262e+31, -1.09758023e+31,\n",
              "        -6.15661932e+05, -6.16875970e+05, -6.19622832e+05, -7.26952320e+06,\n",
              "        -7.26928906e+06, -7.26954721e+06, -2.91184714e+31, -1.74875467e+31,\n",
              "        -2.47476630e+31, -6.15304068e+05, -6.15922216e+05, -6.15886582e+05,\n",
              "        -2.68033199e+08, -2.68033909e+08, -2.68033593e+08, -1.97812439e+31,\n",
              "        -2.23969783e+31, -1.85753814e+31, -6.93411046e+05, -6.93411425e+05,\n",
              "        -6.93390752e+05, -3.91534356e+08, -3.91534344e+08, -3.91533676e+08,\n",
              "        -1.57501784e+31, -1.33449751e+31, -1.16075883e+31, -7.65488113e+07,\n",
              "        -7.65488309e+07, -7.65488168e+07]),\n",
              " 'mean_test_score': array([7.96488059e+02, 8.74851014e+02, 7.96413056e+02, 4.37408707e+10,\n",
              "        3.27757779e+05, 4.47533560e+10, 8.77293361e+02, 1.18030380e+03,\n",
              "        1.08688131e+03, 7.92247613e+02, 8.67392953e+02, 7.92656473e+02,\n",
              "        4.15636073e+10, 3.24061360e+05, 4.22235203e+10, 8.10733512e+02,\n",
              "        8.80222091e+02, 8.17952960e+02, 8.03445665e+02, 8.68748843e+02,\n",
              "        8.03513792e+02, 4.55763651e+10, 3.52123725e+05, 5.16488978e+10,\n",
              "        7.93499324e+02, 8.67921025e+02, 7.93508489e+02, 2.70143823e+03,\n",
              "        2.70888753e+03, 2.70146910e+03, 4.38613991e+10, 3.27605282e+05,\n",
              "        4.59703526e+10, 7.92179519e+02, 8.67090341e+02, 7.92228873e+02,\n",
              "        1.63891530e+04, 1.63897173e+04, 1.63891415e+04, 4.43236250e+10,\n",
              "        3.44178253e+05, 4.26407262e+10, 8.40693478e+02, 8.87842606e+02,\n",
              "        8.40716237e+02, 1.98145480e+04, 1.98145750e+04, 1.98145489e+04,\n",
              "        3.86878550e+10, 3.23633839e+05, 4.58308794e+10, 8.75655817e+03,\n",
              "        8.75785505e+03, 8.75657094e+03, 7.98944607e+02, 8.00648392e+02,\n",
              "        7.96625223e+02, 1.04273914e+07, 9.10412195e+02, 1.15842216e+07,\n",
              "        9.37758293e+02, 1.14720024e+03, 1.23988263e+03, 7.92316698e+02,\n",
              "        7.96126688e+02, 7.92385016e+02, 1.01424467e+07, 8.81942213e+02,\n",
              "        1.03428704e+07, 8.04884397e+02, 8.16082447e+02, 8.11989959e+02,\n",
              "        8.03475321e+02, 8.07979827e+02, 8.03465969e+02, 7.78177267e+06,\n",
              "        9.15985795e+02, 7.67941699e+06, 7.93440595e+02, 7.96632365e+02,\n",
              "        7.93728026e+02, 2.70146992e+03, 2.70217018e+03, 2.70148329e+03,\n",
              "        1.10893160e+07, 8.86593554e+02, 9.41297596e+06, 7.92338459e+02,\n",
              "        7.95755490e+02, 7.92264095e+02, 1.63891334e+04, 1.63891946e+04,\n",
              "        1.63891344e+04, 8.25354455e+06, 8.93140490e+02, 1.10452365e+07,\n",
              "        8.40715491e+02, 8.44686410e+02, 8.40714589e+02, 1.98145469e+04,\n",
              "        1.98145457e+04, 1.98145491e+04, 1.35145479e+07, 9.39320851e+02,\n",
              "        8.04329543e+06, 8.75656815e+03, 8.75669100e+03, 8.75657025e+03,\n",
              "        7.97792717e+02, 8.00390816e+02, 7.99725962e+02, 7.97778812e+02,\n",
              "        7.98889427e+02, 7.97591797e+02, 1.11110674e+03, 1.21134735e+03,\n",
              "        1.06883192e+03, 7.92357215e+02, 7.92388592e+02, 7.92433928e+02,\n",
              "        8.00672621e+02, 8.00081317e+02, 7.98722660e+02, 8.08248804e+02,\n",
              "        8.08081675e+02, 8.10732825e+02, 8.03480104e+02, 8.03883125e+02,\n",
              "        8.03458830e+02, 7.97667775e+02, 7.98898057e+02, 7.98920601e+02,\n",
              "        7.93113819e+02, 7.93534081e+02, 7.93438259e+02, 2.70149598e+03,\n",
              "        2.70152322e+03, 2.70149038e+03, 7.98046146e+02, 7.97410793e+02,\n",
              "        7.99538348e+02, 7.92326820e+02, 7.92429723e+02, 7.92441813e+02,\n",
              "        1.63891289e+04, 1.63891316e+04, 1.63891159e+04, 7.99071947e+02,\n",
              "        7.97140853e+02, 7.97355374e+02, 8.40716077e+02, 8.41099053e+02,\n",
              "        8.40716191e+02, 1.98145457e+04, 1.98145474e+04, 1.98145480e+04,\n",
              "        8.01259196e+02, 7.97305508e+02, 7.98291606e+02, 8.75656960e+03,\n",
              "        8.75658137e+03, 8.75656985e+03, 7.95291747e+02, 7.97676697e+02,\n",
              "        7.98142126e+02, 8.23291885e+02, 8.38851244e+02, 8.32293679e+02,\n",
              "        1.09997190e+03, 1.14545852e+03, 1.11469771e+03, 7.92532783e+02,\n",
              "        7.92302864e+02, 7.92449508e+02, 8.37432897e+02, 8.29573917e+02,\n",
              "        8.43228697e+02, 8.12679199e+02, 8.12151571e+02, 8.15511084e+02,\n",
              "        8.03450255e+02, 8.03488568e+02, 8.03506287e+02, 8.38127977e+02,\n",
              "        8.47176240e+02, 8.34795898e+02, 7.94237394e+02, 7.93200189e+02,\n",
              "        7.93409857e+02, 2.70146104e+03, 2.70148319e+03, 2.70148738e+03,\n",
              "        8.41832826e+02, 8.43018002e+02, 8.38200209e+02, 7.92239537e+02,\n",
              "        7.92304456e+02, 7.92261657e+02, 1.63891435e+04, 1.63891260e+04,\n",
              "        1.63891309e+04, 8.41904815e+02, 8.43986655e+02, 8.43591040e+02,\n",
              "        8.40715693e+02, 8.40750548e+02, 8.40718241e+02, 1.98145431e+04,\n",
              "        1.98145486e+04, 1.98145437e+04, 8.26461094e+02, 8.44543655e+02,\n",
              "        8.42347906e+02, 8.75656980e+03, 8.75657232e+03, 8.75656938e+03,\n",
              "        7.98575158e+02, 7.99815175e+02, 7.94788146e+02, 1.36384184e+03,\n",
              "        1.75413731e+03, 1.50454635e+03, 1.25730762e+03, 1.19319637e+03,\n",
              "        1.15558310e+03, 7.92497662e+02, 7.92379475e+02, 7.92482512e+02,\n",
              "        1.54090691e+03, 1.53425568e+03, 1.95029976e+03, 8.06973512e+02,\n",
              "        8.11956806e+02, 8.10278379e+02, 8.03487960e+02, 8.03431278e+02,\n",
              "        8.03460191e+02, 1.50957815e+03, 2.06531091e+03, 1.52076220e+03,\n",
              "        7.93199396e+02, 7.93583936e+02, 7.94379494e+02, 2.70145458e+03,\n",
              "        2.70148612e+03, 2.70147269e+03, 1.75721204e+03, 1.86026067e+03,\n",
              "        1.59258282e+03, 7.92364370e+02, 7.92204727e+02, 7.92310554e+02,\n",
              "        1.63891309e+04, 1.63891144e+04, 1.63891433e+04, 1.36585591e+03,\n",
              "        1.40505139e+03, 1.69963045e+03, 8.40718259e+02, 8.40720609e+02,\n",
              "        8.40719328e+02, 1.98145476e+04, 1.98145445e+04, 1.98145464e+04,\n",
              "        1.50454255e+03, 1.80965414e+03, 3.07775621e+03, 8.75656892e+03,\n",
              "        8.75656881e+03, 8.75656790e+03, 7.97628855e+02, 7.98177377e+02,\n",
              "        7.97225235e+02, 6.46550203e+12, 5.67296219e+12, 5.90313779e+12,\n",
              "        1.10925572e+03, 1.29221286e+03, 1.19340509e+03, 7.92253743e+02,\n",
              "        7.92619106e+02, 7.92306249e+02, 5.94413774e+12, 7.90506398e+12,\n",
              "        7.71790385e+12, 8.16127119e+02, 8.21746061e+02, 8.08024602e+02,\n",
              "        8.03440169e+02, 8.03465550e+02, 8.03475865e+02, 6.44061293e+12,\n",
              "        6.29316229e+12, 7.08682318e+12, 7.93201308e+02, 7.92856557e+02,\n",
              "        7.94144724e+02, 2.70146859e+03, 2.70149364e+03, 2.70147447e+03,\n",
              "        6.89742036e+12, 6.05896983e+12, 6.63188491e+12, 7.92300958e+02,\n",
              "        7.92265125e+02, 7.92378671e+02, 1.63891149e+04, 1.63891234e+04,\n",
              "        1.63891183e+04, 5.70864167e+12, 6.98995327e+12, 6.80001972e+12,\n",
              "        8.40716883e+02, 8.40715946e+02, 8.40717114e+02, 1.98145443e+04,\n",
              "        1.98145440e+04, 1.98145475e+04, 7.16518387e+12, 7.08458515e+12,\n",
              "        6.64566753e+12, 8.75656858e+03, 8.75656923e+03, 8.75656940e+03,\n",
              "        7.99151354e+02, 7.98212680e+02, 7.96623889e+02, 5.79519113e+13,\n",
              "        7.33530542e+13, 8.09126430e+13, 1.13402320e+03, 1.14362082e+03,\n",
              "        1.28527853e+03, 7.92643352e+02, 7.92221496e+02, 7.92734268e+02,\n",
              "        6.85865408e+13, 7.70709821e+13, 6.96997375e+13, 8.16553937e+02,\n",
              "        8.10225401e+02, 8.12946882e+02, 8.03472537e+02, 8.03484598e+02,\n",
              "        8.03455188e+02, 7.42627265e+13, 6.13704436e+13, 7.22073904e+13,\n",
              "        7.92741591e+02, 7.94247814e+02, 7.93060314e+02, 2.70148994e+03,\n",
              "        2.70151478e+03, 2.70147090e+03, 6.97233974e+13, 7.32506270e+13,\n",
              "        7.21621641e+13, 7.92441558e+02, 7.92293577e+02, 7.92263757e+02,\n",
              "        1.63891390e+04, 1.63891345e+04, 1.63891475e+04, 7.32081116e+13,\n",
              "        6.29211182e+13, 7.30590655e+13, 8.40716663e+02, 8.40719809e+02,\n",
              "        8.40719298e+02, 1.98145493e+04, 1.98145471e+04, 1.98145506e+04,\n",
              "        7.15591406e+13, 8.08624974e+13, 6.78357649e+13, 8.75656870e+03,\n",
              "        8.75656926e+03, 8.75656937e+03, 7.97286094e+02, 7.97081079e+02,\n",
              "        7.97233774e+02, 5.84755026e+14, 6.82282354e+14, 7.13463024e+14,\n",
              "        1.13333990e+03, 1.12866764e+03, 1.13161537e+03, 7.92295044e+02,\n",
              "        7.92190785e+02, 7.92458292e+02, 6.20607320e+14, 7.10696208e+14,\n",
              "        6.77598490e+14, 8.10012703e+02, 8.08847766e+02, 8.07661173e+02,\n",
              "        8.03492472e+02, 8.03462711e+02, 8.03471900e+02, 6.32584531e+14,\n",
              "        6.86766045e+14, 5.63414621e+14, 7.93461928e+02, 7.93350611e+02,\n",
              "        7.93607196e+02, 2.70146620e+03, 2.70143917e+03, 2.70146610e+03,\n",
              "        6.28596220e+14, 7.73009168e+14, 6.32835325e+14, 7.92350576e+02,\n",
              "        7.92280693e+02, 7.92193818e+02, 1.63891323e+04, 1.63891235e+04,\n",
              "        1.63891311e+04, 6.25388640e+14, 6.42255975e+14, 6.25811542e+14,\n",
              "        8.40721023e+02, 8.40718332e+02, 8.40713595e+02, 1.98145444e+04,\n",
              "        1.98145500e+04, 1.98145497e+04, 6.25111762e+14, 7.13893649e+14,\n",
              "        7.69601921e+14, 8.75656937e+03, 8.75656918e+03, 8.75657022e+03,\n",
              "        7.96418339e+02, 7.95705845e+02, 7.97010277e+02, 4.58332784e+15,\n",
              "        4.74082067e+15, 4.05682146e+15, 1.15236844e+03, 1.13238242e+03,\n",
              "        1.26278087e+03, 7.92187634e+02, 7.92372981e+02, 7.92293741e+02,\n",
              "        4.04592868e+15, 4.79457635e+15, 4.21895174e+15, 8.06197033e+02,\n",
              "        8.10683571e+02, 8.18077149e+02, 8.03448344e+02, 8.03472832e+02,\n",
              "        8.03506253e+02, 4.72353736e+15, 3.86183311e+15, 4.35611628e+15,\n",
              "        7.92889335e+02, 7.93165453e+02, 7.93356191e+02, 2.70148281e+03,\n",
              "        2.70143754e+03, 2.70150051e+03, 5.12070160e+15, 4.24781746e+15,\n",
              "        4.48675728e+15, 7.92260248e+02, 7.92327244e+02, 7.92350569e+02,\n",
              "        1.63891201e+04, 1.63891192e+04, 1.63891367e+04, 4.35168644e+15,\n",
              "        4.72747037e+15, 4.13297863e+15, 8.40715732e+02, 8.40717163e+02,\n",
              "        8.40715100e+02, 1.98145428e+04, 1.98145464e+04, 1.98145423e+04,\n",
              "        4.47425212e+15, 3.88397536e+15, 4.09637653e+15, 8.75656902e+03,\n",
              "        8.75656928e+03, 8.75656942e+03]),\n",
              " 'std_test_score': array([1.28603765e+04, 1.62207039e+04, 1.32924067e+04, 3.47473678e+20,\n",
              "        1.94271729e+10, 5.11918599e+20, 6.13796081e+04, 1.76883215e+05,\n",
              "        2.04962702e+05, 1.38203537e+04, 1.39568229e+04, 1.36860323e+04,\n",
              "        5.08716970e+20, 4.22766290e+10, 1.57512388e+20, 1.59588469e+04,\n",
              "        1.95235910e+04, 2.85524269e+04, 1.34509842e+04, 1.43621423e+04,\n",
              "        1.34304846e+04, 6.17314706e+20, 4.94470903e+10, 7.09893628e+20,\n",
              "        1.46302752e+04, 1.38906816e+04, 1.45414164e+04, 7.46761648e+04,\n",
              "        7.85701993e+04, 7.45668099e+04, 7.36032991e+20, 3.19602947e+10,\n",
              "        6.89837329e+20, 1.39119264e+04, 1.44422361e+04, 1.38203619e+04,\n",
              "        5.98880195e+05, 5.99770482e+05, 5.98383986e+05, 1.91040944e+20,\n",
              "        1.78322377e+10, 4.46565979e+20, 1.44971060e+04, 1.45320802e+04,\n",
              "        1.45198387e+04, 8.48051597e+05, 8.47798718e+05, 8.48186125e+05,\n",
              "        3.14241552e+20, 4.07636415e+10, 2.37175781e+20, 2.83634710e+05,\n",
              "        2.85873524e+05, 2.83620429e+05, 1.47599478e+04, 1.17524557e+04,\n",
              "        1.33178475e+04, 1.01931558e+14, 1.44812183e+05, 8.36670730e+13,\n",
              "        1.06464998e+05, 3.83680888e+05, 7.94944248e+05, 1.38068713e+04,\n",
              "        1.40155398e+04, 1.37899636e+04, 1.24984366e+14, 1.11342463e+05,\n",
              "        7.05432011e+13, 2.10987899e+04, 1.40328318e+04, 1.26184375e+04,\n",
              "        1.34811260e+04, 1.34954647e+04, 1.34464678e+04, 5.29391991e+13,\n",
              "        1.54592335e+05, 2.52917198e+13, 1.40909524e+04, 1.32262107e+04,\n",
              "        1.39709757e+04, 7.44931866e+04, 7.50053392e+04, 7.46053382e+04,\n",
              "        1.44119861e+14, 1.16697373e+05, 4.58612315e+13, 1.37705597e+04,\n",
              "        1.40064628e+04, 1.38697963e+04, 5.98422820e+05, 5.99522651e+05,\n",
              "        5.98994508e+05, 1.73344041e+13, 3.79673628e+04, 9.59003636e+13,\n",
              "        1.45174476e+04, 1.45000813e+04, 1.45098833e+04, 8.48161168e+05,\n",
              "        8.47939614e+05, 8.47714790e+05, 8.15052752e+13, 1.05750860e+05,\n",
              "        1.58691572e+13, 2.83583007e+05, 2.83829055e+05, 2.83589941e+05,\n",
              "        1.29662560e+04, 1.37164118e+04, 1.24820521e+04, 1.87167586e+04,\n",
              "        1.34299111e+04, 1.51562495e+04, 4.20441360e+05, 2.81516931e+05,\n",
              "        1.20735241e+05, 1.40826418e+04, 1.40906672e+04, 1.43050085e+04,\n",
              "        1.44012817e+04, 1.66950187e+04, 1.98806379e+04, 1.75428223e+04,\n",
              "        2.01499623e+04, 2.30652667e+04, 1.34012527e+04, 1.34301674e+04,\n",
              "        1.33571543e+04, 1.30235108e+04, 1.30403528e+04, 1.32942507e+04,\n",
              "        1.42823831e+04, 1.39153229e+04, 1.41240969e+04, 7.44906631e+04,\n",
              "        7.45361051e+04, 7.46460715e+04, 1.50672549e+04, 1.73766918e+04,\n",
              "        1.77332352e+04, 1.39129740e+04, 1.38253934e+04, 1.38849393e+04,\n",
              "        5.98152157e+05, 5.98452375e+05, 5.98249859e+05, 1.33184698e+04,\n",
              "        1.50766174e+04, 1.60607290e+04, 1.45173306e+04, 1.45146667e+04,\n",
              "        1.45193578e+04, 8.48134155e+05, 8.48053232e+05, 8.47926206e+05,\n",
              "        1.88614791e+04, 1.63946618e+04, 1.16725740e+04, 2.83581048e+05,\n",
              "        2.83621869e+05, 2.83579359e+05, 1.42930161e+04, 1.61634026e+04,\n",
              "        1.42749862e+04, 1.76181991e+04, 3.07303447e+04, 4.90755229e+04,\n",
              "        2.26475208e+05, 2.94331799e+05, 2.39964896e+05, 1.38996777e+04,\n",
              "        1.40307254e+04, 1.39042733e+04, 3.25687686e+04, 2.20003622e+04,\n",
              "        2.18213054e+04, 1.59712109e+04, 1.22281278e+04, 2.72208614e+04,\n",
              "        1.34097676e+04, 1.34565702e+04, 1.34440567e+04, 2.89341710e+04,\n",
              "        4.64114515e+04, 2.25080102e+04, 1.42302239e+04, 1.45491925e+04,\n",
              "        1.39019009e+04, 7.45490759e+04, 7.45604571e+04, 7.44784973e+04,\n",
              "        4.75714274e+04, 1.84153689e+04, 2.10160084e+04, 1.41096468e+04,\n",
              "        1.39890992e+04, 1.39343096e+04, 5.98694963e+05, 5.98634251e+05,\n",
              "        5.98341380e+05, 5.21545720e+04, 2.72222947e+04, 1.75467748e+04,\n",
              "        1.45150135e+04, 1.45197297e+04, 1.45113808e+04, 8.47923073e+05,\n",
              "        8.48207108e+05, 8.48078704e+05, 2.02726102e+04, 3.29649854e+04,\n",
              "        3.45721977e+04, 2.83617249e+05, 2.83593602e+05, 2.83593572e+05,\n",
              "        1.31023724e+04, 1.13875445e+04, 1.43296555e+04, 8.20076695e+05,\n",
              "        2.03229288e+06, 9.77141721e+05, 3.62964301e+05, 2.55238927e+05,\n",
              "        1.65256424e+05, 1.42089426e+04, 1.40526475e+04, 1.35348565e+04,\n",
              "        1.02801851e+06, 9.67142455e+05, 1.43028611e+06, 1.60730687e+04,\n",
              "        1.98074098e+04, 1.21868096e+04, 1.34723541e+04, 1.33986945e+04,\n",
              "        1.34229009e+04, 8.02472171e+05, 2.51725455e+06, 8.73122957e+05,\n",
              "        1.44546863e+04, 1.51226994e+04, 1.22830745e+04, 7.44860365e+04,\n",
              "        7.43539893e+04, 7.46299022e+04, 9.49366938e+05, 3.20333024e+06,\n",
              "        2.77789719e+05, 1.36963803e+04, 1.38770894e+04, 1.40886692e+04,\n",
              "        5.98704057e+05, 5.98990858e+05, 5.98706398e+05, 9.00325540e+05,\n",
              "        7.11405535e+05, 1.16055191e+06, 1.45258654e+04, 1.45246137e+04,\n",
              "        1.45185814e+04, 8.48043571e+05, 8.48227812e+05, 8.48155612e+05,\n",
              "        9.15412799e+05, 1.62267958e+06, 1.47993991e+07, 2.83595081e+05,\n",
              "        2.83598520e+05, 2.83586953e+05, 1.40270632e+04, 1.67026980e+04,\n",
              "        1.30670705e+04, 1.22569120e+25, 9.18413002e+24, 7.97724831e+24,\n",
              "        3.29537343e+05, 4.27859490e+05, 3.00274534e+05, 1.40961563e+04,\n",
              "        1.42859760e+04, 1.37649203e+04, 1.32341707e+25, 2.26585663e+25,\n",
              "        3.20453399e+25, 2.60543959e+04, 2.02997498e+04, 1.78563694e+04,\n",
              "        1.34344636e+04, 1.33571899e+04, 1.34505714e+04, 9.02935875e+24,\n",
              "        1.41925557e+25, 2.04823924e+25, 1.36298212e+04, 1.41697018e+04,\n",
              "        1.49635785e+04, 7.44150686e+04, 7.47586959e+04, 7.44745470e+04,\n",
              "        1.75292752e+25, 9.02681221e+24, 1.84213718e+25, 1.38848846e+04,\n",
              "        1.40134808e+04, 1.38374617e+04, 5.98799414e+05, 5.98510038e+05,\n",
              "        5.98403087e+05, 7.90010106e+24, 1.48700707e+25, 7.74113795e+24,\n",
              "        1.45227581e+04, 1.45149235e+04, 1.45231582e+04, 8.48141735e+05,\n",
              "        8.48098522e+05, 8.47768896e+05, 2.34396253e+25, 1.51903903e+25,\n",
              "        1.20715851e+25, 2.83587686e+05, 2.83589526e+05, 2.83609282e+05,\n",
              "        1.70950054e+04, 1.34311540e+04, 1.63070924e+04, 9.14663638e+26,\n",
              "        1.27251666e+27, 8.87476426e+26, 2.41589779e+05, 3.25934713e+05,\n",
              "        8.53123158e+05, 1.37349564e+04, 1.39563483e+04, 1.40778402e+04,\n",
              "        1.08211679e+27, 1.19813441e+27, 1.53436145e+27, 2.73829436e+04,\n",
              "        1.85372978e+04, 3.47478192e+04, 1.33930341e+04, 1.34101280e+04,\n",
              "        1.34802032e+04, 7.97087658e+26, 1.10668685e+27, 9.60155592e+26,\n",
              "        1.39965101e+04, 1.37213400e+04, 1.41795061e+04, 7.47174388e+04,\n",
              "        7.47469074e+04, 7.46662656e+04, 1.24948055e+27, 2.37691459e+27,\n",
              "        8.67910509e+26, 1.37540949e+04, 1.41079705e+04, 1.39508960e+04,\n",
              "        5.98415194e+05, 5.98831205e+05, 5.98292836e+05, 1.65728517e+27,\n",
              "        1.36293349e+27, 1.66515184e+27, 1.45200650e+04, 1.45221854e+04,\n",
              "        1.45241661e+04, 8.48099783e+05, 8.48005458e+05, 8.48016199e+05,\n",
              "        2.19044890e+27, 1.19734149e+27, 7.94722112e+26, 2.83592247e+05,\n",
              "        2.83617549e+05, 2.83595290e+05, 9.58458870e+03, 1.19387545e+04,\n",
              "        1.45346153e+04, 9.54561467e+28, 1.84587839e+29, 1.97487842e+29,\n",
              "        1.58128613e+05, 2.99630319e+05, 1.61880265e+05, 1.40091612e+04,\n",
              "        1.40823861e+04, 1.35486352e+04, 1.57660591e+29, 3.19410469e+28,\n",
              "        7.22999739e+28, 7.44332363e+03, 1.10107336e+04, 1.72745009e+04,\n",
              "        1.34023122e+04, 1.34056612e+04, 1.34730764e+04, 1.37252012e+29,\n",
              "        1.40932956e+29, 8.06908913e+28, 1.44418570e+04, 1.34163780e+04,\n",
              "        1.37522140e+04, 7.44916402e+04, 7.44479472e+04, 7.44036226e+04,\n",
              "        1.17237138e+29, 1.71546508e+29, 1.77695843e+29, 1.37318555e+04,\n",
              "        1.38832492e+04, 1.38842056e+04, 5.99186599e+05, 5.98126110e+05,\n",
              "        5.99142789e+05, 1.31872379e+29, 1.31665122e+29, 9.15959978e+28,\n",
              "        1.45145833e+04, 1.45215743e+04, 1.45167066e+04, 8.47794129e+05,\n",
              "        8.48120890e+05, 8.47809868e+05, 7.14120108e+28, 1.34005386e+29,\n",
              "        3.20740639e+29, 2.83626336e+05, 2.83588164e+05, 2.83587118e+05,\n",
              "        1.53712509e+04, 1.63644791e+04, 1.12737256e+04, 9.49639059e+30,\n",
              "        6.69553744e+30, 8.00915226e+30, 3.20590488e+05, 1.99107424e+05,\n",
              "        1.64824969e+05, 1.42959591e+04, 1.36759004e+04, 1.39476769e+04,\n",
              "        8.75904166e+30, 5.89686370e+30, 5.42692731e+30, 8.76387326e+03,\n",
              "        3.16371408e+04, 2.36672959e+04, 1.34527051e+04, 1.33984714e+04,\n",
              "        1.34571751e+04, 4.57971342e+30, 3.94727556e+30, 5.87400706e+30,\n",
              "        1.46384424e+04, 1.40659136e+04, 1.40304542e+04, 7.43716638e+04,\n",
              "        7.45352237e+04, 7.47085318e+04, 4.18908682e+30, 3.19134024e+30,\n",
              "        4.91509111e+30, 1.39706782e+04, 1.38450169e+04, 1.38235720e+04,\n",
              "        5.99192851e+05, 5.98674154e+05, 5.98348663e+05, 6.70986725e+30,\n",
              "        9.77532565e+30, 2.36575756e+30, 1.45208490e+04, 1.45169431e+04,\n",
              "        1.45253399e+04, 8.47972883e+05, 8.48135422e+05, 8.48269091e+05,\n",
              "        7.74874761e+30, 6.32855455e+30, 4.02400605e+30, 2.83610171e+05,\n",
              "        2.83598460e+05, 2.83592770e+05]),\n",
              " 'rank_test_score': array([ 83, 223,  81, 407, 388, 410, 224, 250, 235,   9, 220,  49, 404,\n",
              "        386, 405, 162, 225, 172, 125, 222, 146, 411, 390, 414,  66, 221,\n",
              "         67, 277, 302, 284, 408, 387, 413,   1, 219,   7, 355, 357, 351,\n",
              "        409, 389, 406, 184, 228, 194, 375, 384, 378, 403, 385, 412, 304,\n",
              "        330, 326, 112, 120,  85, 398, 230, 401, 232, 247, 254,  25,  80,\n",
              "         36, 396, 226, 397, 148, 169, 164, 137, 152, 133, 392, 231, 391,\n",
              "         64,  86,  71, 285, 301, 291, 400, 227, 395,  28,  79,  14, 346,\n",
              "        356, 347, 394, 229, 399, 188, 217, 186, 370, 366, 379, 402, 233,\n",
              "        393, 306, 329, 325, 101, 119, 116, 100, 109,  96, 238, 253, 234,\n",
              "         31,  37,  39, 121, 118, 108, 155, 154, 161, 139, 147, 129,  98,\n",
              "        110, 111,  55,  68,  63, 297, 300, 295, 102,  95, 115,  26,  38,\n",
              "         41, 340, 344, 333, 113,  89,  94, 192, 208, 193, 367, 372, 376,\n",
              "        122,  93, 106, 321, 328, 323,  77,  99, 103, 175, 183, 178, 236,\n",
              "        246, 239,  46,  21,  42, 180, 177, 213, 166, 165, 168, 127, 142,\n",
              "        145, 181, 218, 179,  73,  58,  62, 280, 290, 293, 209, 212, 182,\n",
              "          8,  22,  12, 353, 339, 341, 210, 215, 214, 189, 207, 199, 360,\n",
              "        377, 361, 176, 216, 211, 322, 327, 318, 107, 117,  76, 259, 270,\n",
              "        263, 255, 251, 249,  45,  35,  44, 267, 266, 274, 150, 163, 159,\n",
              "        141, 123, 130, 264, 275, 265,  57,  69,  75, 279, 292, 287, 271,\n",
              "        273, 268,  32,   5,  24, 342, 331, 352, 260, 261, 269, 200, 205,\n",
              "        203, 374, 365, 368, 262, 272, 303, 310, 309, 305,  97, 104,  90,\n",
              "        422, 415, 417, 237, 258, 252,  10,  47,  23, 418, 432, 431, 170,\n",
              "        174, 153, 124, 132, 138, 421, 420, 429,  59,  52,  72, 283, 296,\n",
              "        288, 426, 419, 423,  20,  15,  34, 332, 337, 334, 416, 427, 425,\n",
              "        196, 191, 197, 363, 362, 373, 430, 428, 424, 307, 313, 319, 114,\n",
              "        105,  84, 433, 446, 450, 244, 245, 257,  48,   6,  50, 437, 448,\n",
              "        438, 171, 158, 167, 135, 140, 128, 447, 434, 442,  51,  74,  54,\n",
              "        294, 299, 286, 439, 445, 441,  40,  17,  13, 350, 348, 354, 444,\n",
              "        435, 443, 195, 204, 202, 380, 371, 383, 440, 449, 436, 308, 314,\n",
              "        317,  92,  88,  91, 452, 462, 465, 243, 240, 241,  19,   3,  43,\n",
              "        453, 464, 461, 157, 156, 151, 143, 131, 134, 458, 463, 451,  65,\n",
              "         60,  70, 282, 278, 281, 457, 468, 459,  30,  16,   4, 345, 338,\n",
              "        343, 455, 460, 456, 206, 201, 185, 364, 382, 381, 454, 466, 467,\n",
              "        316, 312, 324,  82,  78,  87, 481, 484, 472, 248, 242, 256,   2,\n",
              "         33,  18, 471, 485, 475, 149, 160, 173, 126, 136, 144, 482, 469,\n",
              "        478,  53,  56,  61, 289, 276, 298, 486, 476, 480,  11,  27,  29,\n",
              "        336, 335, 349, 477, 483, 474, 190, 198, 187, 359, 369, 358, 479,\n",
              "        470, 473, 311, 315, 320], dtype=int32)}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0248f430",
      "metadata": {
        "id": "0248f430"
      },
      "outputs": [],
      "source": [
        "num_of_each_param = (9, 6, 3, 3)\n",
        "grid_score.cv_results_['mean_test_score'] = grid_score.cv_results_['mean_test_score'].reshape(num_of_each_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb58c75f",
      "metadata": {
        "id": "cb58c75f"
      },
      "outputs": [],
      "source": [
        "a = grid_score.cv_results_['mean_test_score']\n",
        "best_solution = list(np.unravel_index(a.argmin(), a.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9caa693d",
      "metadata": {
        "id": "9caa693d"
      },
      "source": [
        "###### Видим, что выиграла комбинация с $l_1$ регуляризацией и с константным learning_rate = $10^{-5}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193f3177",
      "metadata": {
        "id": "193f3177",
        "outputId": "6c2e80d0-5cd4-440c-8e9b-19d0db944d11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "815.4665659226943"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_regressor = SGDRegressor(eta0=1e-5, learning_rate='constant', penalty='l1', alpha=0.1,\n",
        "                              fit_intercept=True, max_iter=1000, shuffle=True)\n",
        "\n",
        "best_regressor.fit(x_train, y_train)\n",
        "y_pred = best_regressor.predict(x_test)\n",
        "mean_squared_error(y_pred, y_test) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9e847a",
      "metadata": {
        "id": "cc9e847a"
      },
      "source": [
        "###### Построим графики зависимости score от каждого из гиперпараметров, фиксируя остальные гиперпараметрами теми, которые состоят в нашей \"лучшей комбинации\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba189ef",
      "metadata": {
        "id": "eba189ef",
        "outputId": "64627e0f-42e3-48ec-8673-446aa27dddf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eta0': [0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07],\n",
              " 'penalty': ['l1', 'l2', None],\n",
              " 'alpha': [0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09],\n",
              " 'learning_rate': ['invscaling', 'optimal', 'constant']}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29cd3b8",
      "metadata": {
        "id": "b29cd3b8",
        "outputId": "4456f13f-fe8c-48a8-d064-cd4caf73b72a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1152x360 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1152x360 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1152x360 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1152x360 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "param_ind = 0\n",
        "\n",
        "fig, axes = plt.subplots(1, len(params), sharey=True, figsize=(15,4))\n",
        "\n",
        "for param_name in ['alpha', 'eta0', 'learning_rate', 'penalty']:\n",
        "    param_values = params[param_name]\n",
        "    scores = []\n",
        "    if param_name == 'penalty':\n",
        "        param_values[2] = 'No penalty'\n",
        "    for ind, value in enumerate(param_values):\n",
        "        solution_ind = deepcopy(best_solution)\n",
        "        solution_ind[param_ind] = ind\n",
        "        scores.append(grid_score.cv_results_['mean_test_score'][solution_ind[0]][solution_ind[1]][solution_ind[2]][solution_ind[3]])\n",
        "        \n",
        "    params_plot = axes[param_ind]\n",
        "    params_plot.plot(param_values, scores, marker='o')\n",
        "    params_plot.grid(True)\n",
        "    params_plot.set_xlabel(param_name)    \n",
        "    params_plot.set_ylabel('RMSE')\n",
        "    plt.figure(figsize=(16, 5))\n",
        "        \n",
        "    param_ind += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8180c9e7",
      "metadata": {
        "id": "8180c9e7"
      },
      "source": [
        "Как мы видим, 'optimal' в гиперпараметре learning rate творит страшные вещи и заставляет модель расходиться. Построим графики без learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef989fd7",
      "metadata": {
        "id": "ef989fd7",
        "outputId": "d157abbb-a13d-4ae8-f319-c87fa80d3684"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "param_ind = 0\n",
        "\n",
        "fig, axes = plt.subplots(1, len(params)-1, sharey=True, figsize=(15,4))\n",
        "\n",
        "for param_name in ['alpha', 'eta0', 'penalty']:\n",
        "    if param_ind == 2:\n",
        "        param_ind += 1\n",
        "    param_values = params[param_name]\n",
        "    scores = []\n",
        "    if param_name == 'penalty':\n",
        "        param_values[2] = 'No penalty'\n",
        "    for ind, value in enumerate(param_values):\n",
        "        solution_ind = deepcopy(best_solution)\n",
        "        solution_ind[param_ind] = ind\n",
        "        scores.append(grid_score.cv_results_['mean_test_score'][solution_ind[0]][solution_ind[1]][solution_ind[2]][solution_ind[3]])\n",
        "        \n",
        "    if param_ind == 3:\n",
        "        param_ind -= 1\n",
        "    params_plot = axes[param_ind]\n",
        "    params_plot.plot(list(map(str, list(param_values))), scores, marker='o')\n",
        "    params_plot.grid(True)\n",
        "    params_plot.set_xlabel(param_name)    \n",
        "    params_plot.set_ylabel('RMSE')\n",
        "        \n",
        "    param_ind += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6040d780-d5c1-4ee8-965e-ca16fd364814",
      "metadata": {
        "id": "6040d780-d5c1-4ee8-965e-ca16fd364814"
      },
      "source": [
        "**Задание 3** (3 балла)\n",
        "\n",
        "Попробуйте обогатить `X` данными из остальных таблиц, так что бы повысить качество модели `SGDRegressor`.\n",
        "\n",
        "Проведите ряд экспериментов, целью которых будет выявить оптимальный набор данных для решения данной задачи. \n",
        "\n",
        "Все эксперименты должны быть подкреплены корректными и понятными графиками.\n",
        "\n",
        "Опишите вашу стратегию добавления признаков и оценки их значимости, а также ответьте на вопросы:\n",
        "\n",
        "1. Удалось ли повысить качество предсказания за счёт добавления новых признаков?\n",
        "2. Есть ли признаки, добавление которых, ухудшило качество модели? Как вы думаете, почему так произошло?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a532431",
      "metadata": {
        "id": "0a532431"
      },
      "outputs": [],
      "source": [
        "cols_to_one_hot = [f'player_{i}' for i in range(0, 10)]\n",
        "items_columns = [f'item_{i}' for i in range(0, 121)]\n",
        "heroes_columns = [f'heroes_{i}' for i in range(0, 111)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ecfa227",
      "metadata": {
        "id": "7ecfa227"
      },
      "source": [
        "###### Чтобы предсказывать количество золота на 600 секунде -- берём значения deny на 540 секунде"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3207b90b",
      "metadata": {
        "id": "3207b90b",
        "outputId": "e03b0a1f-b059-4ebd-c4cd-94ab26495fc6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>radiant_deny</th>\n",
              "      <th>dire_deny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>3</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814648</th>\n",
              "      <td>81464</td>\n",
              "      <td>15</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814658</th>\n",
              "      <td>81465</td>\n",
              "      <td>41</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814668</th>\n",
              "      <td>81466</td>\n",
              "      <td>24</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814678</th>\n",
              "      <td>81467</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814688</th>\n",
              "      <td>81468</td>\n",
              "      <td>14</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81469 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          mid  radiant_deny  dire_deny\n",
              "8           0            -5         -5\n",
              "18          1            -5         -5\n",
              "28          2            -5         -5\n",
              "38          3            -5         -5\n",
              "48          4            -5         -5\n",
              "...       ...           ...        ...\n",
              "814648  81464            15         72\n",
              "814658  81465            41         23\n",
              "814668  81466            24         36\n",
              "814678  81467             9         12\n",
              "814688  81468            14         43\n",
              "\n",
              "[81469 rows x 3 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j1 = pd.read_csv('deny.csv')\n",
        "j1 = j1[(j1['time'] == 540)].drop(['time'], axis=1)\n",
        "j1['radiant_deny'] = j1[[f'player_{i}' for i in range(0, 5)]].sum(axis=1)\n",
        "j1['dire_deny'] = j1[[f'player_{i}' for i in range(5, 10)]].sum(axis=1)\n",
        "deny = j1.drop(cols_to_one_hot, axis=1)\n",
        "deny"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22475b06",
      "metadata": {
        "id": "22475b06"
      },
      "source": [
        "###### Берём столбец events, обрезаем значения на 600 секунде. Все отальные ивенты суммируем для каждой игры, причём каждый ивент берём со знаком +, если были произведены командой radiant, иначе со знаком -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3426df3d",
      "metadata": {
        "id": "3426df3d",
        "outputId": "cbe61897-5099-4fc0-a0f9-82db759db27a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49050</th>\n",
              "      <td>49943</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49051</th>\n",
              "      <td>49944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49052</th>\n",
              "      <td>49945</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49053</th>\n",
              "      <td>49946</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49054</th>\n",
              "      <td>49947</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49055 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mid  0  1  2  3  4  5  6\n",
              "0          0  0  0  0  1  0  0  0\n",
              "1          1  0  0  0  1  0  0  0\n",
              "2          2  0  0  0 -1  0  0  0\n",
              "3          3  0  0  0  1  0  0  0\n",
              "4          4  0  0  0 -1  0  0  0\n",
              "...      ... .. .. .. .. .. .. ..\n",
              "49050  49943  0  0  0  1  0  0 -1\n",
              "49051  49944  0  0  0 -1  0  0  0\n",
              "49052  49945  0  0  0 -1  0  0 -1\n",
              "49053  49946  0  0  0 -1  0  0 -1\n",
              "49054  49947  0  0  0  1  0  0  0\n",
              "\n",
              "[49055 rows x 8 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j2 = pd.read_csv('events.csv')\n",
        "j2 = j2[j2['time'] != 600]\n",
        "j2 = j2.join(pd.get_dummies(j2['event_type'])).drop(['event_type', 'time'], axis=1)\n",
        "j2 = j2.groupby(by=['mid', 'from_team'], as_index=False).sum()\n",
        "j2.loc[j2['from_team'] == 'dire', [0, 1, 2, 3, 4, 5, 6]] = j2.loc[j2['from_team'] == 'dire', [0, 1, 2, 3, 4, 5, 6]].apply(lambda x: x * (-1)).values\n",
        "j2 = j2.groupby(by='mid').sum()\n",
        "events = j2.reset_index()\n",
        "events"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "106e8be0",
      "metadata": {
        "id": "106e8be0"
      },
      "source": [
        "###### Пик героев -- очень важная фича. Делаем one-hot encoding, естественно, разбиваем по командам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0297fd8a",
      "metadata": {
        "scrolled": false,
        "id": "0297fd8a",
        "outputId": "ef09ea08-4cd3-4e20-ec8b-5ed1331ad77f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>radiant_heroes_0</th>\n",
              "      <th>dire_heroes_0</th>\n",
              "      <th>radiant_heroes_1</th>\n",
              "      <th>dire_heroes_1</th>\n",
              "      <th>radiant_heroes_2</th>\n",
              "      <th>dire_heroes_2</th>\n",
              "      <th>radiant_heroes_3</th>\n",
              "      <th>dire_heroes_3</th>\n",
              "      <th>radiant_heroes_4</th>\n",
              "      <th>...</th>\n",
              "      <th>radiant_heroes_106</th>\n",
              "      <th>dire_heroes_106</th>\n",
              "      <th>radiant_heroes_107</th>\n",
              "      <th>dire_heroes_107</th>\n",
              "      <th>radiant_heroes_108</th>\n",
              "      <th>dire_heroes_108</th>\n",
              "      <th>radiant_heroes_109</th>\n",
              "      <th>dire_heroes_109</th>\n",
              "      <th>radiant_heroes_110</th>\n",
              "      <th>dire_heroes_110</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>49943</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>49944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>49945</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>49946</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>49947</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 223 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mid  radiant_heroes_0  dire_heroes_0  radiant_heroes_1  \\\n",
              "0          0                 0              0                 0   \n",
              "1          1                 0              0                 0   \n",
              "2          2                 0              0                 0   \n",
              "3          3                 0              0                 0   \n",
              "4          4                 0              0                 0   \n",
              "...      ...               ...            ...               ...   \n",
              "49943  49943                 0              0                 0   \n",
              "49944  49944                 0              0                 0   \n",
              "49945  49945                 0              0                 0   \n",
              "49946  49946                 0              0                 0   \n",
              "49947  49947                 0              0                 0   \n",
              "\n",
              "       dire_heroes_1  radiant_heroes_2  dire_heroes_2  radiant_heroes_3  \\\n",
              "0                  0                 0              0                 0   \n",
              "1                  0                 0              0                 0   \n",
              "2                  0                 0              0                 0   \n",
              "3                  0                 0              0                 0   \n",
              "4                  0                 0              0                 0   \n",
              "...              ...               ...            ...               ...   \n",
              "49943              0                 0              0                 1   \n",
              "49944              0                 0              0                 0   \n",
              "49945              0                 1              0                 0   \n",
              "49946              0                 0              0                 0   \n",
              "49947              0                 0              0                 0   \n",
              "\n",
              "       dire_heroes_3  radiant_heroes_4  ...  radiant_heroes_106  \\\n",
              "0                  0                 0  ...                   0   \n",
              "1                  0                 0  ...                   0   \n",
              "2                  0                 0  ...                   0   \n",
              "3                  0                 0  ...                   0   \n",
              "4                  1                 0  ...                   0   \n",
              "...              ...               ...  ...                 ...   \n",
              "49943              0                 0  ...                   0   \n",
              "49944              0                 0  ...                   0   \n",
              "49945              0                 0  ...                   0   \n",
              "49946              0                 0  ...                   0   \n",
              "49947              0                 0  ...                   0   \n",
              "\n",
              "       dire_heroes_106  radiant_heroes_107  dire_heroes_107  \\\n",
              "0                    0                   0                0   \n",
              "1                    0                   0                0   \n",
              "2                    0                   0                0   \n",
              "3                    0                   0                0   \n",
              "4                    0                   0                0   \n",
              "...                ...                 ...              ...   \n",
              "49943                0                   0                0   \n",
              "49944                0                   0                0   \n",
              "49945                0                   0                0   \n",
              "49946                0                   0                0   \n",
              "49947                0                   0                0   \n",
              "\n",
              "       radiant_heroes_108  dire_heroes_108  radiant_heroes_109  \\\n",
              "0                       0                0                   0   \n",
              "1                       0                0                   0   \n",
              "2                       0                0                   0   \n",
              "3                       0                0                   0   \n",
              "4                       0                0                   0   \n",
              "...                   ...              ...                 ...   \n",
              "49943                   0                0                   0   \n",
              "49944                   0                0                   0   \n",
              "49945                   0                0                   1   \n",
              "49946                   0                0                   0   \n",
              "49947                   0                0                   0   \n",
              "\n",
              "       dire_heroes_109  radiant_heroes_110  dire_heroes_110  \n",
              "0                    0                   0                0  \n",
              "1                    0                   0                0  \n",
              "2                    0                   0                0  \n",
              "3                    0                   0                0  \n",
              "4                    0                   0                0  \n",
              "...                ...                 ...              ...  \n",
              "49943                0                   0                0  \n",
              "49944                0                   0                0  \n",
              "49945                0                   0                0  \n",
              "49946                0                   0                0  \n",
              "49947                0                   0                0  \n",
              "\n",
              "[49948 rows x 223 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j4 = pd.read_csv('heroes.csv')\n",
        "for ind in range(0, 111):\n",
        "    j4[f'radiant_heroes_{ind}'] = j4[(j4[cols_to_one_hot[:5]] == ind)].count(axis=1)\n",
        "    j4[f'dire_heroes_{ind}'] = j4[(j4[cols_to_one_hot[5:]] == ind)].count(axis=1)\n",
        "heroes = j4.drop(cols_to_one_hot, axis=1)\n",
        "heroes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a4f73e",
      "metadata": {
        "id": "44a4f73e"
      },
      "source": [
        "###### Items предобрабатываем так же, как и events: radiant суммируем с плюсом, dire -- с минусом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba893b2a",
      "metadata": {
        "id": "ba893b2a",
        "outputId": "8a92a6b8-d34c-403a-cb44-22cebe7541f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>item_0</th>\n",
              "      <th>item_1</th>\n",
              "      <th>item_2</th>\n",
              "      <th>item_3</th>\n",
              "      <th>item_4</th>\n",
              "      <th>item_5</th>\n",
              "      <th>item_6</th>\n",
              "      <th>item_7</th>\n",
              "      <th>item_8</th>\n",
              "      <th>...</th>\n",
              "      <th>item_111</th>\n",
              "      <th>item_112</th>\n",
              "      <th>item_113</th>\n",
              "      <th>item_114</th>\n",
              "      <th>item_115</th>\n",
              "      <th>item_116</th>\n",
              "      <th>item_117</th>\n",
              "      <th>item_118</th>\n",
              "      <th>item_119</th>\n",
              "      <th>item_120</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>49943</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>49944</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>49945</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>49946</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>49947</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 122 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mid  item_0  item_1  item_2  item_3  item_4  item_5  item_6  item_7  \\\n",
              "0          0     0.0    -2.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
              "1          1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "2          2     0.0     2.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "3          3     0.0    -2.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "4          4     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "49943  49943     0.0     2.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "49944  49944     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "49945  49945     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "49946  49946     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "49947  49947     0.0    -2.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "\n",
              "       item_8  ...  item_111  item_112  item_113  item_114  item_115  \\\n",
              "0         0.0  ...       0.0       0.0       0.0       0.0      -1.0   \n",
              "1         0.0  ...       0.0      -1.0       0.0       0.0      -2.0   \n",
              "2         0.0  ...       0.0       0.0      -1.0       0.0       0.0   \n",
              "3         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "4         0.0  ...       0.0       1.0      -1.0       0.0      -2.0   \n",
              "...       ...  ...       ...       ...       ...       ...       ...   \n",
              "49943     0.0  ...       0.0       0.0       0.0       0.0      -3.0   \n",
              "49944     0.0  ...       0.0      -1.0       1.0       0.0       2.0   \n",
              "49945     0.0  ...       0.0       0.0       0.0       0.0      -1.0   \n",
              "49946     0.0  ...       0.0       0.0      -2.0       0.0       1.0   \n",
              "49947     0.0  ...       0.0      -1.0       1.0       0.0      -1.0   \n",
              "\n",
              "       item_116  item_117  item_118  item_119  item_120  \n",
              "0           0.0       0.0       0.0       0.0       0.0  \n",
              "1           0.0       0.0       0.0       0.0       0.0  \n",
              "2           0.0       0.0       0.0       0.0       0.0  \n",
              "3           1.0       0.0       0.0       0.0       0.0  \n",
              "4           0.0       0.0       0.0       0.0       0.0  \n",
              "...         ...       ...       ...       ...       ...  \n",
              "49943       3.0       0.0       0.0       0.0       0.0  \n",
              "49944       3.0       0.0       0.0       0.0       0.0  \n",
              "49945       0.0       0.0       0.0       0.0       0.0  \n",
              "49946       0.0       0.0       0.0       0.0       0.0  \n",
              "49947      -3.0       0.0       0.0       0.0       0.0  \n",
              "\n",
              "[49948 rows x 122 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j5 = pd.read_csv('items.csv')\n",
        "j5.loc[(j5['player'] == 5) | (j5['player'] == 6) |\n",
        "   (j5['player'] == 7) | (j5['player'] == 8) | (j5['player'] == 9), items_columns] = j5[items_columns] * (-1)\n",
        "items = j5.drop('player', axis=1).groupby(by='mid').sum().reset_index()\n",
        "items"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e90e1019",
      "metadata": {
        "id": "e90e1019"
      },
      "source": [
        "###### Выкидываем данные с 600-й секунды, в остальном просто суммируем количество убийств для каждой из команд"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16c4a29",
      "metadata": {
        "id": "d16c4a29",
        "outputId": "fa065687-fdda-470f-a925-6f7838cf7db3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>radiant_killed</th>\n",
              "      <th>dire_killed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8004</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12048</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12920</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15026</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16204</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55997</th>\n",
              "      <td>81464</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55998</th>\n",
              "      <td>81465</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55999</th>\n",
              "      <td>81466</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56000</th>\n",
              "      <td>81467</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56001</th>\n",
              "      <td>81468</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56002 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mid  radiant_killed  dire_killed\n",
              "0       8004               7            1\n",
              "1      12048               3            5\n",
              "2      12920               5            1\n",
              "3      15026               2           11\n",
              "4      16204               6           10\n",
              "...      ...             ...          ...\n",
              "55997  81464               4            8\n",
              "55998  81465               7            0\n",
              "55999  81466               6           14\n",
              "56000  81467              13            5\n",
              "56001  81468               2            4\n",
              "\n",
              "[56002 rows x 3 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j6 = pd.read_csv('kills.csv')\n",
        "j6 = j6[j6['time'] != 600]\n",
        "j6['radiant_killed'] = ((j6['killer'] == 'player_0') | (j6['killer'] == 'player_1') | (j6['killer'] == 'player_2') | \n",
        "                        (j6['killer'] == 'player_3') | (j6['killer'] == 'player_4')).astype(int)\n",
        "j6['dire_killed'] = ((j6['killer'] == 'player_5') | (j6['killer'] == 'player_6') | (j6['killer'] == 'player_7') | \n",
        "                        (j6['killer'] == 'player_8') | (j6['killer'] == 'player_9')).astype(int)\n",
        "j6 = j6.groupby(by='mid').sum().reset_index()\n",
        "kills = j6.drop(['time', 'victim_id'], axis=1)\n",
        "kills"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee47dd6e",
      "metadata": {
        "id": "ee47dd6e"
      },
      "source": [
        "###### Для данных lh и xp мы просто смотрим их количество на 540 секунде, суммируем для обеих команд"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4efd56f",
      "metadata": {
        "id": "d4efd56f",
        "outputId": "4a93063a-c908-45ee-ccee-90958312a786"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>radiant_lh</th>\n",
              "      <th>dire_lh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>126</td>\n",
              "      <td>167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>148</td>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2</td>\n",
              "      <td>154</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>3</td>\n",
              "      <td>116</td>\n",
              "      <td>137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>134</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499438</th>\n",
              "      <td>49943</td>\n",
              "      <td>129</td>\n",
              "      <td>122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499448</th>\n",
              "      <td>49944</td>\n",
              "      <td>115</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499458</th>\n",
              "      <td>49945</td>\n",
              "      <td>110</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499468</th>\n",
              "      <td>49946</td>\n",
              "      <td>86</td>\n",
              "      <td>167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499478</th>\n",
              "      <td>49947</td>\n",
              "      <td>156</td>\n",
              "      <td>144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          mid  radiant_lh  dire_lh\n",
              "8           0         126      167\n",
              "18          1         148      134\n",
              "28          2         154       91\n",
              "38          3         116      137\n",
              "48          4         134      100\n",
              "...       ...         ...      ...\n",
              "499438  49943         129      122\n",
              "499448  49944         115      115\n",
              "499458  49945         110      113\n",
              "499468  49946          86      167\n",
              "499478  49947         156      144\n",
              "\n",
              "[49948 rows x 3 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j7 = pd.read_csv('lh.csv')\n",
        "j7 = j7[(j7['times'] == 540)].drop(['times'], axis=1)\n",
        "j7['radiant_lh'] = j7[[f'player_{i}' for i in range(0, 5)]].sum(axis=1)\n",
        "j7['dire_lh'] = j7[[f'player_{i}' for i in range(5, 10)]].sum(axis=1)\n",
        "lh = j7.drop(cols_to_one_hot, axis=1)\n",
        "lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833d32fc",
      "metadata": {
        "scrolled": false,
        "id": "833d32fc",
        "outputId": "7c3a3920-1b92-4e5e-a941-2d1edd691f48"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>radiant_xp</th>\n",
              "      <th>dire_xp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>14245</td>\n",
              "      <td>13289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>13549</td>\n",
              "      <td>14906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2</td>\n",
              "      <td>15648</td>\n",
              "      <td>11467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>3</td>\n",
              "      <td>12892</td>\n",
              "      <td>13211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>13507</td>\n",
              "      <td>12634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499438</th>\n",
              "      <td>49943</td>\n",
              "      <td>13510</td>\n",
              "      <td>15041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499448</th>\n",
              "      <td>49944</td>\n",
              "      <td>13348</td>\n",
              "      <td>15213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499458</th>\n",
              "      <td>49945</td>\n",
              "      <td>12470</td>\n",
              "      <td>11471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499468</th>\n",
              "      <td>49946</td>\n",
              "      <td>11723</td>\n",
              "      <td>16033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499478</th>\n",
              "      <td>49947</td>\n",
              "      <td>14728</td>\n",
              "      <td>14685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          mid  radiant_xp  dire_xp\n",
              "8           0       14245    13289\n",
              "18          1       13549    14906\n",
              "28          2       15648    11467\n",
              "38          3       12892    13211\n",
              "48          4       13507    12634\n",
              "...       ...         ...      ...\n",
              "499438  49943       13510    15041\n",
              "499448  49944       13348    15213\n",
              "499458  49945       12470    11471\n",
              "499468  49946       11723    16033\n",
              "499478  49947       14728    14685\n",
              "\n",
              "[49948 rows x 3 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "j8 = pd.read_csv('xp.csv')\n",
        "j8 = j8[j8['times'] == 540].drop('times', axis=1)\n",
        "j8['radiant_xp'] = j8[[f'player_{i}' for i in range(0, 5)]].sum(axis=1)\n",
        "j8['dire_xp'] = j8[[f'player_{i}' for i in range(5, 10)]].sum(axis=1)\n",
        "xp = j8.drop(cols_to_one_hot, axis=1)\n",
        "xp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb48d49",
      "metadata": {
        "id": "cdb48d49"
      },
      "source": [
        "###### Таким образом, мы сделали preprocessing для всех столбцов: deny, events, heroes, items, kills, lh, xp\n",
        "\n",
        "Посмотрим теперь, улучшится ли предсказание нашей линейной регрессии, если мы обучим её на всех данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f7f50f",
      "metadata": {
        "id": "67f7f50f",
        "outputId": "56e4210e-d96c-4eab-e52e-6bed43537309"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>player_0_60</th>\n",
              "      <th>player_0_120</th>\n",
              "      <th>player_0_180</th>\n",
              "      <th>player_0_240</th>\n",
              "      <th>player_0_300</th>\n",
              "      <th>player_0_360</th>\n",
              "      <th>player_0_420</th>\n",
              "      <th>player_0_480</th>\n",
              "      <th>player_0_540</th>\n",
              "      <th>...</th>\n",
              "      <th>item_117</th>\n",
              "      <th>item_118</th>\n",
              "      <th>item_119</th>\n",
              "      <th>item_120</th>\n",
              "      <th>radiant_killed</th>\n",
              "      <th>dire_killed</th>\n",
              "      <th>radiant_lh</th>\n",
              "      <th>dire_lh</th>\n",
              "      <th>radiant_xp</th>\n",
              "      <th>dire_xp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>957.0</td>\n",
              "      <td>1161.0</td>\n",
              "      <td>1571.0</td>\n",
              "      <td>1721.0</td>\n",
              "      <td>1871.0</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>2850.0</td>\n",
              "      <td>3303.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>126.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>14245.0</td>\n",
              "      <td>13289.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>285.0</td>\n",
              "      <td>435.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>1334.0</td>\n",
              "      <td>1667.0</td>\n",
              "      <td>1818.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>13549.0</td>\n",
              "      <td>14906.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>288.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>1224.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>2611.0</td>\n",
              "      <td>2879.0</td>\n",
              "      <td>3069.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>154.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15648.0</td>\n",
              "      <td>11467.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>288.0</td>\n",
              "      <td>438.0</td>\n",
              "      <td>1230.0</td>\n",
              "      <td>1381.0</td>\n",
              "      <td>1916.0</td>\n",
              "      <td>2436.0</td>\n",
              "      <td>2585.0</td>\n",
              "      <td>2735.0</td>\n",
              "      <td>2886.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>116.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>12892.0</td>\n",
              "      <td>13211.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>348.0</td>\n",
              "      <td>572.0</td>\n",
              "      <td>745.0</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>1590.0</td>\n",
              "      <td>1787.0</td>\n",
              "      <td>2070.0</td>\n",
              "      <td>2520.0</td>\n",
              "      <td>2948.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>134.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>13507.0</td>\n",
              "      <td>12634.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81464</th>\n",
              "      <td>81464</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81465</th>\n",
              "      <td>81465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81466</th>\n",
              "      <td>81466</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81467</th>\n",
              "      <td>81467</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81468</th>\n",
              "      <td>81468</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81469 rows × 454 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mid  player_0_60  player_0_120  player_0_180  player_0_240  \\\n",
              "0          0        750.0         957.0        1161.0        1571.0   \n",
              "1          1        285.0         435.0         585.0         736.0   \n",
              "2          2        288.0         756.0        1224.0        1617.0   \n",
              "3          3        288.0         438.0        1230.0        1381.0   \n",
              "4          4        348.0         572.0         745.0        1170.0   \n",
              "...      ...          ...           ...           ...           ...   \n",
              "81464  81464          NaN           NaN           NaN           NaN   \n",
              "81465  81465          NaN           NaN           NaN           NaN   \n",
              "81466  81466          NaN           NaN           NaN           NaN   \n",
              "81467  81467          NaN           NaN           NaN           NaN   \n",
              "81468  81468          NaN           NaN           NaN           NaN   \n",
              "\n",
              "       player_0_300  player_0_360  player_0_420  player_0_480  player_0_540  \\\n",
              "0            1721.0        1871.0        2022.0        2850.0        3303.0   \n",
              "1            1334.0        1667.0        1818.0        2016.0        2328.0   \n",
              "2            1920.0        2328.0        2611.0        2879.0        3069.0   \n",
              "3            1916.0        2436.0        2585.0        2735.0        2886.0   \n",
              "4            1590.0        1787.0        2070.0        2520.0        2948.0   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "81464           NaN           NaN           NaN           NaN           NaN   \n",
              "81465           NaN           NaN           NaN           NaN           NaN   \n",
              "81466           NaN           NaN           NaN           NaN           NaN   \n",
              "81467           NaN           NaN           NaN           NaN           NaN   \n",
              "81468           NaN           NaN           NaN           NaN           NaN   \n",
              "\n",
              "       ...  item_117  item_118  item_119  item_120  radiant_killed  \\\n",
              "0      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "1      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "2      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "3      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "4      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "...    ...       ...       ...       ...       ...             ...   \n",
              "81464  ...       NaN       NaN       NaN       NaN             4.0   \n",
              "81465  ...       NaN       NaN       NaN       NaN             7.0   \n",
              "81466  ...       NaN       NaN       NaN       NaN             6.0   \n",
              "81467  ...       NaN       NaN       NaN       NaN            13.0   \n",
              "81468  ...       NaN       NaN       NaN       NaN             2.0   \n",
              "\n",
              "       dire_killed  radiant_lh  dire_lh  radiant_xp  dire_xp  \n",
              "0              NaN       126.0    167.0     14245.0  13289.0  \n",
              "1              NaN       148.0    134.0     13549.0  14906.0  \n",
              "2              NaN       154.0     91.0     15648.0  11467.0  \n",
              "3              NaN       116.0    137.0     12892.0  13211.0  \n",
              "4              NaN       134.0    100.0     13507.0  12634.0  \n",
              "...            ...         ...      ...         ...      ...  \n",
              "81464          8.0         NaN      NaN         NaN      NaN  \n",
              "81465          0.0         NaN      NaN         NaN      NaN  \n",
              "81466         14.0         NaN      NaN         NaN      NaN  \n",
              "81467          5.0         NaN      NaN         NaN      NaN  \n",
              "81468          4.0         NaN      NaN         NaN      NaN  \n",
              "\n",
              "[81469 rows x 454 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframes = [deny, events, heroes, items, kills, lh, xp]\n",
        "\n",
        "joined = X\n",
        "\n",
        "for df in dataframes:\n",
        "    df = df.set_index('mid')\n",
        "    joined = pd.concat([joined, df], axis=1)\n",
        "\n",
        "joined = joined.reset_index()\n",
        "joined"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303acbe4",
      "metadata": {
        "id": "303acbe4"
      },
      "source": [
        "Заметим, что все данные, для которых мы хотим предсказывать, имеют mid $\\le 49947$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7040ae39",
      "metadata": {
        "id": "7040ae39",
        "outputId": "1d9411e9-f3df-4855-d854-d692ae5a4096"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mid</th>\n",
              "      <th>player_0_60</th>\n",
              "      <th>player_0_120</th>\n",
              "      <th>player_0_180</th>\n",
              "      <th>player_0_240</th>\n",
              "      <th>player_0_300</th>\n",
              "      <th>player_0_360</th>\n",
              "      <th>player_0_420</th>\n",
              "      <th>player_0_480</th>\n",
              "      <th>player_0_540</th>\n",
              "      <th>...</th>\n",
              "      <th>item_117</th>\n",
              "      <th>item_118</th>\n",
              "      <th>item_119</th>\n",
              "      <th>item_120</th>\n",
              "      <th>radiant_killed</th>\n",
              "      <th>dire_killed</th>\n",
              "      <th>radiant_lh</th>\n",
              "      <th>dire_lh</th>\n",
              "      <th>radiant_xp</th>\n",
              "      <th>dire_xp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>957.0</td>\n",
              "      <td>1161.0</td>\n",
              "      <td>1571.0</td>\n",
              "      <td>1721.0</td>\n",
              "      <td>1871.0</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>2850.0</td>\n",
              "      <td>3303.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>126.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>14245.0</td>\n",
              "      <td>13289.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>285.0</td>\n",
              "      <td>435.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>1334.0</td>\n",
              "      <td>1667.0</td>\n",
              "      <td>1818.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>13549.0</td>\n",
              "      <td>14906.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>288.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>1224.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>2611.0</td>\n",
              "      <td>2879.0</td>\n",
              "      <td>3069.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>154.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15648.0</td>\n",
              "      <td>11467.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>288.0</td>\n",
              "      <td>438.0</td>\n",
              "      <td>1230.0</td>\n",
              "      <td>1381.0</td>\n",
              "      <td>1916.0</td>\n",
              "      <td>2436.0</td>\n",
              "      <td>2585.0</td>\n",
              "      <td>2735.0</td>\n",
              "      <td>2886.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>116.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>12892.0</td>\n",
              "      <td>13211.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>348.0</td>\n",
              "      <td>572.0</td>\n",
              "      <td>745.0</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>1590.0</td>\n",
              "      <td>1787.0</td>\n",
              "      <td>2070.0</td>\n",
              "      <td>2520.0</td>\n",
              "      <td>2948.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>134.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>13507.0</td>\n",
              "      <td>12634.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>49943</td>\n",
              "      <td>286.0</td>\n",
              "      <td>435.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>981.0</td>\n",
              "      <td>1131.0</td>\n",
              "      <td>1926.0</td>\n",
              "      <td>2076.0</td>\n",
              "      <td>2226.0</td>\n",
              "      <td>2674.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>13510.0</td>\n",
              "      <td>15041.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>49944</td>\n",
              "      <td>564.0</td>\n",
              "      <td>1146.0</td>\n",
              "      <td>1599.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>2409.0</td>\n",
              "      <td>2685.0</td>\n",
              "      <td>3378.0</td>\n",
              "      <td>3722.0</td>\n",
              "      <td>4503.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>13348.0</td>\n",
              "      <td>15213.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>49945</td>\n",
              "      <td>285.0</td>\n",
              "      <td>436.0</td>\n",
              "      <td>586.0</td>\n",
              "      <td>799.0</td>\n",
              "      <td>1013.0</td>\n",
              "      <td>1318.0</td>\n",
              "      <td>1468.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>2116.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>12470.0</td>\n",
              "      <td>11471.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>49946</td>\n",
              "      <td>288.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>959.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>2335.0</td>\n",
              "      <td>3842.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>11723.0</td>\n",
              "      <td>16033.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>49947</td>\n",
              "      <td>484.0</td>\n",
              "      <td>870.0</td>\n",
              "      <td>1210.0</td>\n",
              "      <td>1759.0</td>\n",
              "      <td>2154.0</td>\n",
              "      <td>2640.0</td>\n",
              "      <td>3176.0</td>\n",
              "      <td>3760.0</td>\n",
              "      <td>4041.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>14728.0</td>\n",
              "      <td>14685.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 454 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mid  player_0_60  player_0_120  player_0_180  player_0_240  \\\n",
              "0          0        750.0         957.0        1161.0        1571.0   \n",
              "1          1        285.0         435.0         585.0         736.0   \n",
              "2          2        288.0         756.0        1224.0        1617.0   \n",
              "3          3        288.0         438.0        1230.0        1381.0   \n",
              "4          4        348.0         572.0         745.0        1170.0   \n",
              "...      ...          ...           ...           ...           ...   \n",
              "49943  49943        286.0         435.0         650.0         981.0   \n",
              "49944  49944        564.0        1146.0        1599.0        2008.0   \n",
              "49945  49945        285.0         436.0         586.0         799.0   \n",
              "49946  49946        288.0         437.0         585.0         736.0   \n",
              "49947  49947        484.0         870.0        1210.0        1759.0   \n",
              "\n",
              "       player_0_300  player_0_360  player_0_420  player_0_480  player_0_540  \\\n",
              "0            1721.0        1871.0        2022.0        2850.0        3303.0   \n",
              "1            1334.0        1667.0        1818.0        2016.0        2328.0   \n",
              "2            1920.0        2328.0        2611.0        2879.0        3069.0   \n",
              "3            1916.0        2436.0        2585.0        2735.0        2886.0   \n",
              "4            1590.0        1787.0        2070.0        2520.0        2948.0   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "49943        1131.0        1926.0        2076.0        2226.0        2674.0   \n",
              "49944        2409.0        2685.0        3378.0        3722.0        4503.0   \n",
              "49945        1013.0        1318.0        1468.0        1617.0        2116.0   \n",
              "49946         959.0        1110.0        1258.0        2335.0        3842.0   \n",
              "49947        2154.0        2640.0        3176.0        3760.0        4041.0   \n",
              "\n",
              "       ...  item_117  item_118  item_119  item_120  radiant_killed  \\\n",
              "0      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "1      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "2      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "3      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "4      ...       0.0       0.0       0.0       0.0             NaN   \n",
              "...    ...       ...       ...       ...       ...             ...   \n",
              "49943  ...       0.0       0.0       0.0       0.0             0.0   \n",
              "49944  ...       0.0       0.0       0.0       0.0             2.0   \n",
              "49945  ...       0.0       0.0       0.0       0.0             6.0   \n",
              "49946  ...       0.0       0.0       0.0       0.0             5.0   \n",
              "49947  ...       0.0       0.0       0.0       0.0             2.0   \n",
              "\n",
              "       dire_killed  radiant_lh  dire_lh  radiant_xp  dire_xp  \n",
              "0              NaN       126.0    167.0     14245.0  13289.0  \n",
              "1              NaN       148.0    134.0     13549.0  14906.0  \n",
              "2              NaN       154.0     91.0     15648.0  11467.0  \n",
              "3              NaN       116.0    137.0     12892.0  13211.0  \n",
              "4              NaN       134.0    100.0     13507.0  12634.0  \n",
              "...            ...         ...      ...         ...      ...  \n",
              "49943          5.0       129.0    122.0     13510.0  15041.0  \n",
              "49944          8.0       115.0    115.0     13348.0  15213.0  \n",
              "49945          4.0       110.0    113.0     12470.0  11471.0  \n",
              "49946          2.0        86.0    167.0     11723.0  16033.0  \n",
              "49947          3.0       156.0    144.0     14728.0  14685.0  \n",
              "\n",
              "[49948 rows x 454 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = joined['mid'] <= 49947\n",
        "joined[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa229d7",
      "metadata": {
        "id": "2aa229d7",
        "outputId": "89e40c42-9e3e-4f2e-d678-f7a993a734c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "mid                 0\n",
              "player_0_60         0\n",
              "player_0_120        0\n",
              "player_0_180        0\n",
              "player_0_240        0\n",
              "                ...  \n",
              "dire_killed     24402\n",
              "radiant_lh          0\n",
              "dire_lh             0\n",
              "radiant_xp          0\n",
              "dire_xp             0\n",
              "Length: 454, dtype: int64"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joined[mask].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32487a6",
      "metadata": {
        "id": "b32487a6"
      },
      "source": [
        "Заполним пропуски в данных по убийствам значением -1, которого нет среди реальных значений данного признака"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbd3d76",
      "metadata": {
        "id": "4dbd3d76",
        "outputId": "f78acd4d-fdcf-476d-dffb-44f9196f36a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joined.loc[mask, 'radiant_killed'] = joined.loc[mask, 'radiant_killed'].fillna(-1)\n",
        "joined.loc[mask, 'dire_killed'] = joined.loc[mask, 'dire_killed'].fillna(-1)\n",
        "joined[mask]['radiant_killed'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a20c76e",
      "metadata": {
        "id": "3a20c76e"
      },
      "source": [
        "Пропуски по ивентам -- нулями (таким образом мы вроде как сохраняем баланс по этой фиче и даём остальным решать исход)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81e818d",
      "metadata": {
        "id": "e81e818d"
      },
      "outputs": [],
      "source": [
        "joined.loc[mask, [0, 1, 2, 3, 4, 5, 6]] = joined.loc[mask, [0, 1, 2, 3, 4, 5, 6]].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f9dc5b",
      "metadata": {
        "id": "99f9dc5b",
        "outputId": "07fbc6a3-74f3-4f89-8a56-794548f7cbe4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player_0_60</th>\n",
              "      <th>player_0_120</th>\n",
              "      <th>player_0_180</th>\n",
              "      <th>player_0_240</th>\n",
              "      <th>player_0_300</th>\n",
              "      <th>player_0_360</th>\n",
              "      <th>player_0_420</th>\n",
              "      <th>player_0_480</th>\n",
              "      <th>player_0_540</th>\n",
              "      <th>player_1_60</th>\n",
              "      <th>...</th>\n",
              "      <th>item_117</th>\n",
              "      <th>item_118</th>\n",
              "      <th>item_119</th>\n",
              "      <th>item_120</th>\n",
              "      <th>radiant_killed</th>\n",
              "      <th>dire_killed</th>\n",
              "      <th>radiant_lh</th>\n",
              "      <th>dire_lh</th>\n",
              "      <th>radiant_xp</th>\n",
              "      <th>dire_xp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>750.0</td>\n",
              "      <td>957.0</td>\n",
              "      <td>1161.0</td>\n",
              "      <td>1571.0</td>\n",
              "      <td>1721.0</td>\n",
              "      <td>1871.0</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>2850.0</td>\n",
              "      <td>3303.0</td>\n",
              "      <td>350.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>14245.0</td>\n",
              "      <td>13289.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>285.0</td>\n",
              "      <td>435.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>1334.0</td>\n",
              "      <td>1667.0</td>\n",
              "      <td>1818.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>344.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>13549.0</td>\n",
              "      <td>14906.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>288.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>1224.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>2611.0</td>\n",
              "      <td>2879.0</td>\n",
              "      <td>3069.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15648.0</td>\n",
              "      <td>11467.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>288.0</td>\n",
              "      <td>438.0</td>\n",
              "      <td>1230.0</td>\n",
              "      <td>1381.0</td>\n",
              "      <td>1916.0</td>\n",
              "      <td>2436.0</td>\n",
              "      <td>2585.0</td>\n",
              "      <td>2735.0</td>\n",
              "      <td>2886.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>12892.0</td>\n",
              "      <td>13211.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>348.0</td>\n",
              "      <td>572.0</td>\n",
              "      <td>745.0</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>1590.0</td>\n",
              "      <td>1787.0</td>\n",
              "      <td>2070.0</td>\n",
              "      <td>2520.0</td>\n",
              "      <td>2948.0</td>\n",
              "      <td>561.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>13507.0</td>\n",
              "      <td>12634.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>286.0</td>\n",
              "      <td>435.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>981.0</td>\n",
              "      <td>1131.0</td>\n",
              "      <td>1926.0</td>\n",
              "      <td>2076.0</td>\n",
              "      <td>2226.0</td>\n",
              "      <td>2674.0</td>\n",
              "      <td>345.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>13510.0</td>\n",
              "      <td>15041.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>564.0</td>\n",
              "      <td>1146.0</td>\n",
              "      <td>1599.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>2409.0</td>\n",
              "      <td>2685.0</td>\n",
              "      <td>3378.0</td>\n",
              "      <td>3722.0</td>\n",
              "      <td>4503.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>13348.0</td>\n",
              "      <td>15213.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>285.0</td>\n",
              "      <td>436.0</td>\n",
              "      <td>586.0</td>\n",
              "      <td>799.0</td>\n",
              "      <td>1013.0</td>\n",
              "      <td>1318.0</td>\n",
              "      <td>1468.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>2116.0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>12470.0</td>\n",
              "      <td>11471.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>288.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>959.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>2335.0</td>\n",
              "      <td>3842.0</td>\n",
              "      <td>540.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>11723.0</td>\n",
              "      <td>16033.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>484.0</td>\n",
              "      <td>870.0</td>\n",
              "      <td>1210.0</td>\n",
              "      <td>1759.0</td>\n",
              "      <td>2154.0</td>\n",
              "      <td>2640.0</td>\n",
              "      <td>3176.0</td>\n",
              "      <td>3760.0</td>\n",
              "      <td>4041.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>14728.0</td>\n",
              "      <td>14685.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 453 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       player_0_60  player_0_120  player_0_180  player_0_240  player_0_300  \\\n",
              "mid                                                                          \n",
              "0            750.0         957.0        1161.0        1571.0        1721.0   \n",
              "1            285.0         435.0         585.0         736.0        1334.0   \n",
              "2            288.0         756.0        1224.0        1617.0        1920.0   \n",
              "3            288.0         438.0        1230.0        1381.0        1916.0   \n",
              "4            348.0         572.0         745.0        1170.0        1590.0   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "49943        286.0         435.0         650.0         981.0        1131.0   \n",
              "49944        564.0        1146.0        1599.0        2008.0        2409.0   \n",
              "49945        285.0         436.0         586.0         799.0        1013.0   \n",
              "49946        288.0         437.0         585.0         736.0         959.0   \n",
              "49947        484.0         870.0        1210.0        1759.0        2154.0   \n",
              "\n",
              "       player_0_360  player_0_420  player_0_480  player_0_540  player_1_60  \\\n",
              "mid                                                                          \n",
              "0            1871.0        2022.0        2850.0        3303.0        350.0   \n",
              "1            1667.0        1818.0        2016.0        2328.0        344.0   \n",
              "2            2328.0        2611.0        2879.0        3069.0        288.0   \n",
              "3            2436.0        2585.0        2735.0        2886.0        556.0   \n",
              "4            1787.0        2070.0        2520.0        2948.0        561.0   \n",
              "...             ...           ...           ...           ...          ...   \n",
              "49943        1926.0        2076.0        2226.0        2674.0        345.0   \n",
              "49944        2685.0        3378.0        3722.0        4503.0        286.0   \n",
              "49945        1318.0        1468.0        1617.0        2116.0        285.0   \n",
              "49946        1110.0        1258.0        2335.0        3842.0        540.0   \n",
              "49947        2640.0        3176.0        3760.0        4041.0        288.0   \n",
              "\n",
              "       ...  item_117  item_118  item_119  item_120  radiant_killed  \\\n",
              "mid    ...                                                           \n",
              "0      ...       0.0       0.0       0.0       0.0            -1.0   \n",
              "1      ...       0.0       0.0       0.0       0.0            -1.0   \n",
              "2      ...       0.0       0.0       0.0       0.0            -1.0   \n",
              "3      ...       0.0       0.0       0.0       0.0            -1.0   \n",
              "4      ...       0.0       0.0       0.0       0.0            -1.0   \n",
              "...    ...       ...       ...       ...       ...             ...   \n",
              "49943  ...       0.0       0.0       0.0       0.0             0.0   \n",
              "49944  ...       0.0       0.0       0.0       0.0             2.0   \n",
              "49945  ...       0.0       0.0       0.0       0.0             6.0   \n",
              "49946  ...       0.0       0.0       0.0       0.0             5.0   \n",
              "49947  ...       0.0       0.0       0.0       0.0             2.0   \n",
              "\n",
              "       dire_killed  radiant_lh  dire_lh  radiant_xp  dire_xp  \n",
              "mid                                                           \n",
              "0             -1.0       126.0    167.0     14245.0  13289.0  \n",
              "1             -1.0       148.0    134.0     13549.0  14906.0  \n",
              "2             -1.0       154.0     91.0     15648.0  11467.0  \n",
              "3             -1.0       116.0    137.0     12892.0  13211.0  \n",
              "4             -1.0       134.0    100.0     13507.0  12634.0  \n",
              "...            ...         ...      ...         ...      ...  \n",
              "49943          5.0       129.0    122.0     13510.0  15041.0  \n",
              "49944          8.0       115.0    115.0     13348.0  15213.0  \n",
              "49945          4.0       110.0    113.0     12470.0  11471.0  \n",
              "49946          2.0        86.0    167.0     11723.0  16033.0  \n",
              "49947          3.0       156.0    144.0     14728.0  14685.0  \n",
              "\n",
              "[49948 rows x 453 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data = joined[mask].set_index('mid')\n",
        "all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b16079",
      "metadata": {
        "id": "79b16079",
        "outputId": "5f0d265c-5b80-4bf2-c67f-0f8d31909354"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "757a1e3d",
      "metadata": {
        "id": "757a1e3d",
        "outputId": "8548dfa3-dd94-4e40-d0e8-93454549bef2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mid</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49943</th>\n",
              "      <td>21723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49944</th>\n",
              "      <td>19926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>17017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>19283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49947</th>\n",
              "      <td>25107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49948 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0\n",
              "mid         \n",
              "0      21454\n",
              "1      22165\n",
              "2      21392\n",
              "3      20628\n",
              "4      18038\n",
              "...      ...\n",
              "49943  21723\n",
              "49944  19926\n",
              "49945  17017\n",
              "49946  19283\n",
              "49947  25107\n",
              "\n",
              "[49948 rows x 1 columns]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y.to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c0f9d1",
      "metadata": {
        "id": "57c0f9d1",
        "outputId": "0d8d9b5e-4d14-4a7e-d049-2cc32b329620"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  750.,   957.,  1161., ...,   167., 14245., 13289.],\n",
              "       [  285.,   435.,   585., ...,   134., 13549., 14906.],\n",
              "       [  288.,   756.,  1224., ...,    91., 15648., 11467.],\n",
              "       ...,\n",
              "       [  285.,   436.,   586., ...,   113., 12470., 11471.],\n",
              "       [  288.,   437.,   585., ...,   167., 11723., 16033.],\n",
              "       [  484.,   870.,  1210., ...,   144., 14728., 14685.]])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_arr = all_data.to_numpy()\n",
        "X_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec76193",
      "metadata": {
        "id": "8ec76193"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_arr, Y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ded4e754",
      "metadata": {
        "id": "ded4e754"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# масштабируем признаки\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4eb059",
      "metadata": {
        "id": "5a4eb059",
        "outputId": "8bd1f7fb-67d9-4656-baa3-9ae9ba87bff9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.0854789 ,  0.09289928,  0.30530145, ..., -1.01060228,\n",
              "        -0.50724164, -1.18888377],\n",
              "       [ 0.20861036,  0.99633198,  0.79298896, ..., -0.59433976,\n",
              "        -0.13064607, -0.5490756 ],\n",
              "       [ 0.33709537,  0.42574291,  0.38294324, ...,  0.54092168,\n",
              "         1.27797281, -0.5490756 ],\n",
              "       ...,\n",
              "       [-0.17149111,  0.54461563,  0.15972308, ..., -0.40512952,\n",
              "        -0.69863756, -0.33900983],\n",
              "       [ 0.40669141, -0.15843162, -0.62882638, ..., -0.29160338,\n",
              "        -0.82807077, -0.33695036],\n",
              "       [ 0.71184331,  0.85368472,  0.83666247, ..., -1.16197048,\n",
              "         5.58093861, -2.34699147]])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7663ab4",
      "metadata": {
        "id": "f7663ab4",
        "outputId": "30790686-3c5a-4e28-c98f-fd0cb6ed046e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "mid\n",
              "17838    17889\n",
              "41003    20124\n",
              "19095    23913\n",
              "18306    27950\n",
              "24158    16383\n",
              "         ...  \n",
              "2716     19593\n",
              "46961    26061\n",
              "27974    18818\n",
              "18292    19961\n",
              "35601    32383\n",
              "Length: 34963, dtype: int64"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224db058",
      "metadata": {
        "id": "224db058"
      },
      "source": [
        "Подберём learning rate для обучения на всех данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1805fa64",
      "metadata": {
        "id": "1805fa64",
        "outputId": "e9603d4a-f5ed-49d9-df73-35dd4edc1d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eta0': [1e-05, 5e-05, 1e-06, 4.9999999999999996e-06]}\n"
          ]
        }
      ],
      "source": [
        "params = dict(eta0 = [k * 10 ** (-i) for i in range(5, 7) for k in [1, 5]])\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7e86aa",
      "metadata": {
        "id": "7e7e86aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sgd_regressor = SGDRegressor(fit_intercept=True, max_iter=1000, shuffle=True)\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "grid_cv = GridSearchCV(estimator=sgd_regressor, param_grid=params, scoring='neg_mean_squared_error', cv=kf)\n",
        "grid_score = grid_cv.fit(x_train, y_train)\n",
        "\n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe98c874",
      "metadata": {
        "id": "fe98c874",
        "outputId": "b4611524-f02a-4809-8970-79b01954895f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "842.7156964055848"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(-grid_score.best_score_ ) ** 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85bb1e59",
      "metadata": {
        "id": "85bb1e59",
        "outputId": "ded3cee2-69ef-4a06-8eb3-5d559bd69be2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eta0': 5e-05}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6631e03",
      "metadata": {
        "id": "e6631e03"
      },
      "source": [
        "Дадим теперь модели сойтись: поставим побольше итераций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f531b058",
      "metadata": {
        "id": "f531b058",
        "outputId": "a66dad76-c0a7-4336-9661-cc036531f180"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "757.5655349030686"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "sgd_regressor = SGDRegressor(eta0=5e-5, fit_intercept=True, max_iter=10000, shuffle=True)\n",
        "\n",
        "sgd_regressor.fit(x_train, y_train)\n",
        "y_pred = sgd_regressor.predict(x_test)\n",
        "mse_score = mean_squared_error(y_pred, y_test) ** 0.5\n",
        "mse_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f32e985",
      "metadata": {
        "id": "5f32e985",
        "outputId": "d2da3892-bd0c-4650-c956-8737f55558dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'all_together': 757.5655349030686}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = dict(all_together = mse_score)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8bf635",
      "metadata": {
        "id": "ce8bf635"
      },
      "source": [
        "###### Как можем видеть, результат стал чуточку лучше :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8097c0",
      "metadata": {
        "id": "2d8097c0"
      },
      "source": [
        "###### У нас есть датасеты deny, events, heroes, items, kills, lh, xp и базовый X (gold)\n",
        "\n",
        "Попробуем обогатить Х каждым из них по отдельности и посмотрим, какая комбинация них покажет себя лучше всего"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c25597",
      "metadata": {
        "id": "11c25597"
      },
      "outputs": [],
      "source": [
        "dataframes = [deny, deny, events, heroes, items, kills, lh, xp]\n",
        "dataframes_names = ['vanila', 'deny', 'events', 'heroes', 'items', 'kills', 'lh', 'xp']\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ind = 0\n",
        "for df in dataframes:\n",
        "    df_name = dataframes_names[ind]\n",
        "    ind += 1\n",
        "    df = df.set_index('mid')\n",
        "    if df_name != 'vanila':\n",
        "        now_data = pd.concat([X, df], axis=1)\n",
        "    else:\n",
        "        now_data = X\n",
        "    if 'radiant_killed' in df.columns:\n",
        "        now_data.loc[mask, 'radiant_killed'] = now_data.loc[mask, 'radiant_killed'].fillna(-1)\n",
        "        now_data.loc[mask, 'dire_killed'] = now_data.loc[mask, 'dire_killed'].fillna(-1)\n",
        "    \n",
        "    if 0 in df.columns:\n",
        "        now_data.loc[mask, [0, 1, 2, 3, 4, 5, 6]] = now_data.loc[mask, [0, 1, 2, 3, 4, 5, 6]].fillna(0)\n",
        "    \n",
        "    now_data = now_data[mask]\n",
        "    X_arr = now_data.to_numpy()\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X_arr, Y, test_size=0.3)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train = scaler.transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "    \n",
        "    sgd_regressor = SGDRegressor(eta0=5e-5, fit_intercept=True, max_iter=10000, shuffle=True)\n",
        "\n",
        "    sgd_regressor.fit(x_train, y_train)\n",
        "    y_pred = sgd_regressor.predict(x_test)\n",
        "    scores[df_name] = mean_squared_error(y_pred, y_test) ** 0.5\n",
        "    \n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06edbaac",
      "metadata": {
        "id": "06edbaac",
        "outputId": "c615993b-4a31-41af-fe96-729b2070f9a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'all_together': 757.5655349030686,\n",
              " 'vanila': 788.2437463824022,\n",
              " 'deny': 799.4900002874975,\n",
              " 'events': 777.2581069345264,\n",
              " 'heroes': 792.1401515325235,\n",
              " 'items': 784.2671942242183,\n",
              " 'kills': 791.6138790183728,\n",
              " 'lh': 784.490790954141,\n",
              " 'xp': 793.3887771234146}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596bf7e2",
      "metadata": {
        "id": "596bf7e2",
        "outputId": "42f9d2cd-582b-4e55-c6c5-87d1cb45a7da"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.plot(scores.keys(), scores.values(), '--o')\n",
        "plt.ylabel('RMSE loss')\n",
        "plt.xlabel('Datasets')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a21cf6ee",
      "metadata": {
        "id": "a21cf6ee"
      },
      "source": [
        "Как мы видим, наиболее важными признаком оказался стобец events, в то время как добавление deny и xp вообще ухудшило качество нашей модели."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf62acfc",
      "metadata": {
        "id": "cf62acfc"
      },
      "source": [
        "###### Собственно, ответики:\n",
        "1. Да, добавление новых данных заметно улучшило предсказания нашей модели. Причём если обучаться на всех данных, то получается заметно лучшее качество, что можно видеть на графике выше\n",
        "2. Добавление к исходным данным deny.csv немного ухудшило предсказание -- скорее всего это произошло из-за того, что эта табличка содержит не очень информативные признаки (лично я, никогда не играя в доту, не смог расшифровать, что deny вообще значит)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7509dc-071e-483f-812c-5a9ab05323f2",
      "metadata": {
        "id": "ff7509dc-071e-483f-812c-5a9ab05323f2"
      },
      "source": [
        "## Бинарная классификация"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e002d140-b694-42f3-97a7-4f1e013ef50d",
      "metadata": {
        "id": "e002d140-b694-42f3-97a7-4f1e013ef50d"
      },
      "source": [
        "**Задание 4** (3 балла)\n",
        "\n",
        "Реализуйте класс `CustomSGDClassifier`, который бы обучал логистическую регрессию, используя `SGD`.\n",
        "\n",
        "Класс, должен принимать следующие параметры при инициализации:\n",
        "    \n",
        "    learning_rate : float : параметр задающий скорось обучения\n",
        "    fit_intercept : bool : если True, то добавляем свободный член\n",
        "    max_iter : int : максимальное число эпох\n",
        "    shuffle : bool : если True, то перемещиваем данные обучения перед каждой эпохой\n",
        "    \n",
        "и иметь методы `fit`, `predict` и `predict_proba`.\n",
        "\n",
        "После обучения (запуска метода `fit`) мы должны мочь обратиться к атрибутам класса: \n",
        "    \n",
        "    coef_  : ndarray of shape (n_features,)\n",
        "    intercept_ : ndarray of shape (1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b422105",
      "metadata": {
        "id": "2b422105"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CustomSGDClassifier(BaseEstimator):\n",
        "    def __init__(self, learning_rate: float, fit_intercept: bool,\n",
        "                 max_iter: int, shuffle: bool, batch_size: int = 256):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.max_iter = max_iter\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_history = []\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigm(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "        \n",
        "    def fit(self, x_train: np.ndarray, y_train: np.ndarray):\n",
        "    \n",
        "        self.batch_size = min(self.batch_size, len(x_train))\n",
        "        _x_train = np.array(x_train)\n",
        "        _y_train = np.array(y_train).reshape(-1, 1)\n",
        "\n",
        "        if self.fit_intercept:\n",
        "            _x_train = np.hstack((_x_train, np.ones((len(x_train), 1))))\n",
        "\n",
        "        self.weights = np.random.randn(_x_train.shape[1])\n",
        "                \n",
        "        last_ind = 0\n",
        "        for epoch_num in range(1, self.max_iter + 1):\n",
        "            if self.shuffle:\n",
        "                indexes = np.random.randint(low=0, high=len(_x_train), size=self.batch_size)\n",
        "            else:\n",
        "                indexes = np.array([ind % len(_x_train) for ind in range(last_ind, last_ind + self.batch_size)])\n",
        "                last_ind = (last_ind + self.batch_size) % len(_x_train)\n",
        "        \n",
        "    \n",
        "            y_pred = self._sigm(_x_train[indexes] @ self.weights.reshape(-1, 1))\n",
        "            grad = ((_y_train[indexes] - y_pred) * _x_train[indexes]).sum(axis=0)\n",
        "            self.weights += self.learning_rate * grad\n",
        "        \n",
        "        if self.fit_intercept:\n",
        "            self.coef_ = self.weights[:-1].flatten()\n",
        "            self.intercept_ = float(self.weights[-1])\n",
        "        else:\n",
        "            self.coef_ = self.weight.flatten()\n",
        "            self.intercept_ = None\n",
        "            \n",
        "        return self\n",
        "\n",
        "    \n",
        "    def predict_proba(self, x_test: np.ndarray) -> np.ndarray:\n",
        "        _x_test = np.array(x_test)\n",
        "        \n",
        "        if self.fit_intercept:\n",
        "            _x_test = np.hstack((_x_test, np.ones((len(x_test), 1))))\n",
        "        \n",
        "        predictions = self._sigm(_x_test @ self.weights)\n",
        "        \n",
        "        return predictions.flatten()\n",
        "            \n",
        "    \n",
        "    def predict(self, x_test: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        probas = self.predict_proba(x_test)\n",
        "        labels = (probas >= 0.5)\n",
        "        return labels.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c34f70-2e57-4afa-a170-434db2383ce8",
      "metadata": {
        "id": "a6c34f70-2e57-4afa-a170-434db2383ce8"
      },
      "source": [
        "Используя `CustomSGDClassifier` научитесь прогнозировать победу команды Radiant.\n",
        "\n",
        "Оптимальный набор гиперпараметров и признаков отберите, опираясь на такие метрики, как `precision`,  `recall` и `ROC-AUC`.\n",
        "    \n",
        "Все эксперименты должны быть подкреплены корректными и понятными графиками."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee55a785",
      "metadata": {
        "id": "ee55a785",
        "outputId": "d7b4d7d4-f63d-4756-e7e0-6ec3c196ed06"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player_0_60</th>\n",
              "      <th>player_0_120</th>\n",
              "      <th>player_0_180</th>\n",
              "      <th>player_0_240</th>\n",
              "      <th>player_0_300</th>\n",
              "      <th>player_0_360</th>\n",
              "      <th>player_0_420</th>\n",
              "      <th>player_0_480</th>\n",
              "      <th>player_0_540</th>\n",
              "      <th>player_1_60</th>\n",
              "      <th>...</th>\n",
              "      <th>item_118</th>\n",
              "      <th>item_119</th>\n",
              "      <th>item_120</th>\n",
              "      <th>radiant_killed</th>\n",
              "      <th>dire_killed</th>\n",
              "      <th>radiant_lh</th>\n",
              "      <th>dire_lh</th>\n",
              "      <th>radiant_xp</th>\n",
              "      <th>dire_xp</th>\n",
              "      <th>radiant_won</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>750.0</td>\n",
              "      <td>957.0</td>\n",
              "      <td>1161.0</td>\n",
              "      <td>1571.0</td>\n",
              "      <td>1721.0</td>\n",
              "      <td>1871.0</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>2850.0</td>\n",
              "      <td>3303.0</td>\n",
              "      <td>350.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>14245.0</td>\n",
              "      <td>13289.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>285.0</td>\n",
              "      <td>435.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>1334.0</td>\n",
              "      <td>1667.0</td>\n",
              "      <td>1818.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>344.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>13549.0</td>\n",
              "      <td>14906.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>288.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>1224.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>2328.0</td>\n",
              "      <td>2611.0</td>\n",
              "      <td>2879.0</td>\n",
              "      <td>3069.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>15648.0</td>\n",
              "      <td>11467.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>348.0</td>\n",
              "      <td>572.0</td>\n",
              "      <td>745.0</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>1590.0</td>\n",
              "      <td>1787.0</td>\n",
              "      <td>2070.0</td>\n",
              "      <td>2520.0</td>\n",
              "      <td>2948.0</td>\n",
              "      <td>561.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>13507.0</td>\n",
              "      <td>12634.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>344.0</td>\n",
              "      <td>682.0</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>2207.0</td>\n",
              "      <td>2734.0</td>\n",
              "      <td>3125.0</td>\n",
              "      <td>3599.0</td>\n",
              "      <td>4071.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>10416.0</td>\n",
              "      <td>14672.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49939</th>\n",
              "      <td>287.0</td>\n",
              "      <td>560.0</td>\n",
              "      <td>1086.0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>1698.0</td>\n",
              "      <td>2043.0</td>\n",
              "      <td>2434.0</td>\n",
              "      <td>3232.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>405.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>15439.0</td>\n",
              "      <td>12277.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49940</th>\n",
              "      <td>894.0</td>\n",
              "      <td>1806.0</td>\n",
              "      <td>2201.0</td>\n",
              "      <td>2947.0</td>\n",
              "      <td>3485.0</td>\n",
              "      <td>4515.0</td>\n",
              "      <td>4729.0</td>\n",
              "      <td>5343.0</td>\n",
              "      <td>5840.0</td>\n",
              "      <td>845.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>15941.0</td>\n",
              "      <td>11089.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49941</th>\n",
              "      <td>373.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>757.0</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>1597.0</td>\n",
              "      <td>1783.0</td>\n",
              "      <td>2294.0</td>\n",
              "      <td>2445.0</td>\n",
              "      <td>408.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>13691.0</td>\n",
              "      <td>14906.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49945</th>\n",
              "      <td>285.0</td>\n",
              "      <td>436.0</td>\n",
              "      <td>586.0</td>\n",
              "      <td>799.0</td>\n",
              "      <td>1013.0</td>\n",
              "      <td>1318.0</td>\n",
              "      <td>1468.0</td>\n",
              "      <td>1617.0</td>\n",
              "      <td>2116.0</td>\n",
              "      <td>285.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>12470.0</td>\n",
              "      <td>11471.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49946</th>\n",
              "      <td>288.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>959.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>2335.0</td>\n",
              "      <td>3842.0</td>\n",
              "      <td>540.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>11723.0</td>\n",
              "      <td>16033.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24974 rows × 454 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       player_0_60  player_0_120  player_0_180  player_0_240  player_0_300  \\\n",
              "mid                                                                          \n",
              "0            750.0         957.0        1161.0        1571.0        1721.0   \n",
              "1            285.0         435.0         585.0         736.0        1334.0   \n",
              "2            288.0         756.0        1224.0        1617.0        1920.0   \n",
              "4            348.0         572.0         745.0        1170.0        1590.0   \n",
              "5            344.0         682.0        1041.0        1692.0        2207.0   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "49939        287.0         560.0        1086.0        1300.0        1698.0   \n",
              "49940        894.0        1806.0        2201.0        2947.0        3485.0   \n",
              "49941        373.0         556.0         757.0        1050.0        1297.0   \n",
              "49945        285.0         436.0         586.0         799.0        1013.0   \n",
              "49946        288.0         437.0         585.0         736.0         959.0   \n",
              "\n",
              "       player_0_360  player_0_420  player_0_480  player_0_540  player_1_60  \\\n",
              "mid                                                                          \n",
              "0            1871.0        2022.0        2850.0        3303.0        350.0   \n",
              "1            1667.0        1818.0        2016.0        2328.0        344.0   \n",
              "2            2328.0        2611.0        2879.0        3069.0        288.0   \n",
              "4            1787.0        2070.0        2520.0        2948.0        561.0   \n",
              "5            2734.0        3125.0        3599.0        4071.0        437.0   \n",
              "...             ...           ...           ...           ...          ...   \n",
              "49939        2043.0        2434.0        3232.0        3504.0        405.0   \n",
              "49940        4515.0        4729.0        5343.0        5840.0        845.0   \n",
              "49941        1597.0        1783.0        2294.0        2445.0        408.0   \n",
              "49945        1318.0        1468.0        1617.0        2116.0        285.0   \n",
              "49946        1110.0        1258.0        2335.0        3842.0        540.0   \n",
              "\n",
              "       ...  item_118  item_119  item_120  radiant_killed  dire_killed  \\\n",
              "mid    ...                                                              \n",
              "0      ...       0.0       0.0       0.0            -1.0         -1.0   \n",
              "1      ...       0.0       0.0       0.0            -1.0         -1.0   \n",
              "2      ...       0.0       0.0       0.0            -1.0         -1.0   \n",
              "4      ...       0.0       0.0       0.0            -1.0         -1.0   \n",
              "5      ...       0.0       0.0       0.0            -1.0         -1.0   \n",
              "...    ...       ...       ...       ...             ...          ...   \n",
              "49939  ...       0.0       0.0       0.0             3.0          3.0   \n",
              "49940  ...       0.0       0.0       0.0             5.0          0.0   \n",
              "49941  ...       0.0       0.0       0.0             2.0          4.0   \n",
              "49945  ...       0.0       0.0       0.0             6.0          4.0   \n",
              "49946  ...       0.0       0.0       0.0             5.0          2.0   \n",
              "\n",
              "       radiant_lh  dire_lh  radiant_xp  dire_xp  radiant_won  \n",
              "mid                                                           \n",
              "0           126.0    167.0     14245.0  13289.0            1  \n",
              "1           148.0    134.0     13549.0  14906.0            0  \n",
              "2           154.0     91.0     15648.0  11467.0            1  \n",
              "4           134.0    100.0     13507.0  12634.0            1  \n",
              "5            94.0    123.0     10416.0  14672.0            1  \n",
              "...           ...      ...         ...      ...          ...  \n",
              "49939       149.0    116.0     15439.0  12277.0            0  \n",
              "49940       152.0    123.0     15941.0  11089.0            1  \n",
              "49941       159.0    118.0     13691.0  14906.0            1  \n",
              "49945       110.0    113.0     12470.0  11471.0            0  \n",
              "49946        86.0    167.0     11723.0  16033.0            0  \n",
              "\n",
              "[24974 rows x 454 columns]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "train = all_data.join(train_data.set_index('mid'), how='inner', lsuffix='_left', rsuffix='_right')#.drop(['mid', 'mid_left', 'mid_right'], axis=1)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1011b884",
      "metadata": {
        "id": "1011b884",
        "outputId": "5944b162-e1ad-49d8-985f-e149a4b58f1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  750.,   957.,  1161., ...,   167., 14245., 13289.],\n",
              "       [  285.,   435.,   585., ...,   134., 13549., 14906.],\n",
              "       [  288.,   756.,  1224., ...,    91., 15648., 11467.],\n",
              "       ...,\n",
              "       [  373.,   556.,   757., ...,   118., 13691., 14906.],\n",
              "       [  285.,   436.,   586., ...,   113., 12470., 11471.],\n",
              "       [  288.,   437.,   585., ...,   167., 11723., 16033.]])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train = train[train.columns[:-1]].to_numpy()\n",
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f263e03-2a62-4c4a-b181-9eb0e11695d2",
      "metadata": {
        "id": "3f263e03-2a62-4c4a-b181-9eb0e11695d2",
        "outputId": "5e18ca99-3cca-49e5-98af-8828bdac40b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 0, 0])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train = train[train.columns[-1]].to_numpy()\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e005ca10",
      "metadata": {
        "id": "e005ca10"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x_train,y_train,test_size = 0.2,random_state = 42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494be5c3",
      "metadata": {
        "id": "494be5c3",
        "outputId": "1add7bcb-5f46-43b2-fc90-96cd232fc2a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((19979, 453), (19979,))"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9da84c6",
      "metadata": {
        "id": "b9da84c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler  \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)  \n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265240c5",
      "metadata": {
        "id": "265240c5",
        "outputId": "eafa769c-2109-48ab-9515-bc4e2fcfec2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': [0.03,\n",
              "  0.06,\n",
              "  0.09,\n",
              "  0.003,\n",
              "  0.006,\n",
              "  0.009000000000000001,\n",
              "  0.00030000000000000003,\n",
              "  0.0006000000000000001,\n",
              "  0.0009000000000000001,\n",
              "  3.0000000000000004e-05,\n",
              "  6.000000000000001e-05,\n",
              "  9e-05,\n",
              "  3e-06,\n",
              "  6e-06,\n",
              "  9e-06]}"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = dict(learning_rate = [10 ** (-k) * (3 * i) for k in range(2, 7) for i in [1, 2, 3]])\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8fbbf0",
      "metadata": {
        "id": "bf8fbbf0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
        "\n",
        "roc_auc_history = dict()\n",
        "precision_history = dict()\n",
        "recall_history = dict()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "for lr in params['learning_rate']:\n",
        "    cust = CustomSGDClassifier(learning_rate=lr, fit_intercept=True, max_iter=10000, shuffle=True)\n",
        "    cust.fit(x_train, y_train)\n",
        "    y_pred = cust.predict(X_test)\n",
        "    roc_auc_history[lr] = roc_auc_score(Y_test, cust.predict_proba(X_test))\n",
        "    precision_history[lr] = precision_score(y_true=Y_test, y_pred=y_pred)\n",
        "    recall_history[lr] = recall_score(y_true=Y_test, y_pred=y_pred)\n",
        "    \n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097a0774",
      "metadata": {
        "id": "097a0774",
        "outputId": "74c1f6b3-fff5-4e4f-e51c-23ad874a36a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0.03: 0.642243279508566,\n",
              " 0.06: 0.6500045144942577,\n",
              " 0.09: 0.6500964912900786,\n",
              " 0.003: 0.6432451437858389,\n",
              " 0.006: 0.6455802297925775,\n",
              " 0.009000000000000001: 0.6476274967081144,\n",
              " 0.00030000000000000003: 0.6507376780012069,\n",
              " 0.0006000000000000001: 0.6500530332581669,\n",
              " 0.0009000000000000001: 0.6434831588478624,\n",
              " 3.0000000000000004e-05: 0.6753917970514729,\n",
              " 6.000000000000001e-05: 0.6663361392816267,\n",
              " 9e-05: 0.6632082855910839,\n",
              " 3e-06: 0.688699112331542,\n",
              " 6e-06: 0.6799777038607442,\n",
              " 9e-06: 0.6768998951866173}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roc_auc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ccf76a2",
      "metadata": {
        "id": "9ccf76a2",
        "outputId": "22cf8632-398f-4eb8-ce80-5beca184921f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0.03: 0.6616954474097331,\n",
              " 0.06: 0.6715705765407555,\n",
              " 0.09: 0.6722554890219561,\n",
              " 0.003: 0.6622047244094488,\n",
              " 0.006: 0.6634203350214258,\n",
              " 0.009000000000000001: 0.6695894778796333,\n",
              " 0.00030000000000000003: 0.6662749706227967,\n",
              " 0.0006000000000000001: 0.668389662027833,\n",
              " 0.0009000000000000001: 0.6613218615565115,\n",
              " 3.0000000000000004e-05: 0.675826363998407,\n",
              " 6.000000000000001e-05: 0.6759776536312849,\n",
              " 9e-05: 0.6682206682206682,\n",
              " 3e-06: 0.6646848989298454,\n",
              " 6e-06: 0.6590551181102362,\n",
              " 9e-06: 0.6625791139240507}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "precision_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186c6864",
      "metadata": {
        "id": "186c6864",
        "outputId": "763eca4e-87d2-4495-9a3b-7fc8005a4d37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0.03: 0.6454823889739663,\n",
              " 0.06: 0.6466309341500766,\n",
              " 0.09: 0.6447166921898928,\n",
              " 0.003: 0.6439509954058193,\n",
              " 0.006: 0.6519908116385911,\n",
              " 0.009000000000000001: 0.6431852986217458,\n",
              " 0.00030000000000000003: 0.6512251148545176,\n",
              " 0.0006000000000000001: 0.6435681470137825,\n",
              " 0.0009000000000000001: 0.6473966309341501,\n",
              " 3.0000000000000004e-05: 0.6496937212863706,\n",
              " 6.000000000000001e-05: 0.6485451761102603,\n",
              " 9e-05: 0.6584992343032159,\n",
              " 3e-06: 0.6420367534456355,\n",
              " 6e-06: 0.6408882082695253,\n",
              " 9e-06: 0.641271056661562}"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recall_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359e9fb5",
      "metadata": {
        "id": "359e9fb5",
        "outputId": "f1a5d9a9-331e-4443-dbb7-b47e1d0fad73"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "VERY_BIG_NUMBER = 1e10\n",
        "\n",
        "to_plot = dict()\n",
        "for key, value in roc_auc_history.items():\n",
        "    if value < VERY_BIG_NUMBER:\n",
        "        to_plot[f'{key:.6f}'] = value\n",
        "\n",
        "plt.plot(to_plot.keys(), to_plot.values(), '--o')\n",
        "plt.ylabel('Roc Auc')\n",
        "plt.xlabel('learning rate')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f53c650",
      "metadata": {
        "id": "9f53c650",
        "outputId": "f5b093ec-bf48-4134-fe47-983a95e3e2fa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "VERY_BIG_NUMBER = 1e10\n",
        "\n",
        "to_plot = dict()\n",
        "for key, value in precision_history.items():\n",
        "    if value < VERY_BIG_NUMBER:\n",
        "        to_plot[f'{key:.6f}'] = value\n",
        "\n",
        "plt.plot(to_plot.keys(), to_plot.values(), '--o')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('learning rate')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe60b79",
      "metadata": {
        "id": "9fe60b79",
        "outputId": "bfa05a41-1832-4e79-d721-fd50ab7b36c2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "VERY_BIG_NUMBER = 1e10\n",
        "\n",
        "to_plot = dict()\n",
        "for key, value in recall_history.items():\n",
        "    if value < VERY_BIG_NUMBER:\n",
        "        to_plot[f'{key:.6f}'] = value\n",
        "\n",
        "plt.plot(to_plot.keys(), to_plot.values(), '--o')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('learning rate')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e2e07de",
      "metadata": {
        "id": "0e2e07de"
      },
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb49f634",
      "metadata": {
        "id": "cb49f634"
      },
      "source": [
        "Лучшее значение метрики ROC-AUC получаем при learning rate = $3 \\cdot 10^{-6}$\n",
        "\n",
        "Дадим теперь модели сойтись: поставим побольше итераций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8954fdb8",
      "metadata": {
        "id": "8954fdb8",
        "outputId": "938d9803-391d-4562-8c97-3959d8f12fdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7539756467936809"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "custom_model = CustomSGDClassifier(learning_rate=3e-6, fit_intercept=True, max_iter=500000, shuffle=True)\n",
        "custom_model.fit(X_train, Y_train)\n",
        "\n",
        "roc_auc_score(Y_test, custom_model.predict_proba(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cde2dd8",
      "metadata": {
        "id": "1cde2dd8",
        "outputId": "2b987d9d-bb32-4214-c5ef-9045970d4ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision: 0.6992928917007816, recall: 0.7193721286370597\n"
          ]
        }
      ],
      "source": [
        "print(f'precision: {precision_score(y_true=Y_test, y_pred=custom_model.predict(X_test))}, recall: {recall_score(y_true=Y_test, y_pred=custom_model.predict(X_test))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcd4805",
      "metadata": {
        "id": "8bcd4805"
      },
      "source": [
        "Обучим теперь, для сравнения, логистическую регрессию из sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f12783b",
      "metadata": {
        "scrolled": true,
        "id": "6f12783b",
        "outputId": "755547ee-c5d6-4b5e-d86d-9a2ebaeffa69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.759521727088058"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logModel = LogisticRegression(max_iter=10000)\n",
        "\n",
        "logModel.fit(X_train, Y_train)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(Y_test, logModel.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43bcbb2a",
      "metadata": {
        "id": "43bcbb2a",
        "outputId": "fc0741ef-0243-4791-fbd3-fcd752dc018f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision: 0.7042569081404033, recall: 0.722052067381317\n"
          ]
        }
      ],
      "source": [
        "print(f'precision: {precision_score(y_true=Y_test, y_pred=logModel.predict(X_test))}, recall: {recall_score(y_true=Y_test, y_pred=logModel.predict(X_test))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7134b55f",
      "metadata": {
        "id": "7134b55f"
      },
      "source": [
        "Как видим, мы получили похожие результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba188fe-9ebf-42a2-9e2b-26210b79f21b",
      "metadata": {
        "id": "0ba188fe-9ebf-42a2-9e2b-26210b79f21b"
      },
      "source": [
        "**Задание 5** (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f637e4-eb7c-4bb3-90dc-3f6f6c0866dd",
      "metadata": {
        "id": "a5f637e4-eb7c-4bb3-90dc-3f6f6c0866dd"
      },
      "source": [
        "Попробуйте повысить качество предсказания, используя:\n",
        "\n",
        " - [Logistic Regression Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        " - [C-Support Vector Classification](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        " - [Multi-layer Perceptron Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
        " \n",
        "Проведите эксперементы по подбору гиперпараметров. Какой алгоритм показал наилучшее качество?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e6d266",
      "metadata": {
        "id": "b5e6d266"
      },
      "outputs": [],
      "source": [
        "# Чтобы признаки были не масштабированы\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503c2f36",
      "metadata": {
        "id": "503c2f36",
        "outputId": "3ee1dc0c-f3c7-4a9c-cd8a-7be736130a93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7009642381365196"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time\n",
        "\n",
        "logModel = LogisticRegression(max_iter=1000, C=1, penalty='l1', solver='saga')\n",
        "\n",
        "t_start = time.time()\n",
        "logModel.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, logModel.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d06f3f",
      "metadata": {
        "id": "83d06f3f",
        "outputId": "d38afef7-5653-4694-a610-225208be5547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Logistic Regression training time is 136.16s for 1000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Logistic Regression training time is {t_end - t_start:.2f}s for {logModel.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5743be",
      "metadata": {
        "id": "2e5743be",
        "outputId": "1a1923eb-f806-4030-efe8-7677f058f784"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(gamma='auto', probability=True, max_iter=1000)\n",
        "\n",
        "t_start = time.time()\n",
        "svc.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, svc.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7d058d",
      "metadata": {
        "id": "dc7d058d",
        "outputId": "09c0429a-5b8f-4a6a-dea1-a997cd1fdb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Support Vector Machine training time is 97.71s for 1000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Support Vector Machine training time is {t_end - t_start:.2f}s for {svc.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dddd1a9",
      "metadata": {
        "id": "9dddd1a9",
        "outputId": "101b6f80-82e8-4737-a29f-26f1896d7527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6703771739458736"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(solver='lbfgs', alpha=1e-3, max_iter=1000, learning_rate='invscaling',\n",
        "                     hidden_layer_sizes=(5, 25, 5), random_state=42)\n",
        "\n",
        "t_start = time.time()\n",
        "mlp.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, mlp.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b20c3d8a",
      "metadata": {
        "id": "b20c3d8a",
        "outputId": "d11e621a-1476-426c-b44d-390493c63ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Multi Layer Perceptron training time is 10.77s for 1000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Multi Layer Perceptron training time is {t_end - t_start:.2f}s for {mlp.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e2eb56",
      "metadata": {
        "id": "17e2eb56"
      },
      "source": [
        "###### Видим, что SVM вообще не хочет обучаться. Посмотрим, поможет ли масштабирование признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca7014b-fa1d-4c80-b9d9-3fcccd29b0cb",
      "metadata": {
        "id": "8ca7014b-fa1d-4c80-b9d9-3fcccd29b0cb"
      },
      "source": [
        "Может ли масштабирование повлиять на работу этих алгоритмов?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb72ed76-5b0b-4d44-9fdd-18a218353fe4",
      "metadata": {
        "id": "bb72ed76-5b0b-4d44-9fdd-18a218353fe4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler  \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)  \n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d7a4cd",
      "metadata": {
        "id": "60d7a4cd",
        "outputId": "60fbdb99-b624-4e9a-fc14-24b1e15b34c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7599659468966948"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_start = time.time()\n",
        "logModel.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, logModel.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac10c6e1",
      "metadata": {
        "id": "ac10c6e1",
        "outputId": "c0ca80cb-a010-4975-919e-bbe624460dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Logistic Regression training time is 129.51s for 1000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Logistic Regression training time is {t_end - t_start:.2f}s for {logModel.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2aa812",
      "metadata": {
        "id": "be2aa812",
        "outputId": "c50f05a6-25e2-41f3-f908-f3f5a6089419"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5874897741082026"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_start = time.time()\n",
        "svc.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, svc.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de9658b",
      "metadata": {
        "id": "5de9658b",
        "outputId": "f2fa93cb-e5c8-4b04-c57a-f84eda7b0a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Support Vector Machine training time is 97.61s for 1000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Support Vector Machine training time is {t_end - t_start:.2f}s for {svc.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4189b8",
      "metadata": {
        "id": "6c4189b8",
        "outputId": "a076f57f-bcb0-42c3-eb66-1de380a241b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6947818712048527"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_start = time.time()\n",
        "mlp.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, mlp.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881a31f5",
      "metadata": {
        "id": "881a31f5",
        "outputId": "0b78d337-cb96-4213-e4d7-560f878d978c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Multi Layer Perceptron training time is 62.46s for 1000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Multi Layer Perceptron training time is {t_end - t_start:.2f}s for {mlp.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28dcedb8",
      "metadata": {
        "id": "28dcedb8"
      },
      "source": [
        "###### Мы в очередной раз убедились, что масштабирование решает. Модели, обучаемые на отмасштабированных данных и сходятся быстрее, и показывают лучшие метрики при прочих равных"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da4ce78",
      "metadata": {
        "id": "5da4ce78"
      },
      "source": [
        "###### Проведём эксперименты, чтобы подобрать оптимальные гиперпараметры каждой из моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d82444",
      "metadata": {
        "id": "56d82444",
        "outputId": "8d5faefb-a9aa-4c26-f089-814ce082eccb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'penalty': ['l1', 'l2'],\n",
              " 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga']}"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params_log = dict({'penalty': ['l1', 'l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear','saga']})\n",
        "params_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d305d7",
      "metadata": {
        "id": "82d305d7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "grid_cv = GridSearchCV(estimator=logModel, param_grid=params_log, scoring='roc_auc', cv=kf)\n",
        "grid_score = grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6593c6",
      "metadata": {
        "id": "2f6593c6",
        "outputId": "104b5b6b-3ca3-4a27-dfd8-2a6175131958"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7530442445653863"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333f2947",
      "metadata": {
        "id": "333f2947",
        "outputId": "6568ad80-852e-42e4-e813-c82f437797d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'penalty': 'l1', 'solver': 'saga'}"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150822d4",
      "metadata": {
        "id": "150822d4"
      },
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ec76f1",
      "metadata": {
        "id": "16ec76f1",
        "outputId": "f1352e7e-fccd-4171-9226-7f600a896714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params_svm = dict({'kernel': ['linear', 'poly', 'rbf', 'sigmoid']})\n",
        "params_svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94db6c70",
      "metadata": {
        "id": "94db6c70"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "grid_cv = GridSearchCV(estimator=svc, param_grid=params_svm, scoring='roc_auc', cv=kf)\n",
        "grid_score = grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed9d137",
      "metadata": {
        "id": "5ed9d137",
        "outputId": "06b1488a-b50c-4380-fbdd-54c7c38f4c30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5871595805897356"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97232788",
      "metadata": {
        "id": "97232788",
        "outputId": "db1ccca4-092f-4def-ad5a-a6e5884e30d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'kernel': 'poly'}"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf51b600",
      "metadata": {
        "id": "cf51b600"
      },
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a33e2d",
      "metadata": {
        "id": "10a33e2d",
        "outputId": "d3798374-ff2b-4e01-9914-048f2be506b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': ['tanh', 'relu'],\n",
              " 'learning_rate_init': [0.01, 0.001, 0.0001, 1e-05],\n",
              " 'alpha': [0.001, 0.0001, 1e-05]}"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params_mlp = dict({'activation': ['tanh', 'relu'],\n",
        "                   'learning_rate_init': [10 ** (-k) for k in range(2, 6)],\n",
        "                  'alpha': [10 ** (-k) for k in range(3, 6)]})\n",
        "params_mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ec9f49",
      "metadata": {
        "id": "58ec9f49"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "grid_cv = GridSearchCV(estimator=mlp, param_grid=params_mlp, scoring='roc_auc', cv=kf)\n",
        "grid_score = grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "warnings.resetwarnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ecea881",
      "metadata": {
        "id": "3ecea881",
        "outputId": "588070fe-30e2-435f-aca1-35b99c24e51b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6874304487932459"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7f3644",
      "metadata": {
        "id": "9b7f3644",
        "outputId": "a2a10b1e-69ea-43d4-cc3b-77e0f2333328"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'alpha': 1e-05, 'learning_rate_init': 0.01}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_score.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d6c198",
      "metadata": {
        "id": "99d6c198"
      },
      "source": [
        "###### Возьмём лучшие модели и дадим им больше итераций на обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de6966c",
      "metadata": {
        "id": "2de6966c",
        "outputId": "5aa2f077-df93-4eb6-f076-0d6a10546ecf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7599204806377999"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_logModel = LogisticRegression(max_iter=100000, C=1, penalty='l1', solver='saga')\n",
        "\n",
        "t_start = time.time()\n",
        "best_logModel.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, best_logModel.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d7f36a",
      "metadata": {
        "id": "01d7f36a",
        "outputId": "3ff2773a-8db6-457e-d3c9-630ea31095d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Logistic Regression training time is 165.65s for 100000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Logistic Regression training time is {t_end - t_start:.2f}s for {best_logModel.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b19bf5",
      "metadata": {
        "id": "65b19bf5",
        "outputId": "86eba0b9-6589-4cc2-ecb8-bb37cb59406b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7551900618148332"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_svc = SVC(gamma='auto', probability=True, max_iter=100000, kernel='sigmoid')\n",
        "\n",
        "t_start = time.time()\n",
        "best_svc.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, best_svc.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c588489",
      "metadata": {
        "id": "3c588489",
        "outputId": "e9ae8e8a-6ff7-4c86-8268-9f44d25f513b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Support Vector Machine training time is 704.90s for 100000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Support Vector Machine training time is {t_end - t_start:.2f}s for {best_svc.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a723912",
      "metadata": {
        "id": "2a723912",
        "outputId": "40737352-7fd4-48d8-97b8-741149718bc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6823172079668454"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, max_iter=100000, learning_rate='invscaling',\n",
        "                     hidden_layer_sizes=(5, 25, 5), random_state=42, activation='relu', learning_rate_init=0.01)\n",
        "\n",
        "t_start = time.time()\n",
        "best_mlp.fit(X_train, Y_train)\n",
        "t_end = time.time()\n",
        "\n",
        "roc_auc_score(Y_test, best_mlp.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0308eb9f",
      "metadata": {
        "id": "0308eb9f",
        "outputId": "8a4b7a52-66c0-4fb3-d084-02506fccf493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Multi Layer Perceptron training time is 342.87s for 100000 epochs\n"
          ]
        }
      ],
      "source": [
        "print(f'For Multi Layer Perceptron training time is {t_end - t_start:.2f}s for {best_mlp.max_iter} epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3d6d39",
      "metadata": {
        "id": "3b3d6d39"
      },
      "source": [
        "###### Лучше всего в предсказании себя проявила логистическая регрессия. Сообственно, результат её работы (хотя немножко не этот) и был мной загружен в конкурс на кэггле"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95fbf274",
      "metadata": {
        "id": "95fbf274"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ebabac8-1307-4c05-8bcd-e295a31973f7",
      "metadata": {
        "id": "2ebabac8-1307-4c05-8bcd-e295a31973f7"
      },
      "source": [
        "### Сравнение построенных моделей\n",
        "\n",
        "![](http://cdn.shopify.com/s/files/1/0870/1066/files/compare_e8b89647-3cb6-4871-a976-2e36e5987773.png?1750043340268621065)\n",
        "\n",
        "После того как было построено много моделей, правильным продолжением является сравнение их между собой.  Воспользуйтесь диаграммой размаха (\"ящик с усами\") для сравнения алгоритмов между собой. Эту диаграмму можно построить при помощи [boxplot](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.boxplot.html) из matplotlib либо через [обертку](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html) над ней из pandas.\n",
        "\n",
        "**Задание 6** (2 балла)\n",
        "\n",
        "Для каждого типа классификатора, выберите тот, которых давал наилучшее качество на кросс-валидации и постройте диаграмму размаха. Все классификаторы должны быть изображены на одном графике.\n",
        " \n",
        "Сделайте общие итоговые выводы о классификаторах с точки зрения их работы с признаками и сложности самой модели (какие гиперпараметры есть у модели, сильно ли изменение значения гиперпараметра влияет на качество модели)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a529d9a",
      "metadata": {
        "id": "2a529d9a"
      },
      "source": [
        "###### Сохраним в словарь res результаты кросс-валидаций для наших моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b4330f",
      "metadata": {
        "id": "e1b4330f"
      },
      "outputs": [],
      "source": [
        "res = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599acff3",
      "metadata": {
        "scrolled": true,
        "id": "599acff3",
        "outputId": "265423f7-7925-41e4-ab48-d911558666e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.75173424, 0.75235661, 0.74856295, 0.75652067, 0.75382607])"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "cv_score = cross_val_score(best_logModel, X_train, Y_train, cv=kfold, scoring='roc_auc')\n",
        "res['LogReg'] = cv_score\n",
        "cv_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2fb96f",
      "metadata": {
        "id": "dc2fb96f",
        "outputId": "e6898404-8cb2-4af7-bf4b-568f400bf3e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.75551055, 0.75107996, 0.74694526, 0.75263464, 0.7468319 ])"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_score = cross_val_score(best_svc, X_train, Y_train, cv=kfold, scoring='roc_auc')\n",
        "res['SVC'] = cv_score\n",
        "cv_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fcea9ec",
      "metadata": {
        "id": "1fcea9ec",
        "outputId": "7c543bd9-f41f-4c9f-bb3c-73625a786784"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/home/vlad/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.66703068, 0.65832561, 0.67216636, 0.67795635, 0.66045956])"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_score = cross_val_score(best_mlp, X_train, Y_train, cv=kfold, scoring='roc_auc')\n",
        "res['MLP'] = cv_score\n",
        "cv_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6949c367",
      "metadata": {
        "id": "6949c367",
        "outputId": "f66ccc47-8338-46b3-bafc-be019b8a69bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'LogReg': array([0.75173424, 0.75235661, 0.74856295, 0.75652067, 0.75382607]),\n",
              " 'SVC': array([0.75551055, 0.75107996, 0.74694526, 0.75263464, 0.7468319 ]),\n",
              " 'MLP': array([0.66703068, 0.65832561, 0.67216636, 0.67795635, 0.66045956])}"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f7b344",
      "metadata": {
        "id": "12f7b344"
      },
      "source": [
        "###### Построим теперь по этим данным ящики с усами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c92137",
      "metadata": {
        "id": "46c92137",
        "outputId": "2cd4474c-4462-4e9a-b341-357adb7d338d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "plt.boxplot(res.values(), labels = res.keys())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d5f99a",
      "metadata": {
        "id": "02d5f99a"
      },
      "source": [
        "###### Видим, что модель перцептрона сильно проигрывает двум остальным. Чтобы сравнить LogReg и SVC, построим для них отдельный boxplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4391f1e",
      "metadata": {
        "id": "b4391f1e",
        "outputId": "550ed175-9be5-44e8-c421-b26ea4911969"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "plt.boxplot(list(res.values())[:-1], labels = list(res.keys())[:-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74384f95",
      "metadata": {
        "id": "74384f95"
      },
      "source": [
        "###### Собственно, получили, что логистическая регрессия и SVC одинаково хорошо справляются с полученной задачей.\n",
        "\n",
        "Перцептрон нам обучить не удалось. Возможно, могло бы помочь STACK MORE LAYERS, но заниматься этим не будем)\n",
        "\n",
        "Гиперпараметры действительно решают -- самый важный это learning rate. В зависимости от него наша модель может как не сойтись в принципе, так и сойтись к какому-то глобальному минимуму.\n",
        "\n",
        "Также мы поняли, что масштабирование данных очень важно: модели, обучаемые на отмасштабированных данных и сходятся быстрее, и показывают лучшие метрики при прочих равных"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}